{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ruptures # ruptures is a Python library for off-line change point detection. This package provides methods for the analysis and segmentation of non-stationary signals. Implemented algorithms include exact and approximate detection for various parametric and non-parametric models. ruptures focuses on ease of use by providing a well-documented and consistent interface. In addition, thanks to its modular structure, different algorithms and models can be connected and extended within this package. How to cite. If you use ruptures in a scientific publication, we would appreciate citations to the following paper: C. Truong, L. Oudre, N. Vayatis. Selective review of offline change point detection methods. Signal Processing , 167:107299, 2020. [journal] [pdf] Basic usage # (Please refer to the documentation for more advanced use.) The following snippet creates a noisy piecewise constant signal, performs a penalized kernel change point detection and displays the results (alternating colors mark true regimes and dashed lines mark estimated change points). import matplotlib.pyplot as plt import ruptures as rpt # generate signal n_samples , dim , sigma = 1000 , 3 , 4 n_bkps = 4 # number of breakpoints signal , bkps = rpt . pw_constant ( n_samples , dim , n_bkps , noise_std = sigma ) # detection algo = rpt . Pelt ( model = \"rbf\" ) . fit ( signal ) result = algo . predict ( pen = 10 ) # display rpt . display ( signal , bkps , result ) plt . show () General information # Contact # Concerning this package, its use and bugs, use the issue page of the ruptures repository . For other inquiries, you can contact me here . Important links # Documentation: link . Pypi package index: link Dependencies and install # Installation instructions can be found here . Changelog # See the changelog for a history of notable changes to ruptures . Thanks to all our contributors # License # This project is under BSD license. BSD 2-Clause License Copyright (c) 2017-2021, ENS Paris-Saclay, CNRS All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Welcome to ruptures"},{"location":"#welcome-to-ruptures","text":"ruptures is a Python library for off-line change point detection. This package provides methods for the analysis and segmentation of non-stationary signals. Implemented algorithms include exact and approximate detection for various parametric and non-parametric models. ruptures focuses on ease of use by providing a well-documented and consistent interface. In addition, thanks to its modular structure, different algorithms and models can be connected and extended within this package. How to cite. If you use ruptures in a scientific publication, we would appreciate citations to the following paper: C. Truong, L. Oudre, N. Vayatis. Selective review of offline change point detection methods. Signal Processing , 167:107299, 2020. [journal] [pdf]","title":"Welcome to ruptures"},{"location":"#basic-usage","text":"(Please refer to the documentation for more advanced use.) The following snippet creates a noisy piecewise constant signal, performs a penalized kernel change point detection and displays the results (alternating colors mark true regimes and dashed lines mark estimated change points). import matplotlib.pyplot as plt import ruptures as rpt # generate signal n_samples , dim , sigma = 1000 , 3 , 4 n_bkps = 4 # number of breakpoints signal , bkps = rpt . pw_constant ( n_samples , dim , n_bkps , noise_std = sigma ) # detection algo = rpt . Pelt ( model = \"rbf\" ) . fit ( signal ) result = algo . predict ( pen = 10 ) # display rpt . display ( signal , bkps , result ) plt . show ()","title":"Basic usage"},{"location":"#general-information","text":"","title":"General information"},{"location":"#contact","text":"Concerning this package, its use and bugs, use the issue page of the ruptures repository . For other inquiries, you can contact me here .","title":"Contact"},{"location":"#important-links","text":"Documentation: link . Pypi package index: link","title":"Important links"},{"location":"#dependencies-and-install","text":"Installation instructions can be found here .","title":"Dependencies and install"},{"location":"#changelog","text":"See the changelog for a history of notable changes to ruptures .","title":"Changelog"},{"location":"#thanks-to-all-our-contributors","text":"","title":"Thanks to all our contributors"},{"location":"#license","text":"This project is under BSD license. BSD 2-Clause License Copyright (c) 2017-2021, ENS Paris-Saclay, CNRS All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"contributing/","text":"Contributing # Before contributing # In all following steps, it is highly recommended to use a virtual environment. Build and installation are performed using pip so be sure to have the latest version available. python -m pip install --upgrade pip Install the development version # It is important that you contribute to the latest version of the code. To that end, start by cloning the Github repository. git clone https://github.com/deepcharles/ruptures cd ruptures Then install the downloaded package with pip . python -m pip install --editable .[dev] Note that python -m can be omitted most of the times, but within virtualenvs, it can prevent certain errors. Also, in certain terminals (such as zsh ), the square brackets must be escaped, e.g. replace .[dev] by .\\[dev\\] . In addition to numpy , scipy and ruptures , this command will install all packages needed to develop ruptures . The exact list of librairies can be found in the setup.cfg file (section [options.extras_require] ). Pre-commit hooks # We use pre-commit to run Git hooks before submitting the code to review. These hook scripts perform simple tasks before each commit (code formatting mostly). To activate the hooks, simply run the following command in your terminal. pre-commit install If you try to commit a non-compliant (i.e. badly formatted) file, pre-commit will modify this file and make the commit fail. However you need to stage the new changes yourself as pre-commit will not do that for you (this is by design; see here or here ). Fortunately, pre-commit outputs useful messages. The list of hooks (and their options) can be found in .pre-commit-config.yaml . For more information, see their website . If you want to manually run all pre-commit hooks on a repository, run pre-commit run --all-files . To run individual hooks use pre-commit run <hook_id> . Contribute to the code # Write tests # The following command executes the test suite. python -m pytest Write docstrings # Contribute to the documentation # Use MkDocs . Use mkdocs serve to preview your changes. Once you are satisfied, no need to build the documentation, the CI will take care of that and publish it online at the next release of the package (if the pull request has been merged). Add examples to the gallery # An easy way to showcase your work with ruptures is to write a narrative example. To that, simply put a Jupyter notebook in the docs/examples folder. To make it appear in the documentation, add a reference in mkdocs.yml ( nav > Gallery of examples ): if the notebook's name is my_notebook.ipynb , it will be available as examples/my_notebook.ipynb . It will be rendered automatically when MkDocs builds the documentation. Note To automatically add a Binder link and a download link to your notebook, simply add the following line of code. <!-- {{ add_binder_block(page) }} --> Ideally, place this code below the title of the notebook (same cell) and it will be rendered as in here . We welcome any interesting work about a new cost function, algorithm, data, calibration method, etc. Any other package can be used in combination with ruptures . However, each example should be clearly explained with text and figures. The amount of raw code should also remain limited for readability. Miscellaneous # Naming convention # We try to follow (roughly) a consistent naming convention of modules, classes, functions, etc. When in doubt, you can refer to the PEP 8 style guide for Python code .","title":"Contributing"},{"location":"contributing/#contributing","text":"","title":"Contributing"},{"location":"contributing/#before-contributing","text":"In all following steps, it is highly recommended to use a virtual environment. Build and installation are performed using pip so be sure to have the latest version available. python -m pip install --upgrade pip","title":"Before contributing"},{"location":"contributing/#install-the-development-version","text":"It is important that you contribute to the latest version of the code. To that end, start by cloning the Github repository. git clone https://github.com/deepcharles/ruptures cd ruptures Then install the downloaded package with pip . python -m pip install --editable .[dev] Note that python -m can be omitted most of the times, but within virtualenvs, it can prevent certain errors. Also, in certain terminals (such as zsh ), the square brackets must be escaped, e.g. replace .[dev] by .\\[dev\\] . In addition to numpy , scipy and ruptures , this command will install all packages needed to develop ruptures . The exact list of librairies can be found in the setup.cfg file (section [options.extras_require] ).","title":"Install the development version"},{"location":"contributing/#pre-commit-hooks","text":"We use pre-commit to run Git hooks before submitting the code to review. These hook scripts perform simple tasks before each commit (code formatting mostly). To activate the hooks, simply run the following command in your terminal. pre-commit install If you try to commit a non-compliant (i.e. badly formatted) file, pre-commit will modify this file and make the commit fail. However you need to stage the new changes yourself as pre-commit will not do that for you (this is by design; see here or here ). Fortunately, pre-commit outputs useful messages. The list of hooks (and their options) can be found in .pre-commit-config.yaml . For more information, see their website . If you want to manually run all pre-commit hooks on a repository, run pre-commit run --all-files . To run individual hooks use pre-commit run <hook_id> .","title":"Pre-commit hooks"},{"location":"contributing/#contribute-to-the-code","text":"","title":"Contribute to the code"},{"location":"contributing/#write-tests","text":"The following command executes the test suite. python -m pytest","title":"Write tests"},{"location":"contributing/#write-docstrings","text":"","title":"Write docstrings"},{"location":"contributing/#contribute-to-the-documentation","text":"Use MkDocs . Use mkdocs serve to preview your changes. Once you are satisfied, no need to build the documentation, the CI will take care of that and publish it online at the next release of the package (if the pull request has been merged).","title":"Contribute to the documentation"},{"location":"contributing/#add-examples-to-the-gallery","text":"An easy way to showcase your work with ruptures is to write a narrative example. To that, simply put a Jupyter notebook in the docs/examples folder. To make it appear in the documentation, add a reference in mkdocs.yml ( nav > Gallery of examples ): if the notebook's name is my_notebook.ipynb , it will be available as examples/my_notebook.ipynb . It will be rendered automatically when MkDocs builds the documentation. Note To automatically add a Binder link and a download link to your notebook, simply add the following line of code. <!-- {{ add_binder_block(page) }} --> Ideally, place this code below the title of the notebook (same cell) and it will be rendered as in here . We welcome any interesting work about a new cost function, algorithm, data, calibration method, etc. Any other package can be used in combination with ruptures . However, each example should be clearly explained with text and figures. The amount of raw code should also remain limited for readability.","title":"Add examples to the gallery"},{"location":"contributing/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"contributing/#naming-convention","text":"We try to follow (roughly) a consistent naming convention of modules, classes, functions, etc. When in doubt, you can refer to the PEP 8 style guide for Python code .","title":"Naming convention"},{"location":"custom-cost-function/","text":"Creating a custom cost function # In order to define custom cost functions, simply create a class that inherits from ruptures.base.BaseCost and implement the methods .fit(signal) and .error(start, end) : The method .fit(signal) takes a signal as input and sets parameters. It returns 'self' . The method .error(start, end) takes two indexes 'start' and 'end' and returns the cost on the segment start:end. Example See this custom cost example .","title":"Custom cost function"},{"location":"custom-cost-function/#creating-a-custom-cost-function","text":"In order to define custom cost functions, simply create a class that inherits from ruptures.base.BaseCost and implement the methods .fit(signal) and .error(start, end) : The method .fit(signal) takes a signal as input and sets parameters. It returns 'self' . The method .error(start, end) takes two indexes 'start' and 'end' and returns the cost on the segment start:end. Example See this custom cost example .","title":"Creating a custom cost function"},{"location":"fit-and-predict/","text":"Fitting and prediction: estimator basics # ruptures has an object-oriented modelling approach (largely inspired by scikit-learn ): change point detection algorithms are broken down into two conceptual objects that inherits from base classes: BaseEstimator and BaseCost . Initializing a new estimator # Each change point detection algorithm inherits from the base class ruptures.base.BaseEstimator . When a class that inherits from the base estimator is created, the .__init__() method initializes an estimator with the following arguments: model : \"l1\", \"l2\", \"normal\", \"rbf\", \"linear\", etc. Cost function to use to compute the approximation error. cost : a custom cost function to the detection algorithm. Should be a BaseCost instance. jump : reduce the set of possible change point indexes; predicted change points can only be a multiple of jump . min_size : minimum number of samples between two change points. Making a prediction # The main methods are .fit() , .predict() , .fit_predict() : .fit() : generally takes a signal as input and fit the algorithm to the data. .predict() : performs the change point detection. This method returns a list of indexes corresponding to the end of each regimes. By design, the last element of this list is the number of samples. .fit_predict() : helper method which calls .fit() and .predict() successively.","title":"Fitting and predicting"},{"location":"fit-and-predict/#fitting-and-prediction-estimator-basics","text":"ruptures has an object-oriented modelling approach (largely inspired by scikit-learn ): change point detection algorithms are broken down into two conceptual objects that inherits from base classes: BaseEstimator and BaseCost .","title":"Fitting and prediction: estimator basics"},{"location":"fit-and-predict/#initializing-a-new-estimator","text":"Each change point detection algorithm inherits from the base class ruptures.base.BaseEstimator . When a class that inherits from the base estimator is created, the .__init__() method initializes an estimator with the following arguments: model : \"l1\", \"l2\", \"normal\", \"rbf\", \"linear\", etc. Cost function to use to compute the approximation error. cost : a custom cost function to the detection algorithm. Should be a BaseCost instance. jump : reduce the set of possible change point indexes; predicted change points can only be a multiple of jump . min_size : minimum number of samples between two change points.","title":"Initializing a new estimator"},{"location":"fit-and-predict/#making-a-prediction","text":"The main methods are .fit() , .predict() , .fit_predict() : .fit() : generally takes a signal as input and fit the algorithm to the data. .predict() : performs the change point detection. This method returns a list of indexes corresponding to the end of each regimes. By design, the last element of this list is the number of samples. .fit_predict() : helper method which calls .fit() and .predict() successively.","title":"Making a prediction"},{"location":"install/","text":"Installation # This library requires Python >=3.6 and the following packages: numpy , scipy and matplotlib (the last one is optional and only for display purposes). You can either install the latest stable release or the development version. Stable release # To install the latest stable release, use pip or conda . With pip python -m pip install ruptures With conda ruptures can be installed from the conda-forge channel (run conda config --add channels conda-forge to add it): conda install ruptures Development release # Alternatively, you can install the development version of ruptures which can contain features that have not yet been integrated to the stable release. To that end, refer to the contributing guide . Upgrade # Show the current version of the package. python -m pip show ruptures In order to upgrade to the version, use the following command. python -m pip install -U ruptures","title":"Installation"},{"location":"install/#installation","text":"This library requires Python >=3.6 and the following packages: numpy , scipy and matplotlib (the last one is optional and only for display purposes). You can either install the latest stable release or the development version.","title":"Installation"},{"location":"install/#stable-release","text":"To install the latest stable release, use pip or conda . With pip python -m pip install ruptures With conda ruptures can be installed from the conda-forge channel (run conda config --add channels conda-forge to add it): conda install ruptures","title":"Stable release"},{"location":"install/#development-release","text":"Alternatively, you can install the development version of ruptures which can contain features that have not yet been integrated to the stable release. To that end, refer to the contributing guide .","title":"Development release"},{"location":"install/#upgrade","text":"Show the current version of the package. python -m pip show ruptures In order to upgrade to the version, use the following command. python -m pip install -U ruptures","title":"Upgrade"},{"location":"license/","text":"License # This project is under BSD license. BSD 2-Clause License Copyright (c) 2017-2021, ENS Paris-Saclay, CNRS All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"license/#license","text":"This project is under BSD license. BSD 2-Clause License Copyright (c) 2017-2021, ENS Paris-Saclay, CNRS All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"release-notes/","text":"Changelog # The latest release notes are available directly in Github: ruptures/releases . Earlier releases are documented below \u2b07\ufe0f. 1.1.2 - 2020-12-01 # Added # 12cbc9e feat: add piecewise linear cpd (#91) a12b215 test: add code coverage badge (#97) 2e9b17f docs: add binder for notebooks (#94) da7544f docs(costcosine): add entry for CostCosine in docs (#93) 8c9aa35 build(setup.py/cfg): add build_ext to setup.py (#88) 10ef8e8 build(python39): add py39 to supported versions (#87) Changed # 069bd41 fix(kernelcpd): bug fix in pelt (#95) b4abc34 fix: memory leak in KernelCPD (#89) 1.1.1 - 2020-11-26 # No change to the code compared to the previous version. The package was only partly published to Pypi because of the failure of one provider in the CI. Since Pypi's policy prevents re-uploading twice the same version, we have to increment the version number. 1.1.0 - 2020-11-23 # Added # modify publishing process to Pypi PR#83 add cosine kernel (cost function and in KernelCPD)PR#74 add faster kernel change point detection ( KernelCPD , C implementation) PR#74 add manual trigger to publish to Pypi PR#72 Changed # 1.0.6 - 2020-10-23 # Added # Correct minor error in Dynp (about min_size) PR#74 Fix legacy formatting errors PR#69 New documentation (from Sphinx to Mkdocs) PR#64 Separate requirements.txt and requirements-dev.txt PR#64 A changelog file ( link ) New Github actions for automatic generation of documentation Pre-commit code formatting using black Changed # Correction of display function test #64 Add badges in the README (Github repo) PR#62: pypi version, python version, code style, contributor list Typo in documentation ( PR#60 ) by @gjaeger Documentation theme Documentation site 1.0.5 - 2020-07-22 # Changed # Link to documentation in PyPi description","title":"Release notes"},{"location":"release-notes/#changelog","text":"The latest release notes are available directly in Github: ruptures/releases . Earlier releases are documented below \u2b07\ufe0f.","title":"Changelog"},{"location":"release-notes/#112-2020-12-01","text":"","title":"1.1.2 - 2020-12-01"},{"location":"release-notes/#added","text":"12cbc9e feat: add piecewise linear cpd (#91) a12b215 test: add code coverage badge (#97) 2e9b17f docs: add binder for notebooks (#94) da7544f docs(costcosine): add entry for CostCosine in docs (#93) 8c9aa35 build(setup.py/cfg): add build_ext to setup.py (#88) 10ef8e8 build(python39): add py39 to supported versions (#87)","title":"Added"},{"location":"release-notes/#changed","text":"069bd41 fix(kernelcpd): bug fix in pelt (#95) b4abc34 fix: memory leak in KernelCPD (#89)","title":"Changed"},{"location":"release-notes/#111-2020-11-26","text":"No change to the code compared to the previous version. The package was only partly published to Pypi because of the failure of one provider in the CI. Since Pypi's policy prevents re-uploading twice the same version, we have to increment the version number.","title":"1.1.1 - 2020-11-26"},{"location":"release-notes/#110-2020-11-23","text":"","title":"1.1.0 - 2020-11-23"},{"location":"release-notes/#added_1","text":"modify publishing process to Pypi PR#83 add cosine kernel (cost function and in KernelCPD)PR#74 add faster kernel change point detection ( KernelCPD , C implementation) PR#74 add manual trigger to publish to Pypi PR#72","title":"Added"},{"location":"release-notes/#changed_1","text":"","title":"Changed"},{"location":"release-notes/#106-2020-10-23","text":"","title":"1.0.6 - 2020-10-23"},{"location":"release-notes/#added_2","text":"Correct minor error in Dynp (about min_size) PR#74 Fix legacy formatting errors PR#69 New documentation (from Sphinx to Mkdocs) PR#64 Separate requirements.txt and requirements-dev.txt PR#64 A changelog file ( link ) New Github actions for automatic generation of documentation Pre-commit code formatting using black","title":"Added"},{"location":"release-notes/#changed_2","text":"Correction of display function test #64 Add badges in the README (Github repo) PR#62: pypi version, python version, code style, contributor list Typo in documentation ( PR#60 ) by @gjaeger Documentation theme Documentation site","title":"Changed"},{"location":"release-notes/#105-2020-07-22","text":"","title":"1.0.5 - 2020-07-22"},{"location":"release-notes/#changed_3","text":"Link to documentation in PyPi description","title":"Changed"},{"location":"what-is-cpd/","text":"Getting started # What is change point detection? # Under construction. In the meantime, you can refer to the associated review of methods [Truong2020] . References # [Truong2020] Truong, C., Oudre, L., & Vayatis, N. (2020). Selective review of offline change point detection methods. Signal Processing , 167. [abstract] [doi] [pdf]","title":"Getting started"},{"location":"what-is-cpd/#getting-started","text":"","title":"Getting started"},{"location":"what-is-cpd/#what-is-change-point-detection","text":"Under construction. In the meantime, you can refer to the associated review of methods [Truong2020] .","title":"What is change point detection?"},{"location":"what-is-cpd/#references","text":"[Truong2020] Truong, C., Oudre, L., & Vayatis, N. (2020). Selective review of offline change point detection methods. Signal Processing , 167. [abstract] [doi] [pdf]","title":"References"},{"location":"code-reference/","text":"Introduction # This section describes the API of all functions and classes in the ruptures package. For a more intuitive description of each method, please refer to the User guide . Roughly, each module corresponds to a certain type of procedure: ruptures.base : base classes; ruptures.detection : search methods; ruptures.costs : costs functions; ruptures.datasets : data set generating utilities; ruptures.metrics : evaluation metrics; ruptures.show : display functions.","title":"Code reference"},{"location":"code-reference/#introduction","text":"This section describes the API of all functions and classes in the ruptures package. For a more intuitive description of each method, please refer to the User guide . Roughly, each module corresponds to a certain type of procedure: ruptures.base : base classes; ruptures.detection : search methods; ruptures.costs : costs functions; ruptures.datasets : data set generating utilities; ruptures.metrics : evaluation metrics; ruptures.show : display functions.","title":"Introduction"},{"location":"code-reference/base-reference/","text":"Base classes (ruptures.base) # All estimators and cost functions are subclasses of BaseEstimator and BaseCost respectively. BaseCost # Base class for all segment cost classes. Notes All classes should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). error ( self , start , end ) # Returns the cost on segment [start:end]. Source code in ruptures/base.py @abc . abstractmethod def error ( self , start , end ): \"\"\"Returns the cost on segment [start:end].\"\"\" pass fit ( self , * args , ** kwargs ) # Set the parameters of the cost function, for instance the Gram matrix, etc. Source code in ruptures/base.py @abc . abstractmethod def fit ( self , * args , ** kwargs ): \"\"\"Set the parameters of the cost function, for instance the Gram matrix, etc.\"\"\" pass sum_of_costs ( self , bkps ) # Returns the sum of segments cost for the given segmentation. Parameters: Name Type Description Default bkps list list of change points. By convention, bkps[-1]==n_samples. required Returns: Type Description float sum of costs Source code in ruptures/base.py def sum_of_costs ( self , bkps ): \"\"\"Returns the sum of segments cost for the given segmentation. Args: bkps (list): list of change points. By convention, bkps[-1]==n_samples. Returns: float: sum of costs \"\"\" soc = sum ( self . error ( start , end ) for start , end in pairwise ([ 0 ] + bkps )) return soc BaseEstimator # Base class for all change point detection estimators. Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). fit ( self , * args , ** kwargs ) # To call the segmentation algorithm. Source code in ruptures/base.py @abc . abstractmethod def fit ( self , * args , ** kwargs ): \"\"\"To call the segmentation algorithm.\"\"\" pass fit_predict ( self , * args , ** kwargs ) # To call the segmentation algorithm. Source code in ruptures/base.py @abc . abstractmethod def fit_predict ( self , * args , ** kwargs ): \"\"\"To call the segmentation algorithm.\"\"\" pass predict ( self , * args , ** kwargs ) # To call the segmentation algorithm. Source code in ruptures/base.py @abc . abstractmethod def predict ( self , * args , ** kwargs ): \"\"\"To call the segmentation algorithm.\"\"\" pass","title":"Base classes"},{"location":"code-reference/base-reference/#base-classes-rupturesbase","text":"All estimators and cost functions are subclasses of BaseEstimator and BaseCost respectively.","title":"Base classes (ruptures.base)"},{"location":"code-reference/base-reference/#ruptures.base.BaseCost","text":"Base class for all segment cost classes. Notes All classes should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"BaseCost"},{"location":"code-reference/base-reference/#ruptures.base.BaseCost.error","text":"Returns the cost on segment [start:end]. Source code in ruptures/base.py @abc . abstractmethod def error ( self , start , end ): \"\"\"Returns the cost on segment [start:end].\"\"\" pass","title":"error()"},{"location":"code-reference/base-reference/#ruptures.base.BaseCost.fit","text":"Set the parameters of the cost function, for instance the Gram matrix, etc. Source code in ruptures/base.py @abc . abstractmethod def fit ( self , * args , ** kwargs ): \"\"\"Set the parameters of the cost function, for instance the Gram matrix, etc.\"\"\" pass","title":"fit()"},{"location":"code-reference/base-reference/#ruptures.base.BaseCost.sum_of_costs","text":"Returns the sum of segments cost for the given segmentation. Parameters: Name Type Description Default bkps list list of change points. By convention, bkps[-1]==n_samples. required Returns: Type Description float sum of costs Source code in ruptures/base.py def sum_of_costs ( self , bkps ): \"\"\"Returns the sum of segments cost for the given segmentation. Args: bkps (list): list of change points. By convention, bkps[-1]==n_samples. Returns: float: sum of costs \"\"\" soc = sum ( self . error ( start , end ) for start , end in pairwise ([ 0 ] + bkps )) return soc","title":"sum_of_costs()"},{"location":"code-reference/base-reference/#ruptures.base.BaseEstimator","text":"Base class for all change point detection estimators. Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"BaseEstimator"},{"location":"code-reference/base-reference/#ruptures.base.BaseEstimator.fit","text":"To call the segmentation algorithm. Source code in ruptures/base.py @abc . abstractmethod def fit ( self , * args , ** kwargs ): \"\"\"To call the segmentation algorithm.\"\"\" pass","title":"fit()"},{"location":"code-reference/base-reference/#ruptures.base.BaseEstimator.fit_predict","text":"To call the segmentation algorithm. Source code in ruptures/base.py @abc . abstractmethod def fit_predict ( self , * args , ** kwargs ): \"\"\"To call the segmentation algorithm.\"\"\" pass","title":"fit_predict()"},{"location":"code-reference/base-reference/#ruptures.base.BaseEstimator.predict","text":"To call the segmentation algorithm. Source code in ruptures/base.py @abc . abstractmethod def predict ( self , * args , ** kwargs ): \"\"\"To call the segmentation algorithm.\"\"\" pass","title":"predict()"},{"location":"code-reference/costs/costautoregressive-reference/","text":"Autoregressive model change (CostAutoregressive) # ruptures.costs.costautoregressive # CostAR # Least-squares estimate for changes in autoregressive coefficients. __init__ ( self , order = 4 ) special # Initialize the object. Parameters: Name Type Description Default order int autoregressive order 4 Source code in ruptures/costs/costautoregressive.py def __init__ ( self , order = 4 ): \"\"\"Initialize the object. Args: order (int): autoregressive order \"\"\" self . signal = None self . covar = None self . min_size = max ( 5 , order + 1 ) self . order = order error ( self , start , end ) # Return the approximation cost on the segment [start:end]. Parameters: Name Type Description Default start int start of the segment required end int end of the segment required Returns: Type Description float segment cost Exceptions: Type Description NotEnoughPoints when the segment is too short (less than 'min_size' samples). Source code in ruptures/costs/costautoregressive.py def error ( self , start , end ): \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: float: segment cost Raises: NotEnoughPoints: when the segment is too short (less than ``'min_size'`` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints y , X = self . signal [ start : end ], self . covar [ start : end ] _ , residual , _ , _ = lstsq ( X , y , rcond = None ) return residual . sum () fit ( self , signal ) # Set parameters of the instance. The signal must be 1D. Parameters: Name Type Description Default signal array 1d signal. Shape (n_samples, 1) or (n_samples,). required Returns: Type Description self the current object Source code in ruptures/costs/costautoregressive.py def fit ( self , signal ): \"\"\"Set parameters of the instance. The signal must be 1D. Args: signal (array): 1d signal. Shape (n_samples, 1) or (n_samples,). Returns: self: the current object \"\"\" self . signal = deepcopy ( signal ) if signal . ndim == 1 : self . signal = self . signal . reshape ( - 1 , 1 ) # lagged covariates n_samples , _ = self . signal . shape strides = ( self . signal . itemsize , self . signal . itemsize ) shape = ( n_samples - self . order , self . order ) lagged = as_strided ( self . signal , shape = shape , strides = strides ) # pad the first columns lagged_after_padding = np . pad ( lagged , (( self . order , 0 ), ( 0 , 0 )), mode = \"edge\" ) # add intercept self . covar = np . c_ [ lagged_after_padding , np . ones ( n_samples )] # pad signal on the edges self . signal [: self . order ] = self . signal [ self . order ] return self","title":"CostAR"},{"location":"code-reference/costs/costautoregressive-reference/#autoregressive-model-change-costautoregressive","text":"","title":"Autoregressive model change (CostAutoregressive)"},{"location":"code-reference/costs/costautoregressive-reference/#ruptures.costs.costautoregressive","text":"","title":"costautoregressive"},{"location":"code-reference/costs/costautoregressive-reference/#ruptures.costs.costautoregressive.CostAR","text":"Least-squares estimate for changes in autoregressive coefficients.","title":"CostAR"},{"location":"code-reference/costs/costautoregressive-reference/#ruptures.costs.costautoregressive.CostAR.__init__","text":"Initialize the object. Parameters: Name Type Description Default order int autoregressive order 4 Source code in ruptures/costs/costautoregressive.py def __init__ ( self , order = 4 ): \"\"\"Initialize the object. Args: order (int): autoregressive order \"\"\" self . signal = None self . covar = None self . min_size = max ( 5 , order + 1 ) self . order = order","title":"__init__()"},{"location":"code-reference/costs/costautoregressive-reference/#ruptures.costs.costautoregressive.CostAR.error","text":"Return the approximation cost on the segment [start:end]. Parameters: Name Type Description Default start int start of the segment required end int end of the segment required Returns: Type Description float segment cost Exceptions: Type Description NotEnoughPoints when the segment is too short (less than 'min_size' samples). Source code in ruptures/costs/costautoregressive.py def error ( self , start , end ): \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: float: segment cost Raises: NotEnoughPoints: when the segment is too short (less than ``'min_size'`` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints y , X = self . signal [ start : end ], self . covar [ start : end ] _ , residual , _ , _ = lstsq ( X , y , rcond = None ) return residual . sum ()","title":"error()"},{"location":"code-reference/costs/costautoregressive-reference/#ruptures.costs.costautoregressive.CostAR.fit","text":"Set parameters of the instance. The signal must be 1D. Parameters: Name Type Description Default signal array 1d signal. Shape (n_samples, 1) or (n_samples,). required Returns: Type Description self the current object Source code in ruptures/costs/costautoregressive.py def fit ( self , signal ): \"\"\"Set parameters of the instance. The signal must be 1D. Args: signal (array): 1d signal. Shape (n_samples, 1) or (n_samples,). Returns: self: the current object \"\"\" self . signal = deepcopy ( signal ) if signal . ndim == 1 : self . signal = self . signal . reshape ( - 1 , 1 ) # lagged covariates n_samples , _ = self . signal . shape strides = ( self . signal . itemsize , self . signal . itemsize ) shape = ( n_samples - self . order , self . order ) lagged = as_strided ( self . signal , shape = shape , strides = strides ) # pad the first columns lagged_after_padding = np . pad ( lagged , (( self . order , 0 ), ( 0 , 0 )), mode = \"edge\" ) # add intercept self . covar = np . c_ [ lagged_after_padding , np . ones ( n_samples )] # pad signal on the edges self . signal [: self . order ] = self . signal [ self . order ] return self","title":"fit()"},{"location":"code-reference/costs/costclinear-reference/","text":"Continuous linear change (CostCLinear) # ruptures.costs.costclinear.CostCLinear # Piecewise linear approximation with a continuity constraint. __init__ ( self ) special # Initialize the object. Source code in ruptures/costs/costclinear.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" self . signal = None self . min_size = 3 error ( self , start , end ) # Return the approximation cost on the segment [start:end]. Parameters: Name Type Description Default start int start of the segment required end int end of the segment required Returns: Type Description float segment cost (float) Exceptions: Type Description NotEnoughPoints when the segment is too short (less than min_size samples). Source code in ruptures/costs/costclinear.py def error ( self , start , end ) -> float : \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: segment cost (float) Raises: NotEnoughPoints: when the segment is too short (less than `min_size` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints if start == 0 : start = 1 sub = self . signal [ start : end ] slope = ( self . signal [ end - 1 ] - self . signal [ start - 1 ]) / ( end - start ) intercept = self . signal [ start - 1 ] approx = slope . reshape ( - 1 , 1 ) * np . arange ( 1 , end - start + 1 ) + intercept . reshape ( - 1 , 1 ) return np . sum (( sub - approx . transpose ()) ** 2 ) fit ( self , signal ) # Set parameters of the instance. Parameters: Name Type Description Default signal array signal of shape (n_samples, n_dims) or (n_samples,) required Returns: Type Description CostCLinear self Source code in ruptures/costs/costclinear.py def fit ( self , signal ) -> \"CostCLinear\" : \"\"\"Set parameters of the instance. Args: signal (array): signal of shape (n_samples, n_dims) or (n_samples,) Returns: self \"\"\" if signal . ndim == 1 : self . signal = signal . reshape ( - 1 , 1 ) else : self . signal = signal return self","title":"CostCLinear"},{"location":"code-reference/costs/costclinear-reference/#continuous-linear-change-costclinear","text":"","title":"Continuous linear change (CostCLinear)"},{"location":"code-reference/costs/costclinear-reference/#ruptures.costs.costclinear.CostCLinear","text":"Piecewise linear approximation with a continuity constraint.","title":"CostCLinear"},{"location":"code-reference/costs/costclinear-reference/#ruptures.costs.costclinear.CostCLinear.__init__","text":"Initialize the object. Source code in ruptures/costs/costclinear.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" self . signal = None self . min_size = 3","title":"__init__()"},{"location":"code-reference/costs/costclinear-reference/#ruptures.costs.costclinear.CostCLinear.error","text":"Return the approximation cost on the segment [start:end]. Parameters: Name Type Description Default start int start of the segment required end int end of the segment required Returns: Type Description float segment cost (float) Exceptions: Type Description NotEnoughPoints when the segment is too short (less than min_size samples). Source code in ruptures/costs/costclinear.py def error ( self , start , end ) -> float : \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: segment cost (float) Raises: NotEnoughPoints: when the segment is too short (less than `min_size` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints if start == 0 : start = 1 sub = self . signal [ start : end ] slope = ( self . signal [ end - 1 ] - self . signal [ start - 1 ]) / ( end - start ) intercept = self . signal [ start - 1 ] approx = slope . reshape ( - 1 , 1 ) * np . arange ( 1 , end - start + 1 ) + intercept . reshape ( - 1 , 1 ) return np . sum (( sub - approx . transpose ()) ** 2 )","title":"error()"},{"location":"code-reference/costs/costclinear-reference/#ruptures.costs.costclinear.CostCLinear.fit","text":"Set parameters of the instance. Parameters: Name Type Description Default signal array signal of shape (n_samples, n_dims) or (n_samples,) required Returns: Type Description CostCLinear self Source code in ruptures/costs/costclinear.py def fit ( self , signal ) -> \"CostCLinear\" : \"\"\"Set parameters of the instance. Args: signal (array): signal of shape (n_samples, n_dims) or (n_samples,) Returns: self \"\"\" if signal . ndim == 1 : self . signal = signal . reshape ( - 1 , 1 ) else : self . signal = signal return self","title":"fit()"},{"location":"code-reference/costs/costcosine-reference/","text":"Kernelized mean change (CostCosine) # ruptures.costs.costcosine.CostCosine # Kernel change point detection with the cosine similarity. gram property readonly # Generate the gram matrix (lazy loading). Only access this function after a .fit() (otherwise self.signal is not defined). __init__ ( self ) special # Initialize the object. Source code in ruptures/costs/costcosine.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" self . signal = None self . min_size = 1 self . _gram = None error ( self , start , end ) # Return the approximation cost on the segment [start:end]. Parameters: Name Type Description Default start int start of the segment required end int end of the segment required Returns: Type Description float segment cost Exceptions: Type Description NotEnoughPoints when the segment is too short (less than min_size samples). Source code in ruptures/costs/costcosine.py def error ( self , start , end ) -> float : \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: segment cost Raises: NotEnoughPoints: when the segment is too short (less than `min_size` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints sub_gram = self . gram [ start : end , start : end ] val = np . diagonal ( sub_gram ) . sum () val -= sub_gram . sum () / ( end - start ) return val fit ( self , signal ) # Set parameters of the instance. Parameters: Name Type Description Default signal array array of shape (n_samples,) or (n_samples, n_features) required Returns: Type Description CostCosine self Source code in ruptures/costs/costcosine.py def fit ( self , signal ) -> \"CostCosine\" : \"\"\"Set parameters of the instance. Args: signal (array): array of shape (n_samples,) or (n_samples, n_features) Returns: self \"\"\" if signal . ndim == 1 : self . signal = signal . reshape ( - 1 , 1 ) else : self . signal = signal return self","title":"CostCosine"},{"location":"code-reference/costs/costcosine-reference/#kernelized-mean-change-costcosine","text":"","title":"Kernelized mean change (CostCosine)"},{"location":"code-reference/costs/costcosine-reference/#ruptures.costs.costcosine.CostCosine","text":"Kernel change point detection with the cosine similarity.","title":"CostCosine"},{"location":"code-reference/costs/costcosine-reference/#ruptures.costs.costcosine.CostCosine.gram","text":"Generate the gram matrix (lazy loading). Only access this function after a .fit() (otherwise self.signal is not defined).","title":"gram"},{"location":"code-reference/costs/costcosine-reference/#ruptures.costs.costcosine.CostCosine.__init__","text":"Initialize the object. Source code in ruptures/costs/costcosine.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" self . signal = None self . min_size = 1 self . _gram = None","title":"__init__()"},{"location":"code-reference/costs/costcosine-reference/#ruptures.costs.costcosine.CostCosine.error","text":"Return the approximation cost on the segment [start:end]. Parameters: Name Type Description Default start int start of the segment required end int end of the segment required Returns: Type Description float segment cost Exceptions: Type Description NotEnoughPoints when the segment is too short (less than min_size samples). Source code in ruptures/costs/costcosine.py def error ( self , start , end ) -> float : \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: segment cost Raises: NotEnoughPoints: when the segment is too short (less than `min_size` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints sub_gram = self . gram [ start : end , start : end ] val = np . diagonal ( sub_gram ) . sum () val -= sub_gram . sum () / ( end - start ) return val","title":"error()"},{"location":"code-reference/costs/costcosine-reference/#ruptures.costs.costcosine.CostCosine.fit","text":"Set parameters of the instance. Parameters: Name Type Description Default signal array array of shape (n_samples,) or (n_samples, n_features) required Returns: Type Description CostCosine self Source code in ruptures/costs/costcosine.py def fit ( self , signal ) -> \"CostCosine\" : \"\"\"Set parameters of the instance. Args: signal (array): array of shape (n_samples,) or (n_samples, n_features) Returns: self \"\"\" if signal . ndim == 1 : self . signal = signal . reshape ( - 1 , 1 ) else : self . signal = signal return self","title":"fit()"},{"location":"code-reference/costs/costl1-reference/","text":"CostL1 (least absolute deviation) # ruptures.costs.costl1.CostL1 # Least absolute deviation. __init__ ( self ) special # Initialize the object. Source code in ruptures/costs/costl1.py def __init__ ( self ) -> None : \"\"\"Initialize the object.\"\"\" self . signal = None self . min_size = 2 error ( self , start , end ) # Return the approximation cost on the segment [start:end]. Parameters: Name Type Description Default start int start of the segment required end int end of the segment required Returns: Type Description float segment cost Exceptions: Type Description NotEnoughPoints when the segment is too short (less than min_size samples). Source code in ruptures/costs/costl1.py def error ( self , start , end ) -> float : \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: segment cost Raises: NotEnoughPoints: when the segment is too short (less than `min_size` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints sub = self . signal [ start : end ] med = np . median ( sub , axis = 0 ) return abs ( sub - med ) . sum () fit ( self , signal ) # Set parameters of the instance. Parameters: Name Type Description Default signal array signal. Shape (n_samples,) or (n_samples, n_features) required Returns: Type Description CostL1 self Source code in ruptures/costs/costl1.py def fit ( self , signal ) -> \"CostL1\" : \"\"\"Set parameters of the instance. Args: signal (array): signal. Shape (n_samples,) or (n_samples, n_features) Returns: self \"\"\" if signal . ndim == 1 : self . signal = signal . reshape ( - 1 , 1 ) else : self . signal = signal return self","title":"CostL1"},{"location":"code-reference/costs/costl1-reference/#costl1-least-absolute-deviation","text":"","title":"CostL1 (least absolute deviation)"},{"location":"code-reference/costs/costl1-reference/#ruptures.costs.costl1.CostL1","text":"Least absolute deviation.","title":"CostL1"},{"location":"code-reference/costs/costl1-reference/#ruptures.costs.costl1.CostL1.__init__","text":"Initialize the object. Source code in ruptures/costs/costl1.py def __init__ ( self ) -> None : \"\"\"Initialize the object.\"\"\" self . signal = None self . min_size = 2","title":"__init__()"},{"location":"code-reference/costs/costl1-reference/#ruptures.costs.costl1.CostL1.error","text":"Return the approximation cost on the segment [start:end]. Parameters: Name Type Description Default start int start of the segment required end int end of the segment required Returns: Type Description float segment cost Exceptions: Type Description NotEnoughPoints when the segment is too short (less than min_size samples). Source code in ruptures/costs/costl1.py def error ( self , start , end ) -> float : \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: segment cost Raises: NotEnoughPoints: when the segment is too short (less than `min_size` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints sub = self . signal [ start : end ] med = np . median ( sub , axis = 0 ) return abs ( sub - med ) . sum ()","title":"error()"},{"location":"code-reference/costs/costl1-reference/#ruptures.costs.costl1.CostL1.fit","text":"Set parameters of the instance. Parameters: Name Type Description Default signal array signal. Shape (n_samples,) or (n_samples, n_features) required Returns: Type Description CostL1 self Source code in ruptures/costs/costl1.py def fit ( self , signal ) -> \"CostL1\" : \"\"\"Set parameters of the instance. Args: signal (array): signal. Shape (n_samples,) or (n_samples, n_features) Returns: self \"\"\" if signal . ndim == 1 : self . signal = signal . reshape ( - 1 , 1 ) else : self . signal = signal return self","title":"fit()"},{"location":"code-reference/costs/costl2-reference/","text":"CostL2 (least squared deviation) # ruptures.costs.costl2.CostL2 # Least squared deviation. __init__ ( self ) special # Initialize the object. Source code in ruptures/costs/costl2.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" self . signal = None self . min_size = 2 error ( self , start , end ) # Return the approximation cost on the segment [start:end]. Parameters: Name Type Description Default start int start of the segment required end int end of the segment required Returns: Type Description float segment cost Exceptions: Type Description NotEnoughPoints when the segment is too short (less than min_size samples). Source code in ruptures/costs/costl2.py def error ( self , start , end ) -> float : \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: segment cost Raises: NotEnoughPoints: when the segment is too short (less than `min_size` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints return self . signal [ start : end ] . var ( axis = 0 ) . sum () * ( end - start ) fit ( self , signal ) # Set parameters of the instance. Parameters: Name Type Description Default signal array array of shape (n_samples,) or (n_samples, n_features) required Returns: Type Description CostL2 self Source code in ruptures/costs/costl2.py def fit ( self , signal ) -> \"CostL2\" : \"\"\"Set parameters of the instance. Args: signal (array): array of shape (n_samples,) or (n_samples, n_features) Returns: self \"\"\" if signal . ndim == 1 : self . signal = signal . reshape ( - 1 , 1 ) else : self . signal = signal return self","title":"CostL2"},{"location":"code-reference/costs/costl2-reference/#costl2-least-squared-deviation","text":"","title":"CostL2 (least squared deviation)"},{"location":"code-reference/costs/costl2-reference/#ruptures.costs.costl2.CostL2","text":"Least squared deviation.","title":"CostL2"},{"location":"code-reference/costs/costl2-reference/#ruptures.costs.costl2.CostL2.__init__","text":"Initialize the object. Source code in ruptures/costs/costl2.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" self . signal = None self . min_size = 2","title":"__init__()"},{"location":"code-reference/costs/costl2-reference/#ruptures.costs.costl2.CostL2.error","text":"Return the approximation cost on the segment [start:end]. Parameters: Name Type Description Default start int start of the segment required end int end of the segment required Returns: Type Description float segment cost Exceptions: Type Description NotEnoughPoints when the segment is too short (less than min_size samples). Source code in ruptures/costs/costl2.py def error ( self , start , end ) -> float : \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: segment cost Raises: NotEnoughPoints: when the segment is too short (less than `min_size` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints return self . signal [ start : end ] . var ( axis = 0 ) . sum () * ( end - start )","title":"error()"},{"location":"code-reference/costs/costl2-reference/#ruptures.costs.costl2.CostL2.fit","text":"Set parameters of the instance. Parameters: Name Type Description Default signal array array of shape (n_samples,) or (n_samples, n_features) required Returns: Type Description CostL2 self Source code in ruptures/costs/costl2.py def fit ( self , signal ) -> \"CostL2\" : \"\"\"Set parameters of the instance. Args: signal (array): array of shape (n_samples,) or (n_samples, n_features) Returns: self \"\"\" if signal . ndim == 1 : self . signal = signal . reshape ( - 1 , 1 ) else : self . signal = signal return self","title":"fit()"},{"location":"code-reference/costs/costlinear-reference/","text":"Linear model change (CostLinear) # ruptures.costs.costlinear.CostLinear # Least-square estimate for linear changes. __init__ ( self ) special # Initialize the object. Source code in ruptures/costs/costlinear.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" self . signal = None self . covar = None self . min_size = 2 error ( self , start , end ) # Return the approximation cost on the segment [start:end]. Parameters: Name Type Description Default start int start of the segment required end int end of the segment required Returns: Type Description float segment cost Exceptions: Type Description NotEnoughPoints when the segment is too short (less than min_size samples). Source code in ruptures/costs/costlinear.py def error ( self , start , end ) -> float : \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: segment cost Raises: NotEnoughPoints: when the segment is too short (less than `min_size` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints y , X = self . signal [ start : end ], self . covar [ start : end ] _ , residual , _ , _ = lstsq ( X , y , rcond = None ) return residual . sum () fit ( self , signal ) # Set parameters of the instance. The first column contains the observed variable. The other columns contains the covariates. Parameters: Name Type Description Default signal array signal of shape (n_samples, n_regressors+1) required Returns: Type Description CostLinear self Source code in ruptures/costs/costlinear.py def fit ( self , signal ) -> \"CostLinear\" : \"\"\"Set parameters of the instance. The first column contains the observed variable. The other columns contains the covariates. Args: signal (array): signal of shape (n_samples, n_regressors+1) Returns: self \"\"\" assert signal . ndim > 1 , \"Not enough dimensions\" self . signal = signal [:, 0 ] . reshape ( - 1 , 1 ) self . covar = signal [:, 1 :] return self","title":"CostLinear"},{"location":"code-reference/costs/costlinear-reference/#linear-model-change-costlinear","text":"","title":"Linear model change (CostLinear)"},{"location":"code-reference/costs/costlinear-reference/#ruptures.costs.costlinear.CostLinear","text":"Least-square estimate for linear changes.","title":"CostLinear"},{"location":"code-reference/costs/costlinear-reference/#ruptures.costs.costlinear.CostLinear.__init__","text":"Initialize the object. Source code in ruptures/costs/costlinear.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" self . signal = None self . covar = None self . min_size = 2","title":"__init__()"},{"location":"code-reference/costs/costlinear-reference/#ruptures.costs.costlinear.CostLinear.error","text":"Return the approximation cost on the segment [start:end]. Parameters: Name Type Description Default start int start of the segment required end int end of the segment required Returns: Type Description float segment cost Exceptions: Type Description NotEnoughPoints when the segment is too short (less than min_size samples). Source code in ruptures/costs/costlinear.py def error ( self , start , end ) -> float : \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: segment cost Raises: NotEnoughPoints: when the segment is too short (less than `min_size` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints y , X = self . signal [ start : end ], self . covar [ start : end ] _ , residual , _ , _ = lstsq ( X , y , rcond = None ) return residual . sum ()","title":"error()"},{"location":"code-reference/costs/costlinear-reference/#ruptures.costs.costlinear.CostLinear.fit","text":"Set parameters of the instance. The first column contains the observed variable. The other columns contains the covariates. Parameters: Name Type Description Default signal array signal of shape (n_samples, n_regressors+1) required Returns: Type Description CostLinear self Source code in ruptures/costs/costlinear.py def fit ( self , signal ) -> \"CostLinear\" : \"\"\"Set parameters of the instance. The first column contains the observed variable. The other columns contains the covariates. Args: signal (array): signal of shape (n_samples, n_regressors+1) Returns: self \"\"\" assert signal . ndim > 1 , \"Not enough dimensions\" self . signal = signal [:, 0 ] . reshape ( - 1 , 1 ) self . covar = signal [:, 1 :] return self","title":"fit()"},{"location":"code-reference/costs/costml-reference/","text":"Mahalanobis-type change (CostMl) # ruptures.costs.costml.CostMl # Mahalanobis-type cost function. __init__ ( self , metric = None ) special # Create a new instance. Parameters: Name Type Description Default metric ndarray PSD matrix that defines a Mahalanobis-type pseudo distance. If None, defaults to the Mahalanobis matrix. Shape (n_features, n_features). None Source code in ruptures/costs/costml.py def __init__ ( self , metric = None ): \"\"\"Create a new instance. Args: metric (ndarray, optional): PSD matrix that defines a Mahalanobis-type pseudo distance. If None, defaults to the Mahalanobis matrix. Shape (n_features, n_features). \"\"\" self . metric = metric # metric matrix self . has_custom_metric = False if self . metric is None else True self . gram = None self . min_size = 2 error ( self , start , end ) # Return the approximation cost on the segment [start:end]. Parameters: Name Type Description Default start int start of the segment required end int end of the segment required Returns: Type Description float segment cost Exceptions: Type Description NotEnoughPoints when the segment is too short (less than 'min_size' samples). Source code in ruptures/costs/costml.py def error ( self , start , end ): \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: float: segment cost Raises: NotEnoughPoints: when the segment is too short (less than ``'min_size'`` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints sub_gram = self . gram [ start : end , start : end ] val = np . diagonal ( sub_gram ) . sum () val -= sub_gram . sum () / ( end - start ) return val fit ( self , signal ) # Set parameters of the instance. Parameters: Name Type Description Default signal array signal. Shape (n_samples,) or (n_samples, n_features) required Returns: Type Description CostMl self Source code in ruptures/costs/costml.py def fit ( self , signal ) -> \"CostMl\" : \"\"\"Set parameters of the instance. Args: signal (array): signal. Shape (n_samples,) or (n_samples, n_features) Returns: self \"\"\" s_ = signal . reshape ( - 1 , 1 ) if signal . ndim == 1 else signal # fit a Mahalanobis metric if self.has_custom_metric is False if self . has_custom_metric is False : covar = np . cov ( s_ . T ) self . metric = inv ( covar . reshape ( 1 , 1 ) if covar . size == 1 else covar ) self . gram = s_ . dot ( self . metric ) . dot ( s_ . T ) self . signal = s_ return self","title":"CostMl"},{"location":"code-reference/costs/costml-reference/#mahalanobis-type-change-costml","text":"","title":"Mahalanobis-type change (CostMl)"},{"location":"code-reference/costs/costml-reference/#ruptures.costs.costml.CostMl","text":"Mahalanobis-type cost function.","title":"CostMl"},{"location":"code-reference/costs/costml-reference/#ruptures.costs.costml.CostMl.__init__","text":"Create a new instance. Parameters: Name Type Description Default metric ndarray PSD matrix that defines a Mahalanobis-type pseudo distance. If None, defaults to the Mahalanobis matrix. Shape (n_features, n_features). None Source code in ruptures/costs/costml.py def __init__ ( self , metric = None ): \"\"\"Create a new instance. Args: metric (ndarray, optional): PSD matrix that defines a Mahalanobis-type pseudo distance. If None, defaults to the Mahalanobis matrix. Shape (n_features, n_features). \"\"\" self . metric = metric # metric matrix self . has_custom_metric = False if self . metric is None else True self . gram = None self . min_size = 2","title":"__init__()"},{"location":"code-reference/costs/costml-reference/#ruptures.costs.costml.CostMl.error","text":"Return the approximation cost on the segment [start:end]. Parameters: Name Type Description Default start int start of the segment required end int end of the segment required Returns: Type Description float segment cost Exceptions: Type Description NotEnoughPoints when the segment is too short (less than 'min_size' samples). Source code in ruptures/costs/costml.py def error ( self , start , end ): \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: float: segment cost Raises: NotEnoughPoints: when the segment is too short (less than ``'min_size'`` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints sub_gram = self . gram [ start : end , start : end ] val = np . diagonal ( sub_gram ) . sum () val -= sub_gram . sum () / ( end - start ) return val","title":"error()"},{"location":"code-reference/costs/costml-reference/#ruptures.costs.costml.CostMl.fit","text":"Set parameters of the instance. Parameters: Name Type Description Default signal array signal. Shape (n_samples,) or (n_samples, n_features) required Returns: Type Description CostMl self Source code in ruptures/costs/costml.py def fit ( self , signal ) -> \"CostMl\" : \"\"\"Set parameters of the instance. Args: signal (array): signal. Shape (n_samples,) or (n_samples, n_features) Returns: self \"\"\" s_ = signal . reshape ( - 1 , 1 ) if signal . ndim == 1 else signal # fit a Mahalanobis metric if self.has_custom_metric is False if self . has_custom_metric is False : covar = np . cov ( s_ . T ) self . metric = inv ( covar . reshape ( 1 , 1 ) if covar . size == 1 else covar ) self . gram = s_ . dot ( self . metric ) . dot ( s_ . T ) self . signal = s_ return self","title":"fit()"},{"location":"code-reference/costs/costnormal-reference/","text":"Gaussian process change (CostNormal) # ruptures.costs.costnormal.CostNormal # Gaussian process change. __init__ ( self , add_small_diag = True ) special # Initialize the object. Parameters: Name Type Description Default add_small_diag bool For signals with truly constant segments, the covariance matrix is badly conditioned, so we add a small diagonal matrix. Defaults to True. True Source code in ruptures/costs/costnormal.py def __init__ ( self , add_small_diag = True ): \"\"\"Initialize the object. Args: add_small_diag (bool, optional): For signals with truly constant segments, the covariance matrix is badly conditioned, so we add a small diagonal matrix. Defaults to True. \"\"\" self . signal = None self . min_size = 2 self . add_small_diag = add_small_diag if add_small_diag : warnings . warn ( \"New behaviour in v1.1.5: \" \"a small bias is added to the covariance matrix to cope with truly \" \"constant segments (see PR#198).\" , UserWarning , ) error ( self , start , end ) # Return the approximation cost on the segment [start:end]. Parameters: Name Type Description Default start int start of the segment required end int end of the segment required Returns: Type Description float segment cost Exceptions: Type Description NotEnoughPoints when the segment is too short (less than min_size samples). Source code in ruptures/costs/costnormal.py def error ( self , start , end ) -> float : \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: segment cost Raises: NotEnoughPoints: when the segment is too short (less than `min_size` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints sub = self . signal [ start : end ] if self . signal . shape [ 1 ] > 1 : cov = np . cov ( sub . T ) else : cov = np . array ([[ sub . var ()]]) if self . add_small_diag : # adding small bias cov += 1e-6 * np . eye ( self . n_dims ) _ , val = slogdet ( cov ) return val * ( end - start ) fit ( self , signal ) # Set parameters of the instance. Parameters: Name Type Description Default signal array signal of shape (n_samples,) or (n_samples, n_features) required Returns: Type Description CostNormal self Source code in ruptures/costs/costnormal.py def fit ( self , signal ) -> \"CostNormal\" : \"\"\"Set parameters of the instance. Args: signal (array): signal of shape (n_samples,) or (n_samples, n_features) Returns: self \"\"\" if signal . ndim == 1 : self . signal = signal . reshape ( - 1 , 1 ) else : self . signal = signal self . n_samples , self . n_dims = self . signal . shape return self","title":"CostNormal"},{"location":"code-reference/costs/costnormal-reference/#gaussian-process-change-costnormal","text":"","title":"Gaussian process change (CostNormal)"},{"location":"code-reference/costs/costnormal-reference/#ruptures.costs.costnormal.CostNormal","text":"Gaussian process change.","title":"CostNormal"},{"location":"code-reference/costs/costnormal-reference/#ruptures.costs.costnormal.CostNormal.__init__","text":"Initialize the object. Parameters: Name Type Description Default add_small_diag bool For signals with truly constant segments, the covariance matrix is badly conditioned, so we add a small diagonal matrix. Defaults to True. True Source code in ruptures/costs/costnormal.py def __init__ ( self , add_small_diag = True ): \"\"\"Initialize the object. Args: add_small_diag (bool, optional): For signals with truly constant segments, the covariance matrix is badly conditioned, so we add a small diagonal matrix. Defaults to True. \"\"\" self . signal = None self . min_size = 2 self . add_small_diag = add_small_diag if add_small_diag : warnings . warn ( \"New behaviour in v1.1.5: \" \"a small bias is added to the covariance matrix to cope with truly \" \"constant segments (see PR#198).\" , UserWarning , )","title":"__init__()"},{"location":"code-reference/costs/costnormal-reference/#ruptures.costs.costnormal.CostNormal.error","text":"Return the approximation cost on the segment [start:end]. Parameters: Name Type Description Default start int start of the segment required end int end of the segment required Returns: Type Description float segment cost Exceptions: Type Description NotEnoughPoints when the segment is too short (less than min_size samples). Source code in ruptures/costs/costnormal.py def error ( self , start , end ) -> float : \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: segment cost Raises: NotEnoughPoints: when the segment is too short (less than `min_size` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints sub = self . signal [ start : end ] if self . signal . shape [ 1 ] > 1 : cov = np . cov ( sub . T ) else : cov = np . array ([[ sub . var ()]]) if self . add_small_diag : # adding small bias cov += 1e-6 * np . eye ( self . n_dims ) _ , val = slogdet ( cov ) return val * ( end - start )","title":"error()"},{"location":"code-reference/costs/costnormal-reference/#ruptures.costs.costnormal.CostNormal.fit","text":"Set parameters of the instance. Parameters: Name Type Description Default signal array signal of shape (n_samples,) or (n_samples, n_features) required Returns: Type Description CostNormal self Source code in ruptures/costs/costnormal.py def fit ( self , signal ) -> \"CostNormal\" : \"\"\"Set parameters of the instance. Args: signal (array): signal of shape (n_samples,) or (n_samples, n_features) Returns: self \"\"\" if signal . ndim == 1 : self . signal = signal . reshape ( - 1 , 1 ) else : self . signal = signal self . n_samples , self . n_dims = self . signal . shape return self","title":"fit()"},{"location":"code-reference/costs/costrank-reference/","text":"Rank-based change (CostRank) # ruptures.costs.costrank.CostRank # Rank-based cost function __init__ ( self ) special # Initialize the object. Source code in ruptures/costs/costrank.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" self . inv_cov = None self . ranks = None self . min_size = 2 error ( self , start , end ) # Return the approximation cost on the segment [start:end]. Parameters: Name Type Description Default start int start of the segment required end int end of the segment required Returns: Type Description float segment cost Exceptions: Type Description NotEnoughPoints when the segment is too short (less than min_size samples). Source code in ruptures/costs/costrank.py def error ( self , start , end ): \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: float: segment cost Raises: NotEnoughPoints: when the segment is too short (less than `min_size` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints mean = np . reshape ( np . mean ( self . ranks [ start : end ], axis = 0 ), ( - 1 , 1 )) return - ( end - start ) * mean . T @ self . inv_cov @ mean fit ( self , signal ) # Set parameters of the instance. Parameters: Name Type Description Default signal array signal. Shape (n_samples,) or (n_samples, n_features) required Returns: Type Description CostRank self Source code in ruptures/costs/costrank.py def fit ( self , signal ) -> \"CostRank\" : \"\"\"Set parameters of the instance. Args: signal (array): signal. Shape (n_samples,) or (n_samples, n_features) Returns: self \"\"\" if signal . ndim == 1 : signal = signal . reshape ( - 1 , 1 ) obs , vars = signal . shape # Convert signal data into ranks in the range [1, n] ranks = rankdata ( signal , axis = 0 ) # Center the ranks into the range [-(n+1)/2, (n+1)/2] centered_ranks = ranks - (( obs + 1 ) / 2 ) # Sigma is the covariance of these ranks. # If it's a scalar, reshape it into a 1x1 matrix cov = np . cov ( centered_ranks , rowvar = False , bias = True ) . reshape ( vars , vars ) # Use the pseudoinverse to handle linear dependencies # see Lung-Yut-Fong, A., L\u00e9vy-Leduc, C., & Capp\u00e9, O. (2015) try : self . inv_cov = pinv ( cov ) except LinAlgError as e : raise LinAlgError ( \"The covariance matrix of the rank signal is not invertible and the \" \"pseudo-inverse computation did not converge.\" ) from e self . ranks = centered_ranks self . signal = signal return self","title":"CostRank"},{"location":"code-reference/costs/costrank-reference/#rank-based-change-costrank","text":"","title":"Rank-based change (CostRank)"},{"location":"code-reference/costs/costrank-reference/#ruptures.costs.costrank.CostRank","text":"Rank-based cost function","title":"CostRank"},{"location":"code-reference/costs/costrank-reference/#ruptures.costs.costrank.CostRank.__init__","text":"Initialize the object. Source code in ruptures/costs/costrank.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" self . inv_cov = None self . ranks = None self . min_size = 2","title":"__init__()"},{"location":"code-reference/costs/costrank-reference/#ruptures.costs.costrank.CostRank.error","text":"Return the approximation cost on the segment [start:end]. Parameters: Name Type Description Default start int start of the segment required end int end of the segment required Returns: Type Description float segment cost Exceptions: Type Description NotEnoughPoints when the segment is too short (less than min_size samples). Source code in ruptures/costs/costrank.py def error ( self , start , end ): \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: float: segment cost Raises: NotEnoughPoints: when the segment is too short (less than `min_size` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints mean = np . reshape ( np . mean ( self . ranks [ start : end ], axis = 0 ), ( - 1 , 1 )) return - ( end - start ) * mean . T @ self . inv_cov @ mean","title":"error()"},{"location":"code-reference/costs/costrank-reference/#ruptures.costs.costrank.CostRank.fit","text":"Set parameters of the instance. Parameters: Name Type Description Default signal array signal. Shape (n_samples,) or (n_samples, n_features) required Returns: Type Description CostRank self Source code in ruptures/costs/costrank.py def fit ( self , signal ) -> \"CostRank\" : \"\"\"Set parameters of the instance. Args: signal (array): signal. Shape (n_samples,) or (n_samples, n_features) Returns: self \"\"\" if signal . ndim == 1 : signal = signal . reshape ( - 1 , 1 ) obs , vars = signal . shape # Convert signal data into ranks in the range [1, n] ranks = rankdata ( signal , axis = 0 ) # Center the ranks into the range [-(n+1)/2, (n+1)/2] centered_ranks = ranks - (( obs + 1 ) / 2 ) # Sigma is the covariance of these ranks. # If it's a scalar, reshape it into a 1x1 matrix cov = np . cov ( centered_ranks , rowvar = False , bias = True ) . reshape ( vars , vars ) # Use the pseudoinverse to handle linear dependencies # see Lung-Yut-Fong, A., L\u00e9vy-Leduc, C., & Capp\u00e9, O. (2015) try : self . inv_cov = pinv ( cov ) except LinAlgError as e : raise LinAlgError ( \"The covariance matrix of the rank signal is not invertible and the \" \"pseudo-inverse computation did not converge.\" ) from e self . ranks = centered_ranks self . signal = signal return self","title":"fit()"},{"location":"code-reference/costs/costrbf-reference/","text":"Kernelized mean change (CostRbf) # ruptures.costs.costrbf.CostRbf # Kernel cost function (rbf kernel). gram property readonly # Generate the gram matrix (lazy loading). Only access this function after a .fit() (otherwise self.signal is not defined). __init__ ( self , gamma = None ) special # Initialize the object. Source code in ruptures/costs/costrbf.py def __init__ ( self , gamma = None ): \"\"\"Initialize the object.\"\"\" self . min_size = 2 self . gamma = gamma self . _gram = None error ( self , start , end ) # Return the approximation cost on the segment [start:end]. Parameters: Name Type Description Default start int start of the segment required end int end of the segment required Returns: Type Description float segment cost Exceptions: Type Description NotEnoughPoints when the segment is too short (less than min_size samples). Source code in ruptures/costs/costrbf.py def error ( self , start , end ) -> float : \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: segment cost Raises: NotEnoughPoints: when the segment is too short (less than `min_size` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints sub_gram = self . gram [ start : end , start : end ] val = np . diagonal ( sub_gram ) . sum () val -= sub_gram . sum () / ( end - start ) return val fit ( self , signal ) # Sets parameters of the instance. Parameters: Name Type Description Default signal array signal. Shape (n_samples,) or (n_samples, n_features) required Returns: Type Description CostRbf self Source code in ruptures/costs/costrbf.py def fit ( self , signal ) -> \"CostRbf\" : \"\"\"Sets parameters of the instance. Args: signal (array): signal. Shape (n_samples,) or (n_samples, n_features) Returns: self \"\"\" if signal . ndim == 1 : self . signal = signal . reshape ( - 1 , 1 ) else : self . signal = signal # If gamma is none, set it using the median heuristic. # This heuristic involves computing the gram matrix which is lazy loaded # so we simply access the `.gram` property if self . gamma is None : self . gram return self","title":"CostRbf"},{"location":"code-reference/costs/costrbf-reference/#kernelized-mean-change-costrbf","text":"","title":"Kernelized mean change (CostRbf)"},{"location":"code-reference/costs/costrbf-reference/#ruptures.costs.costrbf.CostRbf","text":"Kernel cost function (rbf kernel).","title":"CostRbf"},{"location":"code-reference/costs/costrbf-reference/#ruptures.costs.costrbf.CostRbf.gram","text":"Generate the gram matrix (lazy loading). Only access this function after a .fit() (otherwise self.signal is not defined).","title":"gram"},{"location":"code-reference/costs/costrbf-reference/#ruptures.costs.costrbf.CostRbf.__init__","text":"Initialize the object. Source code in ruptures/costs/costrbf.py def __init__ ( self , gamma = None ): \"\"\"Initialize the object.\"\"\" self . min_size = 2 self . gamma = gamma self . _gram = None","title":"__init__()"},{"location":"code-reference/costs/costrbf-reference/#ruptures.costs.costrbf.CostRbf.error","text":"Return the approximation cost on the segment [start:end]. Parameters: Name Type Description Default start int start of the segment required end int end of the segment required Returns: Type Description float segment cost Exceptions: Type Description NotEnoughPoints when the segment is too short (less than min_size samples). Source code in ruptures/costs/costrbf.py def error ( self , start , end ) -> float : \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: segment cost Raises: NotEnoughPoints: when the segment is too short (less than `min_size` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints sub_gram = self . gram [ start : end , start : end ] val = np . diagonal ( sub_gram ) . sum () val -= sub_gram . sum () / ( end - start ) return val","title":"error()"},{"location":"code-reference/costs/costrbf-reference/#ruptures.costs.costrbf.CostRbf.fit","text":"Sets parameters of the instance. Parameters: Name Type Description Default signal array signal. Shape (n_samples,) or (n_samples, n_features) required Returns: Type Description CostRbf self Source code in ruptures/costs/costrbf.py def fit ( self , signal ) -> \"CostRbf\" : \"\"\"Sets parameters of the instance. Args: signal (array): signal. Shape (n_samples,) or (n_samples, n_features) Returns: self \"\"\" if signal . ndim == 1 : self . signal = signal . reshape ( - 1 , 1 ) else : self . signal = signal # If gamma is none, set it using the median heuristic. # This heuristic involves computing the gram matrix which is lazy loaded # so we simply access the `.gram` property if self . gamma is None : self . gram return self","title":"fit()"},{"location":"code-reference/datasets/pw_constant-reference/","text":"Piecewise constant (pw_constant) # ruptures . datasets . pw_constant . pw_constant ( n_samples = 200 , n_features = 1 , n_bkps = 3 , noise_std = None , delta = ( 1 , 10 ), seed = None ) # Return a piecewise constant signal and the associated changepoints. Parameters: Name Type Description Default n_samples int signal length 200 n_features int number of dimensions 1 n_bkps int number of changepoints 3 noise_std float noise std. If None, no noise is added None delta tuple (delta_min, delta_max) max and min jump values (1, 10) seed int random seed None Returns: Type Description tuple signal of shape (n_samples, n_features), list of breakpoints Source code in ruptures/datasets/pw_constant.py def pw_constant ( n_samples = 200 , n_features = 1 , n_bkps = 3 , noise_std = None , delta = ( 1 , 10 ), seed = None ): \"\"\"Return a piecewise constant signal and the associated changepoints. Args: n_samples (int): signal length n_features (int, optional): number of dimensions n_bkps (int, optional): number of changepoints noise_std (float, optional): noise std. If None, no noise is added delta (tuple, optional): (delta_min, delta_max) max and min jump values seed (int): random seed Returns: tuple: signal of shape (n_samples, n_features), list of breakpoints \"\"\" # breakpoints bkps = draw_bkps ( n_samples , n_bkps , seed = seed ) # we create the signal signal = np . empty (( n_samples , n_features ), dtype = float ) tt_ = np . arange ( n_samples ) delta_min , delta_max = delta # mean value center = np . zeros ( n_features ) rng = np . random . default_rng ( seed = seed ) for ind in np . split ( tt_ , bkps ): if ind . size > 0 : # jump value jump = rng . uniform ( delta_min , delta_max , size = n_features ) spin = rng . choice ([ - 1 , 1 ], n_features ) center += jump * spin signal [ ind ] = center if noise_std is not None : noise = rng . normal ( size = signal . shape ) * noise_std signal = signal + noise return signal , bkps","title":"Piecewise constant"},{"location":"code-reference/datasets/pw_constant-reference/#piecewise-constant-pw_constant","text":"","title":"Piecewise constant (pw_constant)"},{"location":"code-reference/datasets/pw_constant-reference/#ruptures.datasets.pw_constant.pw_constant","text":"Return a piecewise constant signal and the associated changepoints. Parameters: Name Type Description Default n_samples int signal length 200 n_features int number of dimensions 1 n_bkps int number of changepoints 3 noise_std float noise std. If None, no noise is added None delta tuple (delta_min, delta_max) max and min jump values (1, 10) seed int random seed None Returns: Type Description tuple signal of shape (n_samples, n_features), list of breakpoints Source code in ruptures/datasets/pw_constant.py def pw_constant ( n_samples = 200 , n_features = 1 , n_bkps = 3 , noise_std = None , delta = ( 1 , 10 ), seed = None ): \"\"\"Return a piecewise constant signal and the associated changepoints. Args: n_samples (int): signal length n_features (int, optional): number of dimensions n_bkps (int, optional): number of changepoints noise_std (float, optional): noise std. If None, no noise is added delta (tuple, optional): (delta_min, delta_max) max and min jump values seed (int): random seed Returns: tuple: signal of shape (n_samples, n_features), list of breakpoints \"\"\" # breakpoints bkps = draw_bkps ( n_samples , n_bkps , seed = seed ) # we create the signal signal = np . empty (( n_samples , n_features ), dtype = float ) tt_ = np . arange ( n_samples ) delta_min , delta_max = delta # mean value center = np . zeros ( n_features ) rng = np . random . default_rng ( seed = seed ) for ind in np . split ( tt_ , bkps ): if ind . size > 0 : # jump value jump = rng . uniform ( delta_min , delta_max , size = n_features ) spin = rng . choice ([ - 1 , 1 ], n_features ) center += jump * spin signal [ ind ] = center if noise_std is not None : noise = rng . normal ( size = signal . shape ) * noise_std signal = signal + noise return signal , bkps","title":"pw_constant()"},{"location":"code-reference/datasets/pw_linear-reference/","text":"Piecewise linear (pw_linear) # ruptures . datasets . pw_linear . pw_linear ( n_samples = 200 , n_features = 1 , n_bkps = 3 , noise_std = None , seed = None ) # Return piecewise linear signal and the associated changepoints. Parameters: Name Type Description Default n_samples int signal length 200 n_features int number of covariates 1 n_bkps int number of change points 3 noise_std float noise std. If None, no noise is added None seed int random seed None Returns: Type Description tuple signal of shape (n_samples, n_features+1), list of breakpoints Source code in ruptures/datasets/pw_linear.py def pw_linear ( n_samples = 200 , n_features = 1 , n_bkps = 3 , noise_std = None , seed = None ): \"\"\"Return piecewise linear signal and the associated changepoints. Args: n_samples (int, optional): signal length n_features (int, optional): number of covariates n_bkps (int, optional): number of change points noise_std (float, optional): noise std. If None, no noise is added seed (int): random seed Returns: tuple: signal of shape (n_samples, n_features+1), list of breakpoints \"\"\" rng = np . random . default_rng ( seed = seed ) covar = rng . normal ( size = ( n_samples , n_features )) linear_coeff , bkps = pw_constant ( n_samples = n_samples , n_bkps = n_bkps , n_features = n_features , noise_std = None , seed = seed , ) var = np . sum ( linear_coeff * covar , axis = 1 ) if noise_std is not None : var += rng . normal ( scale = noise_std , size = var . shape ) signal = np . c_ [ var , covar ] return signal , bkps","title":"Piecewise linear"},{"location":"code-reference/datasets/pw_linear-reference/#piecewise-linear-pw_linear","text":"","title":"Piecewise linear (pw_linear)"},{"location":"code-reference/datasets/pw_linear-reference/#ruptures.datasets.pw_linear.pw_linear","text":"Return piecewise linear signal and the associated changepoints. Parameters: Name Type Description Default n_samples int signal length 200 n_features int number of covariates 1 n_bkps int number of change points 3 noise_std float noise std. If None, no noise is added None seed int random seed None Returns: Type Description tuple signal of shape (n_samples, n_features+1), list of breakpoints Source code in ruptures/datasets/pw_linear.py def pw_linear ( n_samples = 200 , n_features = 1 , n_bkps = 3 , noise_std = None , seed = None ): \"\"\"Return piecewise linear signal and the associated changepoints. Args: n_samples (int, optional): signal length n_features (int, optional): number of covariates n_bkps (int, optional): number of change points noise_std (float, optional): noise std. If None, no noise is added seed (int): random seed Returns: tuple: signal of shape (n_samples, n_features+1), list of breakpoints \"\"\" rng = np . random . default_rng ( seed = seed ) covar = rng . normal ( size = ( n_samples , n_features )) linear_coeff , bkps = pw_constant ( n_samples = n_samples , n_bkps = n_bkps , n_features = n_features , noise_std = None , seed = seed , ) var = np . sum ( linear_coeff * covar , axis = 1 ) if noise_std is not None : var += rng . normal ( scale = noise_std , size = var . shape ) signal = np . c_ [ var , covar ] return signal , bkps","title":"pw_linear()"},{"location":"code-reference/datasets/pw_normal-reference/","text":"Piecewise Gaussian (pw_normal) # ruptures . datasets . pw_normal . pw_normal ( n_samples = 200 , n_bkps = 3 , seed = None ) # Return a 2D piecewise Gaussian signal and the associated changepoints. Parameters: Name Type Description Default n_samples int signal length 200 n_bkps int number of change points 3 seed int random seed None Returns: Type Description tuple signal of shape (n_samples, 2), list of breakpoints Source code in ruptures/datasets/pw_normal.py def pw_normal ( n_samples = 200 , n_bkps = 3 , seed = None ): \"\"\"Return a 2D piecewise Gaussian signal and the associated changepoints. Args: n_samples (int, optional): signal length n_bkps (int, optional): number of change points seed (int): random seed Returns: tuple: signal of shape (n_samples, 2), list of breakpoints \"\"\" # breakpoints bkps = draw_bkps ( n_samples , n_bkps , seed = seed ) # we create the signal signal = np . zeros (( n_samples , 2 ), dtype = float ) cov1 = np . array ([[ 1 , 0.9 ], [ 0.9 , 1 ]]) cov2 = np . array ([[ 1 , - 0.9 ], [ - 0.9 , 1 ]]) rng = np . random . default_rng ( seed = seed ) for sub , cov in zip ( np . split ( signal , bkps ), cycle (( cov1 , cov2 ))): n_sub , _ = sub . shape sub += rng . multivariate_normal ([ 0 , 0 ], cov , size = n_sub ) return signal , bkps","title":"Piecewise normal"},{"location":"code-reference/datasets/pw_normal-reference/#piecewise-gaussian-pw_normal","text":"","title":"Piecewise Gaussian (pw_normal)"},{"location":"code-reference/datasets/pw_normal-reference/#ruptures.datasets.pw_normal.pw_normal","text":"Return a 2D piecewise Gaussian signal and the associated changepoints. Parameters: Name Type Description Default n_samples int signal length 200 n_bkps int number of change points 3 seed int random seed None Returns: Type Description tuple signal of shape (n_samples, 2), list of breakpoints Source code in ruptures/datasets/pw_normal.py def pw_normal ( n_samples = 200 , n_bkps = 3 , seed = None ): \"\"\"Return a 2D piecewise Gaussian signal and the associated changepoints. Args: n_samples (int, optional): signal length n_bkps (int, optional): number of change points seed (int): random seed Returns: tuple: signal of shape (n_samples, 2), list of breakpoints \"\"\" # breakpoints bkps = draw_bkps ( n_samples , n_bkps , seed = seed ) # we create the signal signal = np . zeros (( n_samples , 2 ), dtype = float ) cov1 = np . array ([[ 1 , 0.9 ], [ 0.9 , 1 ]]) cov2 = np . array ([[ 1 , - 0.9 ], [ - 0.9 , 1 ]]) rng = np . random . default_rng ( seed = seed ) for sub , cov in zip ( np . split ( signal , bkps ), cycle (( cov1 , cov2 ))): n_sub , _ = sub . shape sub += rng . multivariate_normal ([ 0 , 0 ], cov , size = n_sub ) return signal , bkps","title":"pw_normal()"},{"location":"code-reference/datasets/pw_wavy-reference/","text":"Piecewise wavy (pw_wavy) # ruptures . datasets . pw_wavy . pw_wavy ( n_samples = 200 , n_bkps = 3 , noise_std = None , seed = None ) # Return a 1D piecewise wavy signal and the associated changepoints. Parameters: Name Type Description Default n_samples int signal length 200 n_bkps int number of changepoints 3 noise_std float noise std. If None, no noise is added None seed int random seed None Returns: Type Description tuple signal of shape (n_samples, 1), list of breakpoints Source code in ruptures/datasets/pw_wavy.py def pw_wavy ( n_samples = 200 , n_bkps = 3 , noise_std = None , seed = None ): \"\"\"Return a 1D piecewise wavy signal and the associated changepoints. Args: n_samples (int, optional): signal length n_bkps (int, optional): number of changepoints noise_std (float, optional): noise std. If None, no noise is added seed (int): random seed Returns: tuple: signal of shape (n_samples, 1), list of breakpoints \"\"\" # breakpoints bkps = draw_bkps ( n_samples , n_bkps , seed = seed ) # we create the signal f1 = np . array ([ 0.075 , 0.1 ]) f2 = np . array ([ 0.1 , 0.125 ]) freqs = np . zeros (( n_samples , 2 )) for sub , val in zip ( np . split ( freqs , bkps [: - 1 ]), cycle ([ f1 , f2 ])): sub += val tt = np . arange ( n_samples ) # DeprecationWarning: Calling np.sum(generator) is deprecated # Use np.sum(np.from_iter(generator)) or the python sum builtin instead. signal = np . sum ([ np . sin ( 2 * np . pi * tt * f ) for f in freqs . T ], axis = 0 ) if noise_std is not None : rng = np . random . default_rng ( seed = seed ) noise = rng . normal ( scale = noise_std , size = signal . shape ) signal += noise return signal , bkps","title":"Piecewise wavy"},{"location":"code-reference/datasets/pw_wavy-reference/#piecewise-wavy-pw_wavy","text":"","title":"Piecewise wavy (pw_wavy)"},{"location":"code-reference/datasets/pw_wavy-reference/#ruptures.datasets.pw_wavy.pw_wavy","text":"Return a 1D piecewise wavy signal and the associated changepoints. Parameters: Name Type Description Default n_samples int signal length 200 n_bkps int number of changepoints 3 noise_std float noise std. If None, no noise is added None seed int random seed None Returns: Type Description tuple signal of shape (n_samples, 1), list of breakpoints Source code in ruptures/datasets/pw_wavy.py def pw_wavy ( n_samples = 200 , n_bkps = 3 , noise_std = None , seed = None ): \"\"\"Return a 1D piecewise wavy signal and the associated changepoints. Args: n_samples (int, optional): signal length n_bkps (int, optional): number of changepoints noise_std (float, optional): noise std. If None, no noise is added seed (int): random seed Returns: tuple: signal of shape (n_samples, 1), list of breakpoints \"\"\" # breakpoints bkps = draw_bkps ( n_samples , n_bkps , seed = seed ) # we create the signal f1 = np . array ([ 0.075 , 0.1 ]) f2 = np . array ([ 0.1 , 0.125 ]) freqs = np . zeros (( n_samples , 2 )) for sub , val in zip ( np . split ( freqs , bkps [: - 1 ]), cycle ([ f1 , f2 ])): sub += val tt = np . arange ( n_samples ) # DeprecationWarning: Calling np.sum(generator) is deprecated # Use np.sum(np.from_iter(generator)) or the python sum builtin instead. signal = np . sum ([ np . sin ( 2 * np . pi * tt * f ) for f in freqs . T ], axis = 0 ) if noise_std is not None : rng = np . random . default_rng ( seed = seed ) noise = rng . normal ( scale = noise_std , size = signal . shape ) signal += noise return signal , bkps","title":"pw_wavy()"},{"location":"code-reference/detection/binseg-reference/","text":"Binary segmentation # ruptures.detection.binseg.Binseg # Binary segmentation. __init__ ( self , model = 'l2' , custom_cost = None , min_size = 2 , jump = 5 , params = None ) special # Initialize a Binseg instance. Parameters: Name Type Description Default model str segment model, [\"l1\", \"l2\", \"rbf\",...]. Not used if 'custom_cost' is not None. 'l2' custom_cost BaseCost custom cost function. Defaults to None. None min_size int minimum segment length. Defaults to 2 samples. 2 jump int subsample (one every jump points). Defaults to 5 samples. 5 params dict a dictionary of parameters for the cost instance. None Source code in ruptures/detection/binseg.py def __init__ ( self , model = \"l2\" , custom_cost = None , min_size = 2 , jump = 5 , params = None ): \"\"\"Initialize a Binseg instance. Args: model (str, optional): segment model, [\"l1\", \"l2\", \"rbf\",...]. Not used if ``'custom_cost'`` is not None. custom_cost (BaseCost, optional): custom cost function. Defaults to None. min_size (int, optional): minimum segment length. Defaults to 2 samples. jump (int, optional): subsample (one every *jump* points). Defaults to 5 samples. params (dict, optional): a dictionary of parameters for the cost instance. \"\"\" if custom_cost is not None and isinstance ( custom_cost , BaseCost ): self . cost = custom_cost else : if params is None : self . cost = cost_factory ( model = model ) else : self . cost = cost_factory ( model = model , ** params ) self . min_size = max ( min_size , self . cost . min_size ) self . jump = jump self . n_samples = None self . signal = None fit ( self , signal ) # Compute params to segment signal. Parameters: Name Type Description Default signal array signal to segment. Shape (n_samples, n_features) or (n_samples,). required Returns: Type Description Binseg self Source code in ruptures/detection/binseg.py def fit ( self , signal ) -> \"Binseg\" : \"\"\"Compute params to segment signal. Args: signal (array): signal to segment. Shape (n_samples, n_features) or (n_samples,). Returns: self \"\"\" # update some params if signal . ndim == 1 : self . signal = signal . reshape ( - 1 , 1 ) else : self . signal = signal self . n_samples , _ = self . signal . shape self . cost . fit ( signal ) self . single_bkp . cache_clear () return self fit_predict ( self , signal , n_bkps = None , pen = None , epsilon = None ) # Fit to the signal and return the optimal breakpoints. Helper method to call fit and predict once Parameters: Name Type Description Default signal array signal. Shape (n_samples, n_features) or (n_samples,). required n_bkps int number of breakpoints. None pen float penalty value (>0) None epsilon float reconstruction budget (>0) None Returns: Type Description list sorted list of breakpoints Source code in ruptures/detection/binseg.py def fit_predict ( self , signal , n_bkps = None , pen = None , epsilon = None ): \"\"\"Fit to the signal and return the optimal breakpoints. Helper method to call fit and predict once Args: signal (array): signal. Shape (n_samples, n_features) or (n_samples,). n_bkps (int): number of breakpoints. pen (float): penalty value (>0) epsilon (float): reconstruction budget (>0) Returns: list: sorted list of breakpoints \"\"\" self . fit ( signal ) return self . predict ( n_bkps = n_bkps , pen = pen , epsilon = epsilon ) predict ( self , n_bkps = None , pen = None , epsilon = None ) # Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to fit() . The stopping rule depends on the parameter passed to the function. Parameters: Name Type Description Default n_bkps int number of breakpoints to find before stopping. None pen float penalty value (>0) None epsilon float reconstruction budget (>0) None Exceptions: Type Description AssertionError if none of n_bkps , pen , epsilon is set. BadSegmentationParameters in case of impossible segmentation configuration Returns: Type Description list sorted list of breakpoints Source code in ruptures/detection/binseg.py def predict ( self , n_bkps = None , pen = None , epsilon = None ): \"\"\"Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to [`fit()`][ruptures.detection.binseg.Binseg.fit]. The stopping rule depends on the parameter passed to the function. Args: n_bkps (int): number of breakpoints to find before stopping. pen (float): penalty value (>0) epsilon (float): reconstruction budget (>0) Raises: AssertionError: if none of `n_bkps`, `pen`, `epsilon` is set. BadSegmentationParameters: in case of impossible segmentation configuration Returns: list: sorted list of breakpoints \"\"\" msg = \"Give a parameter.\" assert any ( param is not None for param in ( n_bkps , pen , epsilon )), msg # raise an exception in case of impossible segmentation configuration if not sanity_check ( n_samples = self . cost . signal . shape [ 0 ], n_bkps = 0 if n_bkps is None else n_bkps , jump = self . jump , min_size = self . min_size , ): raise BadSegmentationParameters partition = self . _seg ( n_bkps = n_bkps , pen = pen , epsilon = epsilon ) bkps = sorted ( e for s , e in partition . keys ()) return bkps single_bkp ( self , start , end ) # Return the optimal breakpoint of [start:end] (if it exists). Source code in ruptures/detection/binseg.py @lru_cache ( maxsize = None ) def single_bkp ( self , start , end ): \"\"\"Return the optimal breakpoint of [start:end] (if it exists).\"\"\" segment_cost = self . cost . error ( start , end ) if np . isinf ( segment_cost ) and segment_cost < 0 : # if cost is -inf return None , 0 gain_list = list () for bkp in range ( start , end , self . jump ): if bkp - start > self . min_size and end - bkp > self . min_size : gain = ( segment_cost - self . cost . error ( start , bkp ) - self . cost . error ( bkp , end ) ) gain_list . append (( gain , bkp )) try : gain , bkp = max ( gain_list ) except ValueError : # if empty sub_sampling return None , 0 return bkp , gain","title":"Binseg"},{"location":"code-reference/detection/binseg-reference/#binary-segmentation","text":"","title":"Binary segmentation"},{"location":"code-reference/detection/binseg-reference/#ruptures.detection.binseg.Binseg","text":"Binary segmentation.","title":"Binseg"},{"location":"code-reference/detection/binseg-reference/#ruptures.detection.binseg.Binseg.__init__","text":"Initialize a Binseg instance. Parameters: Name Type Description Default model str segment model, [\"l1\", \"l2\", \"rbf\",...]. Not used if 'custom_cost' is not None. 'l2' custom_cost BaseCost custom cost function. Defaults to None. None min_size int minimum segment length. Defaults to 2 samples. 2 jump int subsample (one every jump points). Defaults to 5 samples. 5 params dict a dictionary of parameters for the cost instance. None Source code in ruptures/detection/binseg.py def __init__ ( self , model = \"l2\" , custom_cost = None , min_size = 2 , jump = 5 , params = None ): \"\"\"Initialize a Binseg instance. Args: model (str, optional): segment model, [\"l1\", \"l2\", \"rbf\",...]. Not used if ``'custom_cost'`` is not None. custom_cost (BaseCost, optional): custom cost function. Defaults to None. min_size (int, optional): minimum segment length. Defaults to 2 samples. jump (int, optional): subsample (one every *jump* points). Defaults to 5 samples. params (dict, optional): a dictionary of parameters for the cost instance. \"\"\" if custom_cost is not None and isinstance ( custom_cost , BaseCost ): self . cost = custom_cost else : if params is None : self . cost = cost_factory ( model = model ) else : self . cost = cost_factory ( model = model , ** params ) self . min_size = max ( min_size , self . cost . min_size ) self . jump = jump self . n_samples = None self . signal = None","title":"__init__()"},{"location":"code-reference/detection/binseg-reference/#ruptures.detection.binseg.Binseg.fit","text":"Compute params to segment signal. Parameters: Name Type Description Default signal array signal to segment. Shape (n_samples, n_features) or (n_samples,). required Returns: Type Description Binseg self Source code in ruptures/detection/binseg.py def fit ( self , signal ) -> \"Binseg\" : \"\"\"Compute params to segment signal. Args: signal (array): signal to segment. Shape (n_samples, n_features) or (n_samples,). Returns: self \"\"\" # update some params if signal . ndim == 1 : self . signal = signal . reshape ( - 1 , 1 ) else : self . signal = signal self . n_samples , _ = self . signal . shape self . cost . fit ( signal ) self . single_bkp . cache_clear () return self","title":"fit()"},{"location":"code-reference/detection/binseg-reference/#ruptures.detection.binseg.Binseg.fit_predict","text":"Fit to the signal and return the optimal breakpoints. Helper method to call fit and predict once Parameters: Name Type Description Default signal array signal. Shape (n_samples, n_features) or (n_samples,). required n_bkps int number of breakpoints. None pen float penalty value (>0) None epsilon float reconstruction budget (>0) None Returns: Type Description list sorted list of breakpoints Source code in ruptures/detection/binseg.py def fit_predict ( self , signal , n_bkps = None , pen = None , epsilon = None ): \"\"\"Fit to the signal and return the optimal breakpoints. Helper method to call fit and predict once Args: signal (array): signal. Shape (n_samples, n_features) or (n_samples,). n_bkps (int): number of breakpoints. pen (float): penalty value (>0) epsilon (float): reconstruction budget (>0) Returns: list: sorted list of breakpoints \"\"\" self . fit ( signal ) return self . predict ( n_bkps = n_bkps , pen = pen , epsilon = epsilon )","title":"fit_predict()"},{"location":"code-reference/detection/binseg-reference/#ruptures.detection.binseg.Binseg.predict","text":"Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to fit() . The stopping rule depends on the parameter passed to the function. Parameters: Name Type Description Default n_bkps int number of breakpoints to find before stopping. None pen float penalty value (>0) None epsilon float reconstruction budget (>0) None Exceptions: Type Description AssertionError if none of n_bkps , pen , epsilon is set. BadSegmentationParameters in case of impossible segmentation configuration Returns: Type Description list sorted list of breakpoints Source code in ruptures/detection/binseg.py def predict ( self , n_bkps = None , pen = None , epsilon = None ): \"\"\"Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to [`fit()`][ruptures.detection.binseg.Binseg.fit]. The stopping rule depends on the parameter passed to the function. Args: n_bkps (int): number of breakpoints to find before stopping. pen (float): penalty value (>0) epsilon (float): reconstruction budget (>0) Raises: AssertionError: if none of `n_bkps`, `pen`, `epsilon` is set. BadSegmentationParameters: in case of impossible segmentation configuration Returns: list: sorted list of breakpoints \"\"\" msg = \"Give a parameter.\" assert any ( param is not None for param in ( n_bkps , pen , epsilon )), msg # raise an exception in case of impossible segmentation configuration if not sanity_check ( n_samples = self . cost . signal . shape [ 0 ], n_bkps = 0 if n_bkps is None else n_bkps , jump = self . jump , min_size = self . min_size , ): raise BadSegmentationParameters partition = self . _seg ( n_bkps = n_bkps , pen = pen , epsilon = epsilon ) bkps = sorted ( e for s , e in partition . keys ()) return bkps","title":"predict()"},{"location":"code-reference/detection/binseg-reference/#ruptures.detection.binseg.Binseg.single_bkp","text":"Return the optimal breakpoint of [start:end] (if it exists). Source code in ruptures/detection/binseg.py @lru_cache ( maxsize = None ) def single_bkp ( self , start , end ): \"\"\"Return the optimal breakpoint of [start:end] (if it exists).\"\"\" segment_cost = self . cost . error ( start , end ) if np . isinf ( segment_cost ) and segment_cost < 0 : # if cost is -inf return None , 0 gain_list = list () for bkp in range ( start , end , self . jump ): if bkp - start > self . min_size and end - bkp > self . min_size : gain = ( segment_cost - self . cost . error ( start , bkp ) - self . cost . error ( bkp , end ) ) gain_list . append (( gain , bkp )) try : gain , bkp = max ( gain_list ) except ValueError : # if empty sub_sampling return None , 0 return bkp , gain","title":"single_bkp()"},{"location":"code-reference/detection/bottomup-reference/","text":"Bottom-up segmentation # ruptures.detection.bottomup.BottomUp # Bottom-up segmentation. __init__ ( self , model = 'l2' , custom_cost = None , min_size = 2 , jump = 5 , params = None ) special # Initialize a BottomUp instance. Parameters: Name Type Description Default model str segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if 'custom_cost' is not None. 'l2' custom_cost BaseCost custom cost function. Defaults to None. None min_size int minimum segment length. Defaults to 2 samples. 2 jump int subsample (one every jump points). Defaults to 5 samples. 5 params dict a dictionary of parameters for the cost instance. None Source code in ruptures/detection/bottomup.py def __init__ ( self , model = \"l2\" , custom_cost = None , min_size = 2 , jump = 5 , params = None ): \"\"\"Initialize a BottomUp instance. Args: model (str, optional): segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if ``'custom_cost'`` is not None. custom_cost (BaseCost, optional): custom cost function. Defaults to None. min_size (int, optional): minimum segment length. Defaults to 2 samples. jump (int, optional): subsample (one every *jump* points). Defaults to 5 samples. params (dict, optional): a dictionary of parameters for the cost instance. \"\"\" if custom_cost is not None and isinstance ( custom_cost , BaseCost ): self . cost = custom_cost else : if params is None : self . cost = cost_factory ( model = model ) else : self . cost = cost_factory ( model = model , ** params ) self . min_size = max ( min_size , self . cost . min_size ) self . jump = jump self . n_samples = None self . signal = None self . leaves = None fit ( self , signal ) # Compute params to segment signal. Parameters: Name Type Description Default signal array signal to segment. Shape (n_samples, n_features) or (n_samples,). required Returns: Type Description BottomUp self Source code in ruptures/detection/bottomup.py def fit ( self , signal ) -> \"BottomUp\" : \"\"\"Compute params to segment signal. Args: signal (array): signal to segment. Shape (n_samples, n_features) or (n_samples,). Returns: self \"\"\" # update some params self . cost . fit ( signal ) self . merge . cache_clear () if signal . ndim == 1 : ( n_samples ,) = signal . shape else : n_samples , _ = signal . shape self . n_samples = n_samples self . leaves = self . _grow_tree () return self fit_predict ( self , signal , n_bkps = None , pen = None , epsilon = None ) # Fit to the signal and return the optimal breakpoints. Helper method to call fit and predict once Parameters: Name Type Description Default signal array signal. Shape (n_samples, n_features) or (n_samples,). required n_bkps int number of breakpoints. None pen float penalty value (>0) None epsilon float reconstruction budget (>0) None Returns: Type Description list sorted list of breakpoints Source code in ruptures/detection/bottomup.py def fit_predict ( self , signal , n_bkps = None , pen = None , epsilon = None ): \"\"\"Fit to the signal and return the optimal breakpoints. Helper method to call fit and predict once Args: signal (array): signal. Shape (n_samples, n_features) or (n_samples,). n_bkps (int): number of breakpoints. pen (float): penalty value (>0) epsilon (float): reconstruction budget (>0) Returns: list: sorted list of breakpoints \"\"\" self . fit ( signal ) return self . predict ( n_bkps = n_bkps , pen = pen , epsilon = epsilon ) merge ( self , left , right ) # Merge two contiguous segments. Source code in ruptures/detection/bottomup.py @lru_cache ( maxsize = None ) def merge ( self , left , right ): \"\"\"Merge two contiguous segments.\"\"\" assert left . end == right . start , \"Segments are not contiguous.\" start , end = left . start , right . end val = self . cost . error ( start , end ) node = Bnode ( start , end , val , left = left , right = right ) return node predict ( self , n_bkps = None , pen = None , epsilon = None ) # Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to fit() . The stopping rule depends on the parameter passed to the function. Parameters: Name Type Description Default n_bkps int number of breakpoints to find before stopping. None pen float penalty value (>0) None epsilon float reconstruction budget (>0) None Exceptions: Type Description AssertionError if none of n_bkps , pen , epsilon is set. BadSegmentationParameters in case of impossible segmentation configuration Returns: Type Description list sorted list of breakpoints Source code in ruptures/detection/bottomup.py def predict ( self , n_bkps = None , pen = None , epsilon = None ): \"\"\"Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to [`fit()`][ruptures.detection.bottomup.BottomUp.fit]. The stopping rule depends on the parameter passed to the function. Args: n_bkps (int): number of breakpoints to find before stopping. pen (float): penalty value (>0) epsilon (float): reconstruction budget (>0) Raises: AssertionError: if none of `n_bkps`, `pen`, `epsilon` is set. BadSegmentationParameters: in case of impossible segmentation configuration Returns: list: sorted list of breakpoints \"\"\" msg = \"Give a parameter.\" assert any ( param is not None for param in ( n_bkps , pen , epsilon )), msg # raise an exception in case of impossible segmentation configuration if not sanity_check ( n_samples = self . cost . signal . shape [ 0 ], n_bkps = 0 if n_bkps is None else n_bkps , jump = self . jump , min_size = self . min_size , ): raise BadSegmentationParameters partition = self . _seg ( n_bkps = n_bkps , pen = pen , epsilon = epsilon ) bkps = sorted ( e for s , e in partition . keys ()) return bkps","title":"BottomUp"},{"location":"code-reference/detection/bottomup-reference/#bottom-up-segmentation","text":"","title":"Bottom-up segmentation"},{"location":"code-reference/detection/bottomup-reference/#ruptures.detection.bottomup.BottomUp","text":"Bottom-up segmentation.","title":"BottomUp"},{"location":"code-reference/detection/bottomup-reference/#ruptures.detection.bottomup.BottomUp.__init__","text":"Initialize a BottomUp instance. Parameters: Name Type Description Default model str segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if 'custom_cost' is not None. 'l2' custom_cost BaseCost custom cost function. Defaults to None. None min_size int minimum segment length. Defaults to 2 samples. 2 jump int subsample (one every jump points). Defaults to 5 samples. 5 params dict a dictionary of parameters for the cost instance. None Source code in ruptures/detection/bottomup.py def __init__ ( self , model = \"l2\" , custom_cost = None , min_size = 2 , jump = 5 , params = None ): \"\"\"Initialize a BottomUp instance. Args: model (str, optional): segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if ``'custom_cost'`` is not None. custom_cost (BaseCost, optional): custom cost function. Defaults to None. min_size (int, optional): minimum segment length. Defaults to 2 samples. jump (int, optional): subsample (one every *jump* points). Defaults to 5 samples. params (dict, optional): a dictionary of parameters for the cost instance. \"\"\" if custom_cost is not None and isinstance ( custom_cost , BaseCost ): self . cost = custom_cost else : if params is None : self . cost = cost_factory ( model = model ) else : self . cost = cost_factory ( model = model , ** params ) self . min_size = max ( min_size , self . cost . min_size ) self . jump = jump self . n_samples = None self . signal = None self . leaves = None","title":"__init__()"},{"location":"code-reference/detection/bottomup-reference/#ruptures.detection.bottomup.BottomUp.fit","text":"Compute params to segment signal. Parameters: Name Type Description Default signal array signal to segment. Shape (n_samples, n_features) or (n_samples,). required Returns: Type Description BottomUp self Source code in ruptures/detection/bottomup.py def fit ( self , signal ) -> \"BottomUp\" : \"\"\"Compute params to segment signal. Args: signal (array): signal to segment. Shape (n_samples, n_features) or (n_samples,). Returns: self \"\"\" # update some params self . cost . fit ( signal ) self . merge . cache_clear () if signal . ndim == 1 : ( n_samples ,) = signal . shape else : n_samples , _ = signal . shape self . n_samples = n_samples self . leaves = self . _grow_tree () return self","title":"fit()"},{"location":"code-reference/detection/bottomup-reference/#ruptures.detection.bottomup.BottomUp.fit_predict","text":"Fit to the signal and return the optimal breakpoints. Helper method to call fit and predict once Parameters: Name Type Description Default signal array signal. Shape (n_samples, n_features) or (n_samples,). required n_bkps int number of breakpoints. None pen float penalty value (>0) None epsilon float reconstruction budget (>0) None Returns: Type Description list sorted list of breakpoints Source code in ruptures/detection/bottomup.py def fit_predict ( self , signal , n_bkps = None , pen = None , epsilon = None ): \"\"\"Fit to the signal and return the optimal breakpoints. Helper method to call fit and predict once Args: signal (array): signal. Shape (n_samples, n_features) or (n_samples,). n_bkps (int): number of breakpoints. pen (float): penalty value (>0) epsilon (float): reconstruction budget (>0) Returns: list: sorted list of breakpoints \"\"\" self . fit ( signal ) return self . predict ( n_bkps = n_bkps , pen = pen , epsilon = epsilon )","title":"fit_predict()"},{"location":"code-reference/detection/bottomup-reference/#ruptures.detection.bottomup.BottomUp.merge","text":"Merge two contiguous segments. Source code in ruptures/detection/bottomup.py @lru_cache ( maxsize = None ) def merge ( self , left , right ): \"\"\"Merge two contiguous segments.\"\"\" assert left . end == right . start , \"Segments are not contiguous.\" start , end = left . start , right . end val = self . cost . error ( start , end ) node = Bnode ( start , end , val , left = left , right = right ) return node","title":"merge()"},{"location":"code-reference/detection/bottomup-reference/#ruptures.detection.bottomup.BottomUp.predict","text":"Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to fit() . The stopping rule depends on the parameter passed to the function. Parameters: Name Type Description Default n_bkps int number of breakpoints to find before stopping. None pen float penalty value (>0) None epsilon float reconstruction budget (>0) None Exceptions: Type Description AssertionError if none of n_bkps , pen , epsilon is set. BadSegmentationParameters in case of impossible segmentation configuration Returns: Type Description list sorted list of breakpoints Source code in ruptures/detection/bottomup.py def predict ( self , n_bkps = None , pen = None , epsilon = None ): \"\"\"Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to [`fit()`][ruptures.detection.bottomup.BottomUp.fit]. The stopping rule depends on the parameter passed to the function. Args: n_bkps (int): number of breakpoints to find before stopping. pen (float): penalty value (>0) epsilon (float): reconstruction budget (>0) Raises: AssertionError: if none of `n_bkps`, `pen`, `epsilon` is set. BadSegmentationParameters: in case of impossible segmentation configuration Returns: list: sorted list of breakpoints \"\"\" msg = \"Give a parameter.\" assert any ( param is not None for param in ( n_bkps , pen , epsilon )), msg # raise an exception in case of impossible segmentation configuration if not sanity_check ( n_samples = self . cost . signal . shape [ 0 ], n_bkps = 0 if n_bkps is None else n_bkps , jump = self . jump , min_size = self . min_size , ): raise BadSegmentationParameters partition = self . _seg ( n_bkps = n_bkps , pen = pen , epsilon = epsilon ) bkps = sorted ( e for s , e in partition . keys ()) return bkps","title":"predict()"},{"location":"code-reference/detection/dynp-reference/","text":"Dynamic programming # ruptures.detection.dynp.Dynp # Find optimal change points using dynamic programming. Given a segment model, it computes the best partition for which the sum of errors is minimum. __init__ ( self , model = 'l2' , custom_cost = None , min_size = 2 , jump = 5 , params = None ) special # Creates a Dynp instance. Parameters: Name Type Description Default model str segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if 'custom_cost' is not None. 'l2' custom_cost BaseCost custom cost function. Defaults to None. None min_size int minimum segment length. 2 jump int subsample (one every jump points). 5 params dict a dictionary of parameters for the cost instance. None Source code in ruptures/detection/dynp.py def __init__ ( self , model = \"l2\" , custom_cost = None , min_size = 2 , jump = 5 , params = None ): \"\"\"Creates a Dynp instance. Args: model (str, optional): segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if ``'custom_cost'`` is not None. custom_cost (BaseCost, optional): custom cost function. Defaults to None. min_size (int, optional): minimum segment length. jump (int, optional): subsample (one every *jump* points). params (dict, optional): a dictionary of parameters for the cost instance. \"\"\" if custom_cost is not None and isinstance ( custom_cost , BaseCost ): self . cost = custom_cost else : self . model_name = model if params is None : self . cost = cost_factory ( model = model ) else : self . cost = cost_factory ( model = model , ** params ) self . min_size = max ( min_size , self . cost . min_size ) self . jump = jump self . n_samples = None fit ( self , signal ) # Create the cache associated with the signal. Dynamic programming is a recurrence; intermediate results are cached to speed up computations. This method sets up the cache. Parameters: Name Type Description Default signal array signal. Shape (n_samples, n_features) or (n_samples,). required Returns: Type Description Dynp self Source code in ruptures/detection/dynp.py def fit ( self , signal ) -> \"Dynp\" : \"\"\"Create the cache associated with the signal. Dynamic programming is a recurrence; intermediate results are cached to speed up computations. This method sets up the cache. Args: signal (array): signal. Shape (n_samples, n_features) or (n_samples,). Returns: self \"\"\" # clear cache self . seg . cache_clear () # update some params self . cost . fit ( signal ) self . n_samples = signal . shape [ 0 ] return self fit_predict ( self , signal , n_bkps ) # Fit to the signal and return the optimal breakpoints. Helper method to call fit and predict once Parameters: Name Type Description Default signal array signal. Shape (n_samples, n_features) or (n_samples,). required n_bkps int number of breakpoints. required Returns: Type Description list sorted list of breakpoints Source code in ruptures/detection/dynp.py def fit_predict ( self , signal , n_bkps ): \"\"\"Fit to the signal and return the optimal breakpoints. Helper method to call fit and predict once Args: signal (array): signal. Shape (n_samples, n_features) or (n_samples,). n_bkps (int): number of breakpoints. Returns: list: sorted list of breakpoints \"\"\" self . fit ( signal ) return self . predict ( n_bkps ) predict ( self , n_bkps ) # Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to fit() . Parameters: Name Type Description Default n_bkps int number of breakpoints. required Exceptions: Type Description BadSegmentationParameters in case of impossible segmentation configuration Returns: Type Description list sorted list of breakpoints Source code in ruptures/detection/dynp.py def predict ( self , n_bkps ): \"\"\"Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to [`fit()`][ruptures.detection.dynp.Dynp.fit]. Args: n_bkps (int): number of breakpoints. Raises: BadSegmentationParameters: in case of impossible segmentation configuration Returns: list: sorted list of breakpoints \"\"\" # raise an exception in case of impossible segmentation configuration if not sanity_check ( n_samples = self . cost . signal . shape [ 0 ], n_bkps = n_bkps , jump = self . jump , min_size = self . min_size , ): raise BadSegmentationParameters partition = self . seg ( 0 , self . n_samples , n_bkps ) bkps = sorted ( e for s , e in partition . keys ()) return bkps seg ( self , start , end , n_bkps ) # Recurrence to find the optimal partition of signal[start:end]. This method is to be memoized and then used. Parameters: Name Type Description Default start int start of the segment (inclusive) required end int end of the segment (exclusive) required n_bkps int number of breakpoints required Returns: Type Description dict {(start, end): cost value, ...} Source code in ruptures/detection/dynp.py @lru_cache ( maxsize = None ) def seg ( self , start , end , n_bkps ): \"\"\"Recurrence to find the optimal partition of signal[start:end]. This method is to be memoized and then used. Args: start (int): start of the segment (inclusive) end (int): end of the segment (exclusive) n_bkps (int): number of breakpoints Returns: dict: {(start, end): cost value, ...} \"\"\" jump , min_size = self . jump , self . min_size if n_bkps == 0 : cost = self . cost . error ( start , end ) return {( start , end ): cost } elif n_bkps > 0 : # Let's fill the list of admissible last breakpoints multiple_of_jump = ( k for k in range ( start , end ) if k % jump == 0 ) admissible_bkps = list () for bkp in multiple_of_jump : n_samples = bkp - start # first check if left subproblem is possible if sanity_check ( n_samples = n_samples , n_bkps = n_bkps - 1 , jump = jump , min_size = min_size , ): # second check if the right subproblem has enough points if end - bkp >= min_size : admissible_bkps . append ( bkp ) assert ( len ( admissible_bkps ) > 0 ), \"No admissible last breakpoints found. \\ start, end: ( {} , {} ), n_bkps: {} .\" . format ( start , end , n_bkps ) # Compute the subproblems sub_problems = list () for bkp in admissible_bkps : left_partition = self . seg ( start , bkp , n_bkps - 1 ) right_partition = self . seg ( bkp , end , 0 ) tmp_partition = dict ( left_partition ) tmp_partition [( bkp , end )] = right_partition [( bkp , end )] sub_problems . append ( tmp_partition ) # Find the optimal partition return min ( sub_problems , key = lambda d : sum ( d . values ()))","title":"Dynp"},{"location":"code-reference/detection/dynp-reference/#dynamic-programming","text":"","title":"Dynamic programming"},{"location":"code-reference/detection/dynp-reference/#ruptures.detection.dynp.Dynp","text":"Find optimal change points using dynamic programming. Given a segment model, it computes the best partition for which the sum of errors is minimum.","title":"Dynp"},{"location":"code-reference/detection/dynp-reference/#ruptures.detection.dynp.Dynp.__init__","text":"Creates a Dynp instance. Parameters: Name Type Description Default model str segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if 'custom_cost' is not None. 'l2' custom_cost BaseCost custom cost function. Defaults to None. None min_size int minimum segment length. 2 jump int subsample (one every jump points). 5 params dict a dictionary of parameters for the cost instance. None Source code in ruptures/detection/dynp.py def __init__ ( self , model = \"l2\" , custom_cost = None , min_size = 2 , jump = 5 , params = None ): \"\"\"Creates a Dynp instance. Args: model (str, optional): segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if ``'custom_cost'`` is not None. custom_cost (BaseCost, optional): custom cost function. Defaults to None. min_size (int, optional): minimum segment length. jump (int, optional): subsample (one every *jump* points). params (dict, optional): a dictionary of parameters for the cost instance. \"\"\" if custom_cost is not None and isinstance ( custom_cost , BaseCost ): self . cost = custom_cost else : self . model_name = model if params is None : self . cost = cost_factory ( model = model ) else : self . cost = cost_factory ( model = model , ** params ) self . min_size = max ( min_size , self . cost . min_size ) self . jump = jump self . n_samples = None","title":"__init__()"},{"location":"code-reference/detection/dynp-reference/#ruptures.detection.dynp.Dynp.fit","text":"Create the cache associated with the signal. Dynamic programming is a recurrence; intermediate results are cached to speed up computations. This method sets up the cache. Parameters: Name Type Description Default signal array signal. Shape (n_samples, n_features) or (n_samples,). required Returns: Type Description Dynp self Source code in ruptures/detection/dynp.py def fit ( self , signal ) -> \"Dynp\" : \"\"\"Create the cache associated with the signal. Dynamic programming is a recurrence; intermediate results are cached to speed up computations. This method sets up the cache. Args: signal (array): signal. Shape (n_samples, n_features) or (n_samples,). Returns: self \"\"\" # clear cache self . seg . cache_clear () # update some params self . cost . fit ( signal ) self . n_samples = signal . shape [ 0 ] return self","title":"fit()"},{"location":"code-reference/detection/dynp-reference/#ruptures.detection.dynp.Dynp.fit_predict","text":"Fit to the signal and return the optimal breakpoints. Helper method to call fit and predict once Parameters: Name Type Description Default signal array signal. Shape (n_samples, n_features) or (n_samples,). required n_bkps int number of breakpoints. required Returns: Type Description list sorted list of breakpoints Source code in ruptures/detection/dynp.py def fit_predict ( self , signal , n_bkps ): \"\"\"Fit to the signal and return the optimal breakpoints. Helper method to call fit and predict once Args: signal (array): signal. Shape (n_samples, n_features) or (n_samples,). n_bkps (int): number of breakpoints. Returns: list: sorted list of breakpoints \"\"\" self . fit ( signal ) return self . predict ( n_bkps )","title":"fit_predict()"},{"location":"code-reference/detection/dynp-reference/#ruptures.detection.dynp.Dynp.predict","text":"Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to fit() . Parameters: Name Type Description Default n_bkps int number of breakpoints. required Exceptions: Type Description BadSegmentationParameters in case of impossible segmentation configuration Returns: Type Description list sorted list of breakpoints Source code in ruptures/detection/dynp.py def predict ( self , n_bkps ): \"\"\"Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to [`fit()`][ruptures.detection.dynp.Dynp.fit]. Args: n_bkps (int): number of breakpoints. Raises: BadSegmentationParameters: in case of impossible segmentation configuration Returns: list: sorted list of breakpoints \"\"\" # raise an exception in case of impossible segmentation configuration if not sanity_check ( n_samples = self . cost . signal . shape [ 0 ], n_bkps = n_bkps , jump = self . jump , min_size = self . min_size , ): raise BadSegmentationParameters partition = self . seg ( 0 , self . n_samples , n_bkps ) bkps = sorted ( e for s , e in partition . keys ()) return bkps","title":"predict()"},{"location":"code-reference/detection/dynp-reference/#ruptures.detection.dynp.Dynp.seg","text":"Recurrence to find the optimal partition of signal[start:end]. This method is to be memoized and then used. Parameters: Name Type Description Default start int start of the segment (inclusive) required end int end of the segment (exclusive) required n_bkps int number of breakpoints required Returns: Type Description dict {(start, end): cost value, ...} Source code in ruptures/detection/dynp.py @lru_cache ( maxsize = None ) def seg ( self , start , end , n_bkps ): \"\"\"Recurrence to find the optimal partition of signal[start:end]. This method is to be memoized and then used. Args: start (int): start of the segment (inclusive) end (int): end of the segment (exclusive) n_bkps (int): number of breakpoints Returns: dict: {(start, end): cost value, ...} \"\"\" jump , min_size = self . jump , self . min_size if n_bkps == 0 : cost = self . cost . error ( start , end ) return {( start , end ): cost } elif n_bkps > 0 : # Let's fill the list of admissible last breakpoints multiple_of_jump = ( k for k in range ( start , end ) if k % jump == 0 ) admissible_bkps = list () for bkp in multiple_of_jump : n_samples = bkp - start # first check if left subproblem is possible if sanity_check ( n_samples = n_samples , n_bkps = n_bkps - 1 , jump = jump , min_size = min_size , ): # second check if the right subproblem has enough points if end - bkp >= min_size : admissible_bkps . append ( bkp ) assert ( len ( admissible_bkps ) > 0 ), \"No admissible last breakpoints found. \\ start, end: ( {} , {} ), n_bkps: {} .\" . format ( start , end , n_bkps ) # Compute the subproblems sub_problems = list () for bkp in admissible_bkps : left_partition = self . seg ( start , bkp , n_bkps - 1 ) right_partition = self . seg ( bkp , end , 0 ) tmp_partition = dict ( left_partition ) tmp_partition [( bkp , end )] = right_partition [( bkp , end )] sub_problems . append ( tmp_partition ) # Find the optimal partition return min ( sub_problems , key = lambda d : sum ( d . values ()))","title":"seg()"},{"location":"code-reference/detection/kernelcpd-reference/","text":"Efficient kernel change point detection # ruptures.detection.kernelcpd.KernelCPD # Find optimal change points (using dynamic programming or pelt) for the special case where the cost function derives from a kernel function. Given a segment model, it computes the best partition for which the sum of errors is minimum. See the user guide for more information. __init__ ( self , kernel = 'linear' , min_size = 2 , jump = 1 , params = None ) special # Creates a KernelCPD instance. Available kernels: linear : \\(k(x,y) = x^T y\\) . rbf : \\(k(x, y) = exp(\\gamma \\|x-y\\|^2)\\) where \\(\\gamma>0\\) ( gamma ) is a user-defined parameter. cosine : \\(k(x,y)= (x^T y)/(\\|x\\|\\|y\\|)\\) . Parameters: Name Type Description Default kernel str name of the kernel, [\"linear\", \"rbf\", \"cosine\"] 'linear' min_size int minimum segment length. 2 jump int not considered, set to 1. 1 params dict a dictionary of parameters for the kernel instance None Exceptions: Type Description AssertionError if the kernel is not implemented. Source code in ruptures/detection/kernelcpd.py def __init__ ( self , kernel = \"linear\" , min_size = 2 , jump = 1 , params = None ): r \"\"\"Creates a KernelCPD instance. Available kernels: - `linear`: $k(x,y) = x^T y$. - `rbf`: $k(x, y) = exp(\\gamma \\|x-y\\|^2)$ where $\\gamma>0$ (`gamma`) is a user-defined parameter. - `cosine`: $k(x,y)= (x^T y)/(\\|x\\|\\|y\\|)$. Args: kernel (str, optional): name of the kernel, [\"linear\", \"rbf\", \"cosine\"] min_size (int, optional): minimum segment length. jump (int, optional): not considered, set to 1. params (dict, optional): a dictionary of parameters for the kernel instance Raises: AssertionError: if the kernel is not implemented. \"\"\" self . kernel_name = kernel err_msg = \"Kernel not found: {} .\" . format ( self . kernel_name ) assert self . kernel_name in [ \"linear\" , \"rbf\" , \"cosine\" ], err_msg self . model_name = \"l2\" if self . kernel_name == \"linear\" else self . kernel_name self . params = params # load the associated cost function if self . params is None : self . cost = cost_factory ( model = self . model_name ) else : self . cost = cost_factory ( model = self . model_name , ** self . params ) self . min_size = max ( min_size , self . cost . min_size ) self . jump = 1 # set to 1 self . n_samples = None self . segmentations_dict = dict () # {n_bkps: bkps_list} fit ( self , signal ) # Update some parameters (no computation in this function). Parameters: Name Type Description Default signal array signal. Shape (n_samples, n_features) or (n_samples,). required Returns: Type Description KernelCPD self Source code in ruptures/detection/kernelcpd.py def fit ( self , signal ) -> \"KernelCPD\" : \"\"\"Update some parameters (no computation in this function). Args: signal (array): signal. Shape (n_samples, n_features) or (n_samples,). Returns: self \"\"\" # update some params self . segmentations_dict = dict () self . cost . fit ( signal . astype ( np . double )) self . n_samples = signal . shape [ 0 ] return self fit_predict ( self , signal , n_bkps = None , pen = None ) # Fit to the signal and return the optimal breakpoints. Helper method to call fit and predict once Parameters: Name Type Description Default signal array signal. Shape (n_samples, n_features) or (n_samples,). required n_bkps int Number of change points. Defaults to None. None pen float penalty value (>0). Defaults to None. Not considered if n_bkps is not None. None Returns: Type Description list sorted list of breakpoints Source code in ruptures/detection/kernelcpd.py def fit_predict ( self , signal , n_bkps = None , pen = None ): \"\"\"Fit to the signal and return the optimal breakpoints. Helper method to call fit and predict once Args: signal (array): signal. Shape (n_samples, n_features) or (n_samples,). n_bkps (int, optional): Number of change points. Defaults to None. pen (float, optional): penalty value (>0). Defaults to None. Not considered if n_bkps is not None. Returns: list: sorted list of breakpoints \"\"\" self . fit ( signal ) return self . predict ( n_bkps = n_bkps , pen = pen ) predict ( self , n_bkps = None , pen = None ) # Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to fit() . Parameters: Name Type Description Default n_bkps int Number of change points. Defaults to None. None pen float penalty value (>0). Defaults to None. Not considered if n_bkps is not None. None Exceptions: Type Description AssertionError if pen or n_bkps is not strictly positive. BadSegmentationParameters in case of impossible segmentation configuration Returns: Type Description list[int] sorted list of breakpoints Source code in ruptures/detection/kernelcpd.py def predict ( self , n_bkps = None , pen = None ): \"\"\"Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to [`fit()`][ruptures.detection.kernelcpd.KernelCPD.fit]. Args: n_bkps (int, optional): Number of change points. Defaults to None. pen (float, optional): penalty value (>0). Defaults to None. Not considered if n_bkps is not None. Raises: AssertionError: if `pen` or `n_bkps` is not strictly positive. BadSegmentationParameters: in case of impossible segmentation configuration Returns: list[int]: sorted list of breakpoints \"\"\" # Our KernelCPD implementation with Pelt implies that we have at least one change point # raise an exception in case of impossible segmentation configuration if not sanity_check ( n_samples = self . cost . signal . shape [ 0 ], n_bkps = 1 if n_bkps is None else n_bkps , jump = self . jump , min_size = self . min_size , ): raise BadSegmentationParameters # dynamic programming if the user passed a number change points if n_bkps is not None : n_bkps = int ( n_bkps ) err_msg = \"The number of changes must be positive: {} \" . format ( n_bkps ) assert n_bkps > 0 , err_msg # if we have already computed it, return it without computations. if n_bkps in self . segmentations_dict : return self . segmentations_dict [ n_bkps ] # otherwise, call the C function if self . kernel_name == \"linear\" : path_matrix_flat = ekcpd_L2 ( self . cost . signal , n_bkps , self . min_size ) elif self . kernel_name == \"rbf\" : path_matrix_flat = ekcpd_Gaussian ( self . cost . signal , n_bkps , self . min_size , self . cost . gamma ) elif self . kernel_name == \"cosine\" : path_matrix_flat = ekcpd_cosine ( self . cost . signal , n_bkps , self . min_size ) # from the path matrix, get all segmentation for k=1,...,n_bkps changes for k in range ( 1 , n_bkps + 1 ): self . segmentations_dict [ k ] = from_path_matrix_to_bkps_list ( path_matrix_flat , k , self . n_samples , n_bkps , self . jump ) return self . segmentations_dict [ n_bkps ] # Call pelt if the user passed a penalty if pen is not None : assert pen > 0 , \"The penalty must be positive: {} \" . format ( pen ) if self . kernel_name == \"linear\" : path_matrix = ekcpd_pelt_L2 ( self . cost . signal , pen , self . min_size ) elif self . kernel_name == \"rbf\" : path_matrix = ekcpd_pelt_Gaussian ( self . cost . signal , pen , self . min_size , self . cost . gamma ) elif self . kernel_name == \"cosine\" : path_matrix = ekcpd_pelt_cosine ( self . cost . signal , pen , self . min_size ) my_bkps = list () ind = self . n_samples while ind > 0 : my_bkps . append ( ind ) ind = path_matrix [ ind ] return my_bkps [:: - 1 ]","title":"KernelCPD"},{"location":"code-reference/detection/kernelcpd-reference/#efficient-kernel-change-point-detection","text":"","title":"Efficient kernel change point detection"},{"location":"code-reference/detection/kernelcpd-reference/#ruptures.detection.kernelcpd.KernelCPD","text":"Find optimal change points (using dynamic programming or pelt) for the special case where the cost function derives from a kernel function. Given a segment model, it computes the best partition for which the sum of errors is minimum. See the user guide for more information.","title":"KernelCPD"},{"location":"code-reference/detection/kernelcpd-reference/#ruptures.detection.kernelcpd.KernelCPD.__init__","text":"Creates a KernelCPD instance. Available kernels: linear : \\(k(x,y) = x^T y\\) . rbf : \\(k(x, y) = exp(\\gamma \\|x-y\\|^2)\\) where \\(\\gamma>0\\) ( gamma ) is a user-defined parameter. cosine : \\(k(x,y)= (x^T y)/(\\|x\\|\\|y\\|)\\) . Parameters: Name Type Description Default kernel str name of the kernel, [\"linear\", \"rbf\", \"cosine\"] 'linear' min_size int minimum segment length. 2 jump int not considered, set to 1. 1 params dict a dictionary of parameters for the kernel instance None Exceptions: Type Description AssertionError if the kernel is not implemented. Source code in ruptures/detection/kernelcpd.py def __init__ ( self , kernel = \"linear\" , min_size = 2 , jump = 1 , params = None ): r \"\"\"Creates a KernelCPD instance. Available kernels: - `linear`: $k(x,y) = x^T y$. - `rbf`: $k(x, y) = exp(\\gamma \\|x-y\\|^2)$ where $\\gamma>0$ (`gamma`) is a user-defined parameter. - `cosine`: $k(x,y)= (x^T y)/(\\|x\\|\\|y\\|)$. Args: kernel (str, optional): name of the kernel, [\"linear\", \"rbf\", \"cosine\"] min_size (int, optional): minimum segment length. jump (int, optional): not considered, set to 1. params (dict, optional): a dictionary of parameters for the kernel instance Raises: AssertionError: if the kernel is not implemented. \"\"\" self . kernel_name = kernel err_msg = \"Kernel not found: {} .\" . format ( self . kernel_name ) assert self . kernel_name in [ \"linear\" , \"rbf\" , \"cosine\" ], err_msg self . model_name = \"l2\" if self . kernel_name == \"linear\" else self . kernel_name self . params = params # load the associated cost function if self . params is None : self . cost = cost_factory ( model = self . model_name ) else : self . cost = cost_factory ( model = self . model_name , ** self . params ) self . min_size = max ( min_size , self . cost . min_size ) self . jump = 1 # set to 1 self . n_samples = None self . segmentations_dict = dict () # {n_bkps: bkps_list}","title":"__init__()"},{"location":"code-reference/detection/kernelcpd-reference/#ruptures.detection.kernelcpd.KernelCPD.fit","text":"Update some parameters (no computation in this function). Parameters: Name Type Description Default signal array signal. Shape (n_samples, n_features) or (n_samples,). required Returns: Type Description KernelCPD self Source code in ruptures/detection/kernelcpd.py def fit ( self , signal ) -> \"KernelCPD\" : \"\"\"Update some parameters (no computation in this function). Args: signal (array): signal. Shape (n_samples, n_features) or (n_samples,). Returns: self \"\"\" # update some params self . segmentations_dict = dict () self . cost . fit ( signal . astype ( np . double )) self . n_samples = signal . shape [ 0 ] return self","title":"fit()"},{"location":"code-reference/detection/kernelcpd-reference/#ruptures.detection.kernelcpd.KernelCPD.fit_predict","text":"Fit to the signal and return the optimal breakpoints. Helper method to call fit and predict once Parameters: Name Type Description Default signal array signal. Shape (n_samples, n_features) or (n_samples,). required n_bkps int Number of change points. Defaults to None. None pen float penalty value (>0). Defaults to None. Not considered if n_bkps is not None. None Returns: Type Description list sorted list of breakpoints Source code in ruptures/detection/kernelcpd.py def fit_predict ( self , signal , n_bkps = None , pen = None ): \"\"\"Fit to the signal and return the optimal breakpoints. Helper method to call fit and predict once Args: signal (array): signal. Shape (n_samples, n_features) or (n_samples,). n_bkps (int, optional): Number of change points. Defaults to None. pen (float, optional): penalty value (>0). Defaults to None. Not considered if n_bkps is not None. Returns: list: sorted list of breakpoints \"\"\" self . fit ( signal ) return self . predict ( n_bkps = n_bkps , pen = pen )","title":"fit_predict()"},{"location":"code-reference/detection/kernelcpd-reference/#ruptures.detection.kernelcpd.KernelCPD.predict","text":"Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to fit() . Parameters: Name Type Description Default n_bkps int Number of change points. Defaults to None. None pen float penalty value (>0). Defaults to None. Not considered if n_bkps is not None. None Exceptions: Type Description AssertionError if pen or n_bkps is not strictly positive. BadSegmentationParameters in case of impossible segmentation configuration Returns: Type Description list[int] sorted list of breakpoints Source code in ruptures/detection/kernelcpd.py def predict ( self , n_bkps = None , pen = None ): \"\"\"Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to [`fit()`][ruptures.detection.kernelcpd.KernelCPD.fit]. Args: n_bkps (int, optional): Number of change points. Defaults to None. pen (float, optional): penalty value (>0). Defaults to None. Not considered if n_bkps is not None. Raises: AssertionError: if `pen` or `n_bkps` is not strictly positive. BadSegmentationParameters: in case of impossible segmentation configuration Returns: list[int]: sorted list of breakpoints \"\"\" # Our KernelCPD implementation with Pelt implies that we have at least one change point # raise an exception in case of impossible segmentation configuration if not sanity_check ( n_samples = self . cost . signal . shape [ 0 ], n_bkps = 1 if n_bkps is None else n_bkps , jump = self . jump , min_size = self . min_size , ): raise BadSegmentationParameters # dynamic programming if the user passed a number change points if n_bkps is not None : n_bkps = int ( n_bkps ) err_msg = \"The number of changes must be positive: {} \" . format ( n_bkps ) assert n_bkps > 0 , err_msg # if we have already computed it, return it without computations. if n_bkps in self . segmentations_dict : return self . segmentations_dict [ n_bkps ] # otherwise, call the C function if self . kernel_name == \"linear\" : path_matrix_flat = ekcpd_L2 ( self . cost . signal , n_bkps , self . min_size ) elif self . kernel_name == \"rbf\" : path_matrix_flat = ekcpd_Gaussian ( self . cost . signal , n_bkps , self . min_size , self . cost . gamma ) elif self . kernel_name == \"cosine\" : path_matrix_flat = ekcpd_cosine ( self . cost . signal , n_bkps , self . min_size ) # from the path matrix, get all segmentation for k=1,...,n_bkps changes for k in range ( 1 , n_bkps + 1 ): self . segmentations_dict [ k ] = from_path_matrix_to_bkps_list ( path_matrix_flat , k , self . n_samples , n_bkps , self . jump ) return self . segmentations_dict [ n_bkps ] # Call pelt if the user passed a penalty if pen is not None : assert pen > 0 , \"The penalty must be positive: {} \" . format ( pen ) if self . kernel_name == \"linear\" : path_matrix = ekcpd_pelt_L2 ( self . cost . signal , pen , self . min_size ) elif self . kernel_name == \"rbf\" : path_matrix = ekcpd_pelt_Gaussian ( self . cost . signal , pen , self . min_size , self . cost . gamma ) elif self . kernel_name == \"cosine\" : path_matrix = ekcpd_pelt_cosine ( self . cost . signal , pen , self . min_size ) my_bkps = list () ind = self . n_samples while ind > 0 : my_bkps . append ( ind ) ind = path_matrix [ ind ] return my_bkps [:: - 1 ]","title":"predict()"},{"location":"code-reference/detection/pelt-reference/","text":"Pelt # ruptures.detection.pelt.Pelt # Penalized change point detection. For a given model and penalty level, computes the segmentation which minimizes the constrained sum of approximation errors. __init__ ( self , model = 'l2' , custom_cost = None , min_size = 2 , jump = 5 , params = None ) special # Initialize a Pelt instance. Parameters: Name Type Description Default model str segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if 'custom_cost' is not None. 'l2' custom_cost BaseCost custom cost function. Defaults to None. None min_size int minimum segment length. 2 jump int subsample (one every jump points). 5 params dict a dictionary of parameters for the cost instance. None Source code in ruptures/detection/pelt.py def __init__ ( self , model = \"l2\" , custom_cost = None , min_size = 2 , jump = 5 , params = None ): \"\"\"Initialize a Pelt instance. Args: model (str, optional): segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if ``'custom_cost'`` is not None. custom_cost (BaseCost, optional): custom cost function. Defaults to None. min_size (int, optional): minimum segment length. jump (int, optional): subsample (one every *jump* points). params (dict, optional): a dictionary of parameters for the cost instance. \"\"\" if custom_cost is not None and isinstance ( custom_cost , BaseCost ): self . cost = custom_cost else : if params is None : self . cost = cost_factory ( model = model ) else : self . cost = cost_factory ( model = model , ** params ) self . min_size = max ( min_size , self . cost . min_size ) self . jump = jump self . n_samples = None fit ( self , signal ) # Set params. Parameters: Name Type Description Default signal array signal to segment. Shape (n_samples, n_features) or (n_samples,). required Returns: Type Description Pelt self Source code in ruptures/detection/pelt.py def fit ( self , signal ) -> \"Pelt\" : \"\"\"Set params. Args: signal (array): signal to segment. Shape (n_samples, n_features) or (n_samples,). Returns: self \"\"\" # update params self . cost . fit ( signal ) if signal . ndim == 1 : ( n_samples ,) = signal . shape else : n_samples , _ = signal . shape self . n_samples = n_samples return self fit_predict ( self , signal , pen ) # Fit to the signal and return the optimal breakpoints. Helper method to call fit and predict once Parameters: Name Type Description Default signal array signal. Shape (n_samples, n_features) or (n_samples,). required pen float penalty value (>0) required Returns: Type Description list sorted list of breakpoints Source code in ruptures/detection/pelt.py def fit_predict ( self , signal , pen ): \"\"\"Fit to the signal and return the optimal breakpoints. Helper method to call fit and predict once Args: signal (array): signal. Shape (n_samples, n_features) or (n_samples,). pen (float): penalty value (>0) Returns: list: sorted list of breakpoints \"\"\" self . fit ( signal ) return self . predict ( pen ) predict ( self , pen ) # Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to fit() . Parameters: Name Type Description Default pen float penalty value (>0) required Exceptions: Type Description BadSegmentationParameters in case of impossible segmentation configuration Returns: Type Description list sorted list of breakpoints Source code in ruptures/detection/pelt.py def predict ( self , pen ): \"\"\"Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to [`fit()`][ruptures.detection.pelt.Pelt.fit]. Args: pen (float): penalty value (>0) Raises: BadSegmentationParameters: in case of impossible segmentation configuration Returns: list: sorted list of breakpoints \"\"\" # raise an exception in case of impossible segmentation configuration if not sanity_check ( n_samples = self . cost . signal . shape [ 0 ], n_bkps = 0 , jump = self . jump , min_size = self . min_size , ): raise BadSegmentationParameters partition = self . _seg ( pen ) bkps = sorted ( e for s , e in partition . keys ()) return bkps","title":"Pelt"},{"location":"code-reference/detection/pelt-reference/#pelt","text":"","title":"Pelt"},{"location":"code-reference/detection/pelt-reference/#ruptures.detection.pelt.Pelt","text":"Penalized change point detection. For a given model and penalty level, computes the segmentation which minimizes the constrained sum of approximation errors.","title":"Pelt"},{"location":"code-reference/detection/pelt-reference/#ruptures.detection.pelt.Pelt.__init__","text":"Initialize a Pelt instance. Parameters: Name Type Description Default model str segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if 'custom_cost' is not None. 'l2' custom_cost BaseCost custom cost function. Defaults to None. None min_size int minimum segment length. 2 jump int subsample (one every jump points). 5 params dict a dictionary of parameters for the cost instance. None Source code in ruptures/detection/pelt.py def __init__ ( self , model = \"l2\" , custom_cost = None , min_size = 2 , jump = 5 , params = None ): \"\"\"Initialize a Pelt instance. Args: model (str, optional): segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if ``'custom_cost'`` is not None. custom_cost (BaseCost, optional): custom cost function. Defaults to None. min_size (int, optional): minimum segment length. jump (int, optional): subsample (one every *jump* points). params (dict, optional): a dictionary of parameters for the cost instance. \"\"\" if custom_cost is not None and isinstance ( custom_cost , BaseCost ): self . cost = custom_cost else : if params is None : self . cost = cost_factory ( model = model ) else : self . cost = cost_factory ( model = model , ** params ) self . min_size = max ( min_size , self . cost . min_size ) self . jump = jump self . n_samples = None","title":"__init__()"},{"location":"code-reference/detection/pelt-reference/#ruptures.detection.pelt.Pelt.fit","text":"Set params. Parameters: Name Type Description Default signal array signal to segment. Shape (n_samples, n_features) or (n_samples,). required Returns: Type Description Pelt self Source code in ruptures/detection/pelt.py def fit ( self , signal ) -> \"Pelt\" : \"\"\"Set params. Args: signal (array): signal to segment. Shape (n_samples, n_features) or (n_samples,). Returns: self \"\"\" # update params self . cost . fit ( signal ) if signal . ndim == 1 : ( n_samples ,) = signal . shape else : n_samples , _ = signal . shape self . n_samples = n_samples return self","title":"fit()"},{"location":"code-reference/detection/pelt-reference/#ruptures.detection.pelt.Pelt.fit_predict","text":"Fit to the signal and return the optimal breakpoints. Helper method to call fit and predict once Parameters: Name Type Description Default signal array signal. Shape (n_samples, n_features) or (n_samples,). required pen float penalty value (>0) required Returns: Type Description list sorted list of breakpoints Source code in ruptures/detection/pelt.py def fit_predict ( self , signal , pen ): \"\"\"Fit to the signal and return the optimal breakpoints. Helper method to call fit and predict once Args: signal (array): signal. Shape (n_samples, n_features) or (n_samples,). pen (float): penalty value (>0) Returns: list: sorted list of breakpoints \"\"\" self . fit ( signal ) return self . predict ( pen )","title":"fit_predict()"},{"location":"code-reference/detection/pelt-reference/#ruptures.detection.pelt.Pelt.predict","text":"Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to fit() . Parameters: Name Type Description Default pen float penalty value (>0) required Exceptions: Type Description BadSegmentationParameters in case of impossible segmentation configuration Returns: Type Description list sorted list of breakpoints Source code in ruptures/detection/pelt.py def predict ( self , pen ): \"\"\"Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to [`fit()`][ruptures.detection.pelt.Pelt.fit]. Args: pen (float): penalty value (>0) Raises: BadSegmentationParameters: in case of impossible segmentation configuration Returns: list: sorted list of breakpoints \"\"\" # raise an exception in case of impossible segmentation configuration if not sanity_check ( n_samples = self . cost . signal . shape [ 0 ], n_bkps = 0 , jump = self . jump , min_size = self . min_size , ): raise BadSegmentationParameters partition = self . _seg ( pen ) bkps = sorted ( e for s , e in partition . keys ()) return bkps","title":"predict()"},{"location":"code-reference/detection/window-reference/","text":"Window-based change point detection # ruptures.detection.window.Window # Window sliding method. __init__ ( self , width = 100 , model = 'l2' , custom_cost = None , min_size = 2 , jump = 5 , params = None ) special # Instanciate with window length. Parameters: Name Type Description Default width int window length. Defaults to 100 samples. 100 model str segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if custom_cost is not None. 'l2' custom_cost BaseCost custom cost function. Defaults to None. None min_size int minimum segment length. 2 jump int subsample (one every jump points). 5 params dict a dictionary of parameters for the cost instance.` None Source code in ruptures/detection/window.py def __init__ ( self , width = 100 , model = \"l2\" , custom_cost = None , min_size = 2 , jump = 5 , params = None ): \"\"\"Instanciate with window length. Args: width (int, optional): window length. Defaults to 100 samples. model (str, optional): segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if `custom_cost` is not None. custom_cost (BaseCost, optional): custom cost function. Defaults to None. min_size (int, optional): minimum segment length. jump (int, optional): subsample (one every *jump* points). params (dict, optional): a dictionary of parameters for the cost instance.` \"\"\" self . min_size = min_size self . jump = jump self . width = 2 * ( width // 2 ) self . n_samples = None self . signal = None self . inds = None if custom_cost is not None and isinstance ( custom_cost , BaseCost ): self . cost = custom_cost else : if params is None : self . cost = cost_factory ( model = model ) else : self . cost = cost_factory ( model = model , ** params ) self . score = list () fit ( self , signal ) # Compute params to segment signal. Parameters: Name Type Description Default signal array signal to segment. Shape (n_samples, n_features) or (n_samples,). required Returns: Type Description Window self Source code in ruptures/detection/window.py def fit ( self , signal ) -> \"Window\" : \"\"\"Compute params to segment signal. Args: signal (array): signal to segment. Shape (n_samples, n_features) or (n_samples,). Returns: self \"\"\" # update some params if signal . ndim == 1 : self . signal = signal . reshape ( - 1 , 1 ) else : self . signal = signal self . n_samples , _ = self . signal . shape # indexes self . inds = np . arange ( self . n_samples , step = self . jump ) # delete borders keep = ( self . inds >= self . width // 2 ) & ( self . inds < self . n_samples - self . width // 2 ) self . inds = self . inds [ keep ] self . cost . fit ( signal ) # compute score score = list () for k in self . inds : start , end = k - self . width // 2 , k + self . width // 2 gain = self . cost . error ( start , end ) if np . isinf ( gain ) and gain < 0 : # segment is constant and no improvment possible on start .. end score . append ( 0 ) continue gain -= self . cost . error ( start , k ) + self . cost . error ( k , end ) score . append ( gain ) self . score = np . array ( score ) return self fit_predict ( self , signal , n_bkps = None , pen = None , epsilon = None ) # Helper method to call fit and predict once. Source code in ruptures/detection/window.py def fit_predict ( self , signal , n_bkps = None , pen = None , epsilon = None ): \"\"\"Helper method to call fit and predict once.\"\"\" self . fit ( signal ) return self . predict ( n_bkps = n_bkps , pen = pen , epsilon = epsilon ) predict ( self , n_bkps = None , pen = None , epsilon = None ) # Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to fit() . The stopping rule depends on the parameter passed to the function. Parameters: Name Type Description Default n_bkps int number of breakpoints to find before stopping. None pen float penalty value (>0) None epsilon float reconstruction budget (>0) None Exceptions: Type Description AssertionError if none of n_bkps , pen , epsilon is set. BadSegmentationParameters in case of impossible segmentation configuration Returns: Type Description list sorted list of breakpoints Source code in ruptures/detection/window.py def predict ( self , n_bkps = None , pen = None , epsilon = None ): \"\"\"Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to [`fit()`][ruptures.detection.window.Window.fit]. The stopping rule depends on the parameter passed to the function. Args: n_bkps (int): number of breakpoints to find before stopping. pen (float): penalty value (>0) epsilon (float): reconstruction budget (>0) Raises: AssertionError: if none of `n_bkps`, `pen`, `epsilon` is set. BadSegmentationParameters: in case of impossible segmentation configuration Returns: list: sorted list of breakpoints \"\"\" # raise an exception in case of impossible segmentation configuration if not sanity_check ( n_samples = self . cost . signal . shape [ 0 ], n_bkps = 0 if n_bkps is None else n_bkps , jump = self . jump , min_size = self . min_size , ): raise BadSegmentationParameters msg = \"Give a parameter.\" assert any ( param is not None for param in ( n_bkps , pen , epsilon )), msg bkps = self . _seg ( n_bkps = n_bkps , pen = pen , epsilon = epsilon ) return bkps","title":"Window"},{"location":"code-reference/detection/window-reference/#window-based-change-point-detection","text":"","title":"Window-based change point detection"},{"location":"code-reference/detection/window-reference/#ruptures.detection.window.Window","text":"Window sliding method.","title":"Window"},{"location":"code-reference/detection/window-reference/#ruptures.detection.window.Window.__init__","text":"Instanciate with window length. Parameters: Name Type Description Default width int window length. Defaults to 100 samples. 100 model str segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if custom_cost is not None. 'l2' custom_cost BaseCost custom cost function. Defaults to None. None min_size int minimum segment length. 2 jump int subsample (one every jump points). 5 params dict a dictionary of parameters for the cost instance.` None Source code in ruptures/detection/window.py def __init__ ( self , width = 100 , model = \"l2\" , custom_cost = None , min_size = 2 , jump = 5 , params = None ): \"\"\"Instanciate with window length. Args: width (int, optional): window length. Defaults to 100 samples. model (str, optional): segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if `custom_cost` is not None. custom_cost (BaseCost, optional): custom cost function. Defaults to None. min_size (int, optional): minimum segment length. jump (int, optional): subsample (one every *jump* points). params (dict, optional): a dictionary of parameters for the cost instance.` \"\"\" self . min_size = min_size self . jump = jump self . width = 2 * ( width // 2 ) self . n_samples = None self . signal = None self . inds = None if custom_cost is not None and isinstance ( custom_cost , BaseCost ): self . cost = custom_cost else : if params is None : self . cost = cost_factory ( model = model ) else : self . cost = cost_factory ( model = model , ** params ) self . score = list ()","title":"__init__()"},{"location":"code-reference/detection/window-reference/#ruptures.detection.window.Window.fit","text":"Compute params to segment signal. Parameters: Name Type Description Default signal array signal to segment. Shape (n_samples, n_features) or (n_samples,). required Returns: Type Description Window self Source code in ruptures/detection/window.py def fit ( self , signal ) -> \"Window\" : \"\"\"Compute params to segment signal. Args: signal (array): signal to segment. Shape (n_samples, n_features) or (n_samples,). Returns: self \"\"\" # update some params if signal . ndim == 1 : self . signal = signal . reshape ( - 1 , 1 ) else : self . signal = signal self . n_samples , _ = self . signal . shape # indexes self . inds = np . arange ( self . n_samples , step = self . jump ) # delete borders keep = ( self . inds >= self . width // 2 ) & ( self . inds < self . n_samples - self . width // 2 ) self . inds = self . inds [ keep ] self . cost . fit ( signal ) # compute score score = list () for k in self . inds : start , end = k - self . width // 2 , k + self . width // 2 gain = self . cost . error ( start , end ) if np . isinf ( gain ) and gain < 0 : # segment is constant and no improvment possible on start .. end score . append ( 0 ) continue gain -= self . cost . error ( start , k ) + self . cost . error ( k , end ) score . append ( gain ) self . score = np . array ( score ) return self","title":"fit()"},{"location":"code-reference/detection/window-reference/#ruptures.detection.window.Window.fit_predict","text":"Helper method to call fit and predict once. Source code in ruptures/detection/window.py def fit_predict ( self , signal , n_bkps = None , pen = None , epsilon = None ): \"\"\"Helper method to call fit and predict once.\"\"\" self . fit ( signal ) return self . predict ( n_bkps = n_bkps , pen = pen , epsilon = epsilon )","title":"fit_predict()"},{"location":"code-reference/detection/window-reference/#ruptures.detection.window.Window.predict","text":"Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to fit() . The stopping rule depends on the parameter passed to the function. Parameters: Name Type Description Default n_bkps int number of breakpoints to find before stopping. None pen float penalty value (>0) None epsilon float reconstruction budget (>0) None Exceptions: Type Description AssertionError if none of n_bkps , pen , epsilon is set. BadSegmentationParameters in case of impossible segmentation configuration Returns: Type Description list sorted list of breakpoints Source code in ruptures/detection/window.py def predict ( self , n_bkps = None , pen = None , epsilon = None ): \"\"\"Return the optimal breakpoints. Must be called after the fit method. The breakpoints are associated with the signal passed to [`fit()`][ruptures.detection.window.Window.fit]. The stopping rule depends on the parameter passed to the function. Args: n_bkps (int): number of breakpoints to find before stopping. pen (float): penalty value (>0) epsilon (float): reconstruction budget (>0) Raises: AssertionError: if none of `n_bkps`, `pen`, `epsilon` is set. BadSegmentationParameters: in case of impossible segmentation configuration Returns: list: sorted list of breakpoints \"\"\" # raise an exception in case of impossible segmentation configuration if not sanity_check ( n_samples = self . cost . signal . shape [ 0 ], n_bkps = 0 if n_bkps is None else n_bkps , jump = self . jump , min_size = self . min_size , ): raise BadSegmentationParameters msg = \"Give a parameter.\" assert any ( param is not None for param in ( n_bkps , pen , epsilon )), msg bkps = self . _seg ( n_bkps = n_bkps , pen = pen , epsilon = epsilon ) return bkps","title":"predict()"},{"location":"code-reference/metrics/hausdorff/","text":"Hausdorff metric ( hausdorff ) # ruptures . metrics . hausdorff . hausdorff ( bkps1 , bkps2 ) # Compute the Hausdorff distance between changepoints. Parameters: Name Type Description Default bkps1 list list of the last index of each regime. required bkps2 list list of the last index of each regime. required Returns: Type Description float Hausdorff distance. Source code in ruptures/metrics/hausdorff.py def hausdorff ( bkps1 , bkps2 ): \"\"\"Compute the Hausdorff distance between changepoints. Args: bkps1 (list): list of the last index of each regime. bkps2 (list): list of the last index of each regime. Returns: float: Hausdorff distance. \"\"\" sanity_check ( bkps1 , bkps2 ) bkps1_arr = np . array ( bkps1 [: - 1 ]) . reshape ( - 1 , 1 ) bkps2_arr = np . array ( bkps2 [: - 1 ]) . reshape ( - 1 , 1 ) pw_dist = cdist ( bkps1_arr , bkps2_arr ) res = max ( pw_dist . min ( axis = 0 ) . max (), pw_dist . min ( axis = 1 ) . max ()) return res","title":"Hausdorff metric"},{"location":"code-reference/metrics/hausdorff/#hausdorff-metric-hausdorff","text":"","title":"Hausdorff metric (hausdorff)"},{"location":"code-reference/metrics/hausdorff/#ruptures.metrics.hausdorff.hausdorff","text":"Compute the Hausdorff distance between changepoints. Parameters: Name Type Description Default bkps1 list list of the last index of each regime. required bkps2 list list of the last index of each regime. required Returns: Type Description float Hausdorff distance. Source code in ruptures/metrics/hausdorff.py def hausdorff ( bkps1 , bkps2 ): \"\"\"Compute the Hausdorff distance between changepoints. Args: bkps1 (list): list of the last index of each regime. bkps2 (list): list of the last index of each regime. Returns: float: Hausdorff distance. \"\"\" sanity_check ( bkps1 , bkps2 ) bkps1_arr = np . array ( bkps1 [: - 1 ]) . reshape ( - 1 , 1 ) bkps2_arr = np . array ( bkps2 [: - 1 ]) . reshape ( - 1 , 1 ) pw_dist = cdist ( bkps1_arr , bkps2_arr ) res = max ( pw_dist . min ( axis = 0 ) . max (), pw_dist . min ( axis = 1 ) . max ()) return res","title":"hausdorff()"},{"location":"code-reference/metrics/precisionrecall/","text":"Precision and recall ( precision_recall ) # ruptures . metrics . precisionrecall . precision_recall ( true_bkps , my_bkps , margin = 10 ) # Calculate the precision/recall of an estimated segmentation compared with the true segmentation. Parameters: Name Type Description Default true_bkps list list of the last index of each regime (true partition). required my_bkps list list of the last index of each regime (computed partition). required margin int allowed error (in points). 10 Returns: Type Description tuple (precision, recall) Source code in ruptures/metrics/precisionrecall.py def precision_recall ( true_bkps , my_bkps , margin = 10 ): \"\"\"Calculate the precision/recall of an estimated segmentation compared with the true segmentation. Args: true_bkps (list): list of the last index of each regime (true partition). my_bkps (list): list of the last index of each regime (computed partition). margin (int, optional): allowed error (in points). Returns: tuple: (precision, recall) \"\"\" sanity_check ( true_bkps , my_bkps ) assert margin > 0 , \"Margin of error must be positive (margin = {} )\" . format ( margin ) if len ( my_bkps ) == 1 : return 0 , 0 used = set () true_pos = set ( true_b for true_b , my_b in product ( true_bkps [: - 1 ], my_bkps [: - 1 ]) if my_b - margin < true_b < my_b + margin and not ( my_b in used or used . add ( my_b )) ) tp_ = len ( true_pos ) precision = tp_ / ( len ( my_bkps ) - 1 ) recall = tp_ / ( len ( true_bkps ) - 1 ) return precision , recall","title":"Precision and recall"},{"location":"code-reference/metrics/precisionrecall/#precision-and-recall-precision_recall","text":"","title":"Precision and recall (precision_recall)"},{"location":"code-reference/metrics/precisionrecall/#ruptures.metrics.precisionrecall.precision_recall","text":"Calculate the precision/recall of an estimated segmentation compared with the true segmentation. Parameters: Name Type Description Default true_bkps list list of the last index of each regime (true partition). required my_bkps list list of the last index of each regime (computed partition). required margin int allowed error (in points). 10 Returns: Type Description tuple (precision, recall) Source code in ruptures/metrics/precisionrecall.py def precision_recall ( true_bkps , my_bkps , margin = 10 ): \"\"\"Calculate the precision/recall of an estimated segmentation compared with the true segmentation. Args: true_bkps (list): list of the last index of each regime (true partition). my_bkps (list): list of the last index of each regime (computed partition). margin (int, optional): allowed error (in points). Returns: tuple: (precision, recall) \"\"\" sanity_check ( true_bkps , my_bkps ) assert margin > 0 , \"Margin of error must be positive (margin = {} )\" . format ( margin ) if len ( my_bkps ) == 1 : return 0 , 0 used = set () true_pos = set ( true_b for true_b , my_b in product ( true_bkps [: - 1 ], my_bkps [: - 1 ]) if my_b - margin < true_b < my_b + margin and not ( my_b in used or used . add ( my_b )) ) tp_ = len ( true_pos ) precision = tp_ / ( len ( my_bkps ) - 1 ) recall = tp_ / ( len ( true_bkps ) - 1 ) return precision , recall","title":"precision_recall()"},{"location":"code-reference/metrics/randindex/","text":"Rand index ( randindex ) # ruptures . metrics . randindex . randindex ( bkps1 , bkps2 ) # Computes the Rand index (between 0 and 1) between two segmentations. The Rand index (RI) measures the similarity between two segmentations and is equal to the proportion of aggreement between two partitions. RI is between 0 (total disagreement) and 1 (total agreement). This function uses the efficient implementation of [1]. [1] Prates, L. (2021). A more efficient algorithm to compute the Rand Index for change-point problems. ArXiv:2112.03738. Parameters: Name Type Description Default bkps1 list sorted list of the last index of each regime. required bkps2 list sorted list of the last index of each regime. required Returns: Type Description float Rand index Source code in ruptures/metrics/randindex.py def randindex ( bkps1 , bkps2 ): \"\"\"Computes the Rand index (between 0 and 1) between two segmentations. The Rand index (RI) measures the similarity between two segmentations and is equal to the proportion of aggreement between two partitions. RI is between 0 (total disagreement) and 1 (total agreement). This function uses the efficient implementation of [1]. [1] Prates, L. (2021). A more efficient algorithm to compute the Rand Index for change-point problems. ArXiv:2112.03738. Args: bkps1 (list): sorted list of the last index of each regime. bkps2 (list): sorted list of the last index of each regime. Returns: float: Rand index \"\"\" sanity_check ( bkps1 , bkps2 ) n_samples = bkps1 [ - 1 ] bkps1_with_0 = [ 0 ] + bkps1 bkps2_with_0 = [ 0 ] + bkps2 n_bkps1 = len ( bkps1 ) n_bkps2 = len ( bkps2 ) disagreement = 0 beginj : int = 0 # avoids unnecessary computations for index_bkps1 in range ( n_bkps1 ): start1 : int = bkps1_with_0 [ index_bkps1 ] end1 : int = bkps1_with_0 [ index_bkps1 + 1 ] for index_bkps2 in range ( beginj , n_bkps2 ): start2 : int = bkps2_with_0 [ index_bkps2 ] end2 : int = bkps2_with_0 [ index_bkps2 + 1 ] nij = max ( min ( end1 , end2 ) - max ( start1 , start2 ), 0 ) disagreement += nij * abs ( end1 - end2 ) # we can skip the rest of the iteration, nij will be 0 if end1 < end2 : break else : beginj = index_bkps2 + 1 disagreement /= n_samples * ( n_samples - 1 ) / 2 return 1.0 - disagreement","title":"Rand index"},{"location":"code-reference/metrics/randindex/#rand-index-randindex","text":"","title":"Rand index (randindex)"},{"location":"code-reference/metrics/randindex/#ruptures.metrics.randindex.randindex","text":"Computes the Rand index (between 0 and 1) between two segmentations. The Rand index (RI) measures the similarity between two segmentations and is equal to the proportion of aggreement between two partitions. RI is between 0 (total disagreement) and 1 (total agreement). This function uses the efficient implementation of [1]. [1] Prates, L. (2021). A more efficient algorithm to compute the Rand Index for change-point problems. ArXiv:2112.03738. Parameters: Name Type Description Default bkps1 list sorted list of the last index of each regime. required bkps2 list sorted list of the last index of each regime. required Returns: Type Description float Rand index Source code in ruptures/metrics/randindex.py def randindex ( bkps1 , bkps2 ): \"\"\"Computes the Rand index (between 0 and 1) between two segmentations. The Rand index (RI) measures the similarity between two segmentations and is equal to the proportion of aggreement between two partitions. RI is between 0 (total disagreement) and 1 (total agreement). This function uses the efficient implementation of [1]. [1] Prates, L. (2021). A more efficient algorithm to compute the Rand Index for change-point problems. ArXiv:2112.03738. Args: bkps1 (list): sorted list of the last index of each regime. bkps2 (list): sorted list of the last index of each regime. Returns: float: Rand index \"\"\" sanity_check ( bkps1 , bkps2 ) n_samples = bkps1 [ - 1 ] bkps1_with_0 = [ 0 ] + bkps1 bkps2_with_0 = [ 0 ] + bkps2 n_bkps1 = len ( bkps1 ) n_bkps2 = len ( bkps2 ) disagreement = 0 beginj : int = 0 # avoids unnecessary computations for index_bkps1 in range ( n_bkps1 ): start1 : int = bkps1_with_0 [ index_bkps1 ] end1 : int = bkps1_with_0 [ index_bkps1 + 1 ] for index_bkps2 in range ( beginj , n_bkps2 ): start2 : int = bkps2_with_0 [ index_bkps2 ] end2 : int = bkps2_with_0 [ index_bkps2 + 1 ] nij = max ( min ( end1 , end2 ) - max ( start1 , start2 ), 0 ) disagreement += nij * abs ( end1 - end2 ) # we can skip the rest of the iteration, nij will be 0 if end1 < end2 : break else : beginj = index_bkps2 + 1 disagreement /= n_samples * ( n_samples - 1 ) / 2 return 1.0 - disagreement","title":"randindex()"},{"location":"code-reference/show/display/","text":"Display ( display ) # ruptures . show . display . display ( signal , true_chg_pts , computed_chg_pts = None , computed_chg_pts_color = 'k' , computed_chg_pts_linewidth = 3 , computed_chg_pts_linestyle = '--' , computed_chg_pts_alpha = 1.0 , ** kwargs ) # Display a signal and the change points provided in alternating colors. If another set of change point is provided, they are displayed with dashed vertical dashed lines. The following matplotlib subplots options is set by default, but can be changed when calling display ): figure size figsize , defaults to (10, 2 * n_features) . Parameters: Name Type Description Default signal array signal array, shape (n_samples,) or (n_samples, n_features). required true_chg_pts list list of change point indexes. required computed_chg_pts list list of change point indexes. None computed_chg_pts_color str color of the lines indicating the computed_chg_pts. Defaults to \"k\". 'k' computed_chg_pts_linewidth int linewidth of the lines indicating the computed_chg_pts. Defaults to 3. 3 computed_chg_pts_linestyle str linestyle of the lines indicating the computed_chg_pts. Defaults to \"--\". '--' computed_chg_pts_alpha float alpha of the lines indicating the computed_chg_pts. Defaults to \"1.0\". 1.0 **kwargs all additional keyword arguments are passed to the plt.subplots call. {} Returns: Type Description tuple (figure, axarr) with a :class: matplotlib.figure.Figure object and an array of Axes objects. Source code in ruptures/show/display.py def display ( signal , true_chg_pts , computed_chg_pts = None , computed_chg_pts_color = \"k\" , computed_chg_pts_linewidth = 3 , computed_chg_pts_linestyle = \"--\" , computed_chg_pts_alpha = 1.0 , ** kwargs ): \"\"\"Display a signal and the change points provided in alternating colors. If another set of change point is provided, they are displayed with dashed vertical dashed lines. The following matplotlib subplots options is set by default, but can be changed when calling `display`): - figure size `figsize`, defaults to `(10, 2 * n_features)`. Args: signal (array): signal array, shape (n_samples,) or (n_samples, n_features). true_chg_pts (list): list of change point indexes. computed_chg_pts (list, optional): list of change point indexes. computed_chg_pts_color (str, optional): color of the lines indicating the computed_chg_pts. Defaults to \"k\". computed_chg_pts_linewidth (int, optional): linewidth of the lines indicating the computed_chg_pts. Defaults to 3. computed_chg_pts_linestyle (str, optional): linestyle of the lines indicating the computed_chg_pts. Defaults to \"--\". computed_chg_pts_alpha (float, optional): alpha of the lines indicating the computed_chg_pts. Defaults to \"1.0\". **kwargs : all additional keyword arguments are passed to the plt.subplots call. Returns: tuple: (figure, axarr) with a :class:`matplotlib.figure.Figure` object and an array of Axes objects. \"\"\" try : import matplotlib.pyplot as plt except ImportError : raise MatplotlibMissingError ( \"This feature requires the optional dependency matpotlib, you can install it using `pip install matplotlib`.\" ) if type ( signal ) != np . ndarray : # Try to get array from Pandas dataframe signal = signal . values if signal . ndim == 1 : signal = signal . reshape ( - 1 , 1 ) n_samples , n_features = signal . shape # let's set a sensible defaut size for the subplots matplotlib_options = { \"figsize\" : ( 10 , 2 * n_features ), # figure size } # add/update the options given by the user matplotlib_options . update ( kwargs ) # create plots fig , axarr = plt . subplots ( n_features , sharex = True , ** matplotlib_options ) if n_features == 1 : axarr = [ axarr ] for axe , sig in zip ( axarr , signal . T ): color_cycle = cycle ( COLOR_CYCLE ) # plot s axe . plot ( range ( n_samples ), sig ) # color each (true) regime bkps = [ 0 ] + sorted ( true_chg_pts ) alpha = 0.2 # transparency of the colored background for ( start , end ), col in zip ( pairwise ( bkps ), color_cycle ): axe . axvspan ( max ( 0 , start - 0.5 ), end - 0.5 , facecolor = col , alpha = alpha ) # vertical lines to mark the computed_chg_pts if computed_chg_pts is not None : for bkp in computed_chg_pts : if bkp != 0 and bkp < n_samples : axe . axvline ( x = bkp - 0.5 , color = computed_chg_pts_color , linewidth = computed_chg_pts_linewidth , linestyle = computed_chg_pts_linestyle , alpha = computed_chg_pts_alpha , ) fig . tight_layout () return fig , axarr","title":"Display function"},{"location":"code-reference/show/display/#display-display","text":"","title":"Display (display)"},{"location":"code-reference/show/display/#ruptures.show.display.display","text":"Display a signal and the change points provided in alternating colors. If another set of change point is provided, they are displayed with dashed vertical dashed lines. The following matplotlib subplots options is set by default, but can be changed when calling display ): figure size figsize , defaults to (10, 2 * n_features) . Parameters: Name Type Description Default signal array signal array, shape (n_samples,) or (n_samples, n_features). required true_chg_pts list list of change point indexes. required computed_chg_pts list list of change point indexes. None computed_chg_pts_color str color of the lines indicating the computed_chg_pts. Defaults to \"k\". 'k' computed_chg_pts_linewidth int linewidth of the lines indicating the computed_chg_pts. Defaults to 3. 3 computed_chg_pts_linestyle str linestyle of the lines indicating the computed_chg_pts. Defaults to \"--\". '--' computed_chg_pts_alpha float alpha of the lines indicating the computed_chg_pts. Defaults to \"1.0\". 1.0 **kwargs all additional keyword arguments are passed to the plt.subplots call. {} Returns: Type Description tuple (figure, axarr) with a :class: matplotlib.figure.Figure object and an array of Axes objects. Source code in ruptures/show/display.py def display ( signal , true_chg_pts , computed_chg_pts = None , computed_chg_pts_color = \"k\" , computed_chg_pts_linewidth = 3 , computed_chg_pts_linestyle = \"--\" , computed_chg_pts_alpha = 1.0 , ** kwargs ): \"\"\"Display a signal and the change points provided in alternating colors. If another set of change point is provided, they are displayed with dashed vertical dashed lines. The following matplotlib subplots options is set by default, but can be changed when calling `display`): - figure size `figsize`, defaults to `(10, 2 * n_features)`. Args: signal (array): signal array, shape (n_samples,) or (n_samples, n_features). true_chg_pts (list): list of change point indexes. computed_chg_pts (list, optional): list of change point indexes. computed_chg_pts_color (str, optional): color of the lines indicating the computed_chg_pts. Defaults to \"k\". computed_chg_pts_linewidth (int, optional): linewidth of the lines indicating the computed_chg_pts. Defaults to 3. computed_chg_pts_linestyle (str, optional): linestyle of the lines indicating the computed_chg_pts. Defaults to \"--\". computed_chg_pts_alpha (float, optional): alpha of the lines indicating the computed_chg_pts. Defaults to \"1.0\". **kwargs : all additional keyword arguments are passed to the plt.subplots call. Returns: tuple: (figure, axarr) with a :class:`matplotlib.figure.Figure` object and an array of Axes objects. \"\"\" try : import matplotlib.pyplot as plt except ImportError : raise MatplotlibMissingError ( \"This feature requires the optional dependency matpotlib, you can install it using `pip install matplotlib`.\" ) if type ( signal ) != np . ndarray : # Try to get array from Pandas dataframe signal = signal . values if signal . ndim == 1 : signal = signal . reshape ( - 1 , 1 ) n_samples , n_features = signal . shape # let's set a sensible defaut size for the subplots matplotlib_options = { \"figsize\" : ( 10 , 2 * n_features ), # figure size } # add/update the options given by the user matplotlib_options . update ( kwargs ) # create plots fig , axarr = plt . subplots ( n_features , sharex = True , ** matplotlib_options ) if n_features == 1 : axarr = [ axarr ] for axe , sig in zip ( axarr , signal . T ): color_cycle = cycle ( COLOR_CYCLE ) # plot s axe . plot ( range ( n_samples ), sig ) # color each (true) regime bkps = [ 0 ] + sorted ( true_chg_pts ) alpha = 0.2 # transparency of the colored background for ( start , end ), col in zip ( pairwise ( bkps ), color_cycle ): axe . axvspan ( max ( 0 , start - 0.5 ), end - 0.5 , facecolor = col , alpha = alpha ) # vertical lines to mark the computed_chg_pts if computed_chg_pts is not None : for bkp in computed_chg_pts : if bkp != 0 and bkp < n_samples : axe . axvline ( x = bkp - 0.5 , color = computed_chg_pts_color , linewidth = computed_chg_pts_linewidth , linestyle = computed_chg_pts_linestyle , alpha = computed_chg_pts_alpha , ) fig . tight_layout () return fig , axarr","title":"display()"},{"location":"examples/basic-usage/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Basic usage # Info Try this notebook in an executable environment with Binder . Download this notebook here . Let us start with a simple example to illustrate the use of ruptures : generate a 3-dimensional piecewise constant signal with noise and estimate the change points. Setup # First, we make the necessary imports. import matplotlib.pyplot as plt # for display purposes import ruptures as rpt # our package Generate and display the signal # Let us generate a 3-dimensional piecewise constant signal with Gaussian noise. n_samples , n_dims , sigma = 1000 , 3 , 2 n_bkps = 4 # number of breakpoints signal , bkps = rpt . pw_constant ( n_samples , n_dims , n_bkps , noise_std = sigma ) The true change points of this synthetic signal are available in the bkps variable. print ( bkps ) [193, 382, 586, 798, 1000] Note that the first four element are change point indexes while the last is simply the number of samples. (This is a technical convention so that functions in ruptures always know the length of the signal at hand.) It is also possible to plot our \\(\\mathbb{R}^3\\) -valued signal along with the true change points with the rpt.display function. In the following image, the color changes whenever the mean of the signal shifts. fig , ax_array = rpt . display ( signal , bkps ) Change point detection # We can now perform change point detection, meaning that we find the indexes where the signal mean changes. To that end, we minimize the sum of squared errors when approximating the signal by a piecewise constant signal. Formally, for a signal \\(y_0,y_1,\\dots,y_{T-1}\\) ( \\(T\\) samples), we solve the following optimization problem, over all possible change positions \\(t_1<t_2<\\dots<t_K\\) (where the number \\(K\\) of changes is defined by the user): \\[ \\hat{t}_1, \\hat{t}_2,\\dots,\\hat{t}_K = \\arg\\min_{t_1,\\dots,t_K} V(t_1,t_2,\\dots,t_K) \\] with \\[ V(t_1,t_2,\\dots,t_K) := \\sum_{k=0}^K\\sum_{t=t_k}^{t_{k+1}-1} \\|y_t-\\bar{y}_{t_k..t_{k+1}}\\|^2 \\] where \\(\\bar{y}_{t_k..t_{k+1}}\\) is the empirical mean of the sub-signal \\(y_{t_k}, y_{t_k+1},\\dots,y_{t_{k+1}-1}\\) . (By convention \\(t_0=0\\) and \\(t_{K+1}=T\\) .) This optimization is solved with dynamic programming, using the Dynp class. (More information in the section What is change point detection? and the User guide .) # detection algo = rpt . Dynp ( model = \"l2\" ) . fit ( signal ) result = algo . predict ( n_bkps = 4 ) print ( result ) [195, 380, 585, 800, 1000] Again the first elements are change point indexes and the last is the number of samples. Display the results # To visualy compare the true segmentation ( bkps ) and the estimated one ( result ), we can resort to rpt.display a second time. In the following image, the alternating colors indicate the true breakpoints and the dashed vertical lines, the estimated breakpoints. # display rpt . display ( signal , bkps , result ) plt . show () In this simple example, both are quite similar and almost undistinguishable.","title":"Basic usage"},{"location":"examples/basic-usage/#basic-usage","text":"Info Try this notebook in an executable environment with Binder . Download this notebook here . Let us start with a simple example to illustrate the use of ruptures : generate a 3-dimensional piecewise constant signal with noise and estimate the change points.","title":"Basic usage"},{"location":"examples/basic-usage/#setup","text":"First, we make the necessary imports. import matplotlib.pyplot as plt # for display purposes import ruptures as rpt # our package","title":"Setup"},{"location":"examples/basic-usage/#generate-and-display-the-signal","text":"Let us generate a 3-dimensional piecewise constant signal with Gaussian noise. n_samples , n_dims , sigma = 1000 , 3 , 2 n_bkps = 4 # number of breakpoints signal , bkps = rpt . pw_constant ( n_samples , n_dims , n_bkps , noise_std = sigma ) The true change points of this synthetic signal are available in the bkps variable. print ( bkps ) [193, 382, 586, 798, 1000] Note that the first four element are change point indexes while the last is simply the number of samples. (This is a technical convention so that functions in ruptures always know the length of the signal at hand.) It is also possible to plot our \\(\\mathbb{R}^3\\) -valued signal along with the true change points with the rpt.display function. In the following image, the color changes whenever the mean of the signal shifts. fig , ax_array = rpt . display ( signal , bkps )","title":"Generate and display the signal"},{"location":"examples/basic-usage/#change-point-detection","text":"We can now perform change point detection, meaning that we find the indexes where the signal mean changes. To that end, we minimize the sum of squared errors when approximating the signal by a piecewise constant signal. Formally, for a signal \\(y_0,y_1,\\dots,y_{T-1}\\) ( \\(T\\) samples), we solve the following optimization problem, over all possible change positions \\(t_1<t_2<\\dots<t_K\\) (where the number \\(K\\) of changes is defined by the user): \\[ \\hat{t}_1, \\hat{t}_2,\\dots,\\hat{t}_K = \\arg\\min_{t_1,\\dots,t_K} V(t_1,t_2,\\dots,t_K) \\] with \\[ V(t_1,t_2,\\dots,t_K) := \\sum_{k=0}^K\\sum_{t=t_k}^{t_{k+1}-1} \\|y_t-\\bar{y}_{t_k..t_{k+1}}\\|^2 \\] where \\(\\bar{y}_{t_k..t_{k+1}}\\) is the empirical mean of the sub-signal \\(y_{t_k}, y_{t_k+1},\\dots,y_{t_{k+1}-1}\\) . (By convention \\(t_0=0\\) and \\(t_{K+1}=T\\) .) This optimization is solved with dynamic programming, using the Dynp class. (More information in the section What is change point detection? and the User guide .) # detection algo = rpt . Dynp ( model = \"l2\" ) . fit ( signal ) result = algo . predict ( n_bkps = 4 ) print ( result ) [195, 380, 585, 800, 1000] Again the first elements are change point indexes and the last is the number of samples.","title":"Change point detection"},{"location":"examples/basic-usage/#display-the-results","text":"To visualy compare the true segmentation ( bkps ) and the estimated one ( result ), we can resort to rpt.display a second time. In the following image, the alternating colors indicate the true breakpoints and the dashed vertical lines, the estimated breakpoints. # display rpt . display ( signal , bkps , result ) plt . show () In this simple example, both are quite similar and almost undistinguishable.","title":"Display the results"},{"location":"examples/introduction/","text":"Gallery of examples # These examples illustrate the main features of the ruptures package. Simple examples are direct applications of the library's functions on simulated data. Advanced examples deal with more complex tasks, such as calibration and real-world data.","title":"Gallery of examples"},{"location":"examples/introduction/#gallery-of-examples","text":"These examples illustrate the main features of the ruptures package. Simple examples are direct applications of the library's functions on simulated data. Advanced examples deal with more complex tasks, such as calibration and real-world data.","title":"Gallery of examples"},{"location":"examples/kernel-cpd-performance-comparison/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Kernel change point detection: a performance comparison # Info Try this notebook in an executable environment with Binder . Download this notebook here . Introduction # In ruptures , there are two ways to perform kernel change point detection: by using the pure Python classes Dynp (known number of change points) and Pelt (unknown number of change points), by using the faster class (implemented in C) KernelCPD which contains both the dynamic programming approach and the penalized approach (PELT). This example illustrates the performance of the fast C implementation compared to the pure Python one. The kernel change point detection setting is briefly described in the user guide . The interested reader can refer to [ Celisse2018 , Arlot2019 ] for a more complete introduction. The list of available kernels is available here , but in this example we only consider two: the linear kernel, \\(k_{\\text{linear}}(x, y) = x^T y\\) (Euclidean scalar product) and the induced norm is the Euclidean norm; the Gaussian kernel (also known as radial basis function, rbf), \\(k_{\\text{Gaussian}}(x,y)=\\exp(-\\gamma \\|x-y\\|^2)\\) where \\(\\|\\cdot\\|\\) is the Euclidean norm and \\(\\gamma>0\\) is a user-defined parameter. Setup # First, we make the necessary imports and generate a toy signal import time # for execution time comparison import matplotlib.pyplot as plt # for display purposes import ruptures as rpt # our package from ruptures.metrics import hausdorff # generate signal n_samples , dim , sigma = 500 , 3 , 3 n_bkps = 6 # number of breakpoints signal , bkps = rpt . pw_constant ( n_samples , dim , n_bkps , noise_std = sigma ) fig , ax_array = rpt . display ( signal , bkps ) Linear kernel # The linear kernel (see above) \\(k_{\\text{linear}}\\) can detect changes in the mean of a signal. It also corresponds to the cost function CostL2 . Dynamic programming # When the number of changes to detect is known beforehand, we use dynamic programming. algo_python = rpt . Dynp ( model = \"l2\" , jump = 1 , min_size = 2 ) . fit ( signal ) # written in pure python algo_c = rpt . KernelCPD ( kernel = \"linear\" , min_size = 2 ) . fit ( signal ) # written in C for ( label , algo ) in zip ( ( \"Python implementation\" , \"C implementation\" ), ( algo_python , algo_c ) ): start_time = time . time () result = algo . predict ( n_bkps = n_bkps ) print ( f \" { label } : \\t { time . time () - start_time : .3f } s\" ) Python implementation: 6.751 s C implementation: 0.007 s The speed-up is quite significant and depends on the signal size (number \\(T\\) of samples and dimension \\(d\\) ) and the number \\(K\\) of change points to detect. The C implementation has a time complexity of the order \\(\\mathcal{O}(KdT^2)\\) and space complexity of the order \\(\\mathcal{O}(T)\\) . As to the Python implementation, the complexities in time and space are of the order \\(\\mathcal{O}(KdT^3)\\) and \\(\\mathcal{O}(T^2)\\) respectively. We can also check that both methods return the same set of change points. bkps_python = algo_python . predict ( n_bkps = n_bkps ) bkps_c = algo_c . predict ( n_bkps = n_bkps ) print ( f \"Python implementation: \\t { bkps_python } \" ) print ( f \"C implementation: \\t { bkps_c } \" ) print ( f \"(Hausdorff distance: { hausdorff ( bkps_python , bkps_c ) : .0f } samples)\" ) Python implementation: [71, 138, 207, 284, 354, 431, 500] C implementation: [71, 138, 207, 284, 354, 431, 500] (Hausdorff distance: 0 samples) PELT # When the number of changes to detect is unknown, we resort to PELT [Killick2012] to solve the penalized detection problem. algo_python = rpt . Pelt ( model = \"l2\" , jump = 1 , min_size = 2 ) . fit ( signal ) # written in pure python algo_c = rpt . KernelCPD ( kernel = \"linear\" , min_size = 2 ) . fit ( signal ) # written in C, same class as before penalty_value = 100 # beta for ( label , algo ) in zip ( ( \"Python implementation\" , \"C implementation\" ), ( algo_python , algo_c ) ): start_time = time . time () result = algo . predict ( pen = penalty_value ) print ( f \" { label } : \\t { time . time () - start_time : .3f } s\" ) Python implementation: 0.390 s C implementation: 0.001 s Again, the speed-up is quite significant and depends on the signal size (number \\(T\\) of samples and dimension \\(d\\) ) and the penalty value \\(\\beta\\) . We remark that, for both Python and C implementations, PELT is more efficient then dynamic programming. We can also check that both methods return the same set of change points. bkps_python = algo_python . predict ( pen = penalty_value ) bkps_c = algo_c . predict ( pen = penalty_value ) print ( f \"Python implementation: \\t { bkps_python } \" ) print ( f \"C implementation: \\t { bkps_c } \" ) print ( f \"(Hausdorff distance: { hausdorff ( bkps_python , bkps_c ) : .0f } samples)\" ) Python implementation: [68, 73, 138, 207, 284, 354, 431, 500] C implementation: [68, 73, 138, 207, 284, 354, 431, 500] (Hausdorff distance: 0 samples) Note By default, Dynp and Pelt has jump=5 . In KernelCPD , jump=1 and cannot be changed. This is because, in the C implementation, changing the jump does not improve the running time significatively, while it does in the Python implementation. Gaussian kernel # The Gaussian kernel (see above) \\(k_{\\text{Gaussian}}\\) can detect changes in the distribution of an i.i.d. process. This is a feature of several kernel functions (in particular characteristics kernels; see [Gretton2012] for more information). It also corresponds to the cost function CostRbf . Dynamic programming # When the number of changes to detect is known beforehand, we use dynamic programming. params = { \"gamma\" : 1e-2 } algo_python = rpt . Dynp ( model = \"rbf\" , params = params , jump = 1 , min_size = 2 ) . fit ( signal ) # written in pure python algo_c = rpt . KernelCPD ( kernel = \"rbf\" , params = params , min_size = 2 ) . fit ( signal ) # written in C for ( label , algo ) in zip ( ( \"Python implementation\" , \"C implementation\" ), ( algo_python , algo_c ) ): start_time = time . time () result = algo . predict ( n_bkps = n_bkps ) print ( f \" { label } : \\t { time . time () - start_time : .3f } s\" ) Python implementation: 7.259 s C implementation: 0.012 s Again, the speed-up is quite significant. The C implementation has a time complexity of the order \\(\\mathcal{O}(CKT^2)\\) and space complexity of the order \\(\\mathcal{O}(T)\\) , where \\(C\\) is the complexity of computing \\(k(y_s, y_t)\\) once. As to the Python implementation, the complexities in time and space are of the order \\(\\mathcal{O}(CKT^4)\\) and \\(\\mathcal{O}(T^2)\\) respectively. We can also check that both methods return the same set of change points. bkps_python = algo_python . predict ( n_bkps = n_bkps ) bkps_c = algo_c . predict ( n_bkps = n_bkps ) print ( f \"Python implementation: \\t { bkps_python } \" ) print ( f \"C implementation: \\t { bkps_c } \" ) print ( f \"(Hausdorff distance: { hausdorff ( bkps_python , bkps_c ) : .0f } samples)\" ) Python implementation: [71, 138, 207, 284, 354, 430, 500] C implementation: [71, 138, 207, 284, 354, 430, 500] (Hausdorff distance: 0 samples) Note If not provided by the user, the gamma parameter is chosen using the median heuristics, meaning that it is set to inverse of the median of all pairwise products \\(k(y_s, y_t)\\) . PELT # When the number of changes to detect is unknown, we resort to PELT to solve the penalized detection problem. algo_python = rpt . Pelt ( model = \"rbf\" , jump = 1 , min_size = 2 ) . fit ( signal ) # written in pure python algo_c = rpt . KernelCPD ( kernel = \"rbf\" , min_size = 2 ) . fit ( signal ) # written in C, same class as before penalty_value = 1 # beta for ( label , algo ) in zip ( ( \"Python implementation\" , \"C implementation\" ), ( algo_python , algo_c ) ): start_time = time . time () result = algo . predict ( pen = penalty_value ) print ( f \" { label } : \\t { time . time () - start_time : .3f } s\" ) Python implementation: 0.212 s C implementation: 0.002 s Again, the speed-up is quite significant and depends on the signal size (number \\(T\\) of samples and dimension \\(d\\) ) and the penalty value \\(\\beta\\) . We remark that, for both Python and C implementations, PELT is more efficient then dynamic programming. We can also check that both methods return the same set of change points. bkps_python = algo_python . predict ( pen = penalty_value ) bkps_c = algo_c . predict ( pen = penalty_value ) print ( f \"Python implementation: \\t { bkps_python } \" ) print ( f \"C implementation: \\t { bkps_c } \" ) print ( f \"(Hausdorff distance: { hausdorff ( bkps_python , bkps_c ) : .0f } samples)\" ) Python implementation: [71, 138, 207, 284, 354, 430, 500] C implementation: [71, 138, 207, 284, 354, 430, 500] (Hausdorff distance: 0 samples) References # [Gretton2012] Gretton, A., Borgwardt, K. M., Rasch, M. J., Sch\u00f6lkopf, B., & Smola, A. (2012). A kernel two-sample test. The Journal of Machine Learning Research, 13, 723\u2013773. [Killick2012] Killick, R., Fearnhead, P., & Eckley, I. (2012). Optimal detection of changepoints with a linear computational cost. Journal of the American Statistical Association, 107(500), 1590\u20131598. [Celisse2018] Celisse, A., Marot, G., Pierre-Jean, M., & Rigaill, G. (2018). New efficient algorithms for multiple change-point detection with reproducing kernels. Computational Statistics and Data Analysis, 128, 200\u2013220. [Arlot2019] Arlot, S., Celisse, A., & Harchaoui, Z. (2019). A kernel multiple change-point algorithm via model selection. Journal of Machine Learning Research, 20(162), 1\u201356.","title":"Kernel change point detection: a performance comparison"},{"location":"examples/kernel-cpd-performance-comparison/#kernel-change-point-detection-a-performance-comparison","text":"Info Try this notebook in an executable environment with Binder . Download this notebook here .","title":"Kernel change point detection: a performance comparison"},{"location":"examples/kernel-cpd-performance-comparison/#introduction","text":"In ruptures , there are two ways to perform kernel change point detection: by using the pure Python classes Dynp (known number of change points) and Pelt (unknown number of change points), by using the faster class (implemented in C) KernelCPD which contains both the dynamic programming approach and the penalized approach (PELT). This example illustrates the performance of the fast C implementation compared to the pure Python one. The kernel change point detection setting is briefly described in the user guide . The interested reader can refer to [ Celisse2018 , Arlot2019 ] for a more complete introduction. The list of available kernels is available here , but in this example we only consider two: the linear kernel, \\(k_{\\text{linear}}(x, y) = x^T y\\) (Euclidean scalar product) and the induced norm is the Euclidean norm; the Gaussian kernel (also known as radial basis function, rbf), \\(k_{\\text{Gaussian}}(x,y)=\\exp(-\\gamma \\|x-y\\|^2)\\) where \\(\\|\\cdot\\|\\) is the Euclidean norm and \\(\\gamma>0\\) is a user-defined parameter.","title":"Introduction"},{"location":"examples/kernel-cpd-performance-comparison/#setup","text":"First, we make the necessary imports and generate a toy signal import time # for execution time comparison import matplotlib.pyplot as plt # for display purposes import ruptures as rpt # our package from ruptures.metrics import hausdorff # generate signal n_samples , dim , sigma = 500 , 3 , 3 n_bkps = 6 # number of breakpoints signal , bkps = rpt . pw_constant ( n_samples , dim , n_bkps , noise_std = sigma ) fig , ax_array = rpt . display ( signal , bkps )","title":"Setup"},{"location":"examples/kernel-cpd-performance-comparison/#linear-kernel","text":"The linear kernel (see above) \\(k_{\\text{linear}}\\) can detect changes in the mean of a signal. It also corresponds to the cost function CostL2 .","title":"Linear kernel"},{"location":"examples/kernel-cpd-performance-comparison/#dynamic-programming","text":"When the number of changes to detect is known beforehand, we use dynamic programming. algo_python = rpt . Dynp ( model = \"l2\" , jump = 1 , min_size = 2 ) . fit ( signal ) # written in pure python algo_c = rpt . KernelCPD ( kernel = \"linear\" , min_size = 2 ) . fit ( signal ) # written in C for ( label , algo ) in zip ( ( \"Python implementation\" , \"C implementation\" ), ( algo_python , algo_c ) ): start_time = time . time () result = algo . predict ( n_bkps = n_bkps ) print ( f \" { label } : \\t { time . time () - start_time : .3f } s\" ) Python implementation: 6.751 s C implementation: 0.007 s The speed-up is quite significant and depends on the signal size (number \\(T\\) of samples and dimension \\(d\\) ) and the number \\(K\\) of change points to detect. The C implementation has a time complexity of the order \\(\\mathcal{O}(KdT^2)\\) and space complexity of the order \\(\\mathcal{O}(T)\\) . As to the Python implementation, the complexities in time and space are of the order \\(\\mathcal{O}(KdT^3)\\) and \\(\\mathcal{O}(T^2)\\) respectively. We can also check that both methods return the same set of change points. bkps_python = algo_python . predict ( n_bkps = n_bkps ) bkps_c = algo_c . predict ( n_bkps = n_bkps ) print ( f \"Python implementation: \\t { bkps_python } \" ) print ( f \"C implementation: \\t { bkps_c } \" ) print ( f \"(Hausdorff distance: { hausdorff ( bkps_python , bkps_c ) : .0f } samples)\" ) Python implementation: [71, 138, 207, 284, 354, 431, 500] C implementation: [71, 138, 207, 284, 354, 431, 500] (Hausdorff distance: 0 samples)","title":"Dynamic programming"},{"location":"examples/kernel-cpd-performance-comparison/#pelt","text":"When the number of changes to detect is unknown, we resort to PELT [Killick2012] to solve the penalized detection problem. algo_python = rpt . Pelt ( model = \"l2\" , jump = 1 , min_size = 2 ) . fit ( signal ) # written in pure python algo_c = rpt . KernelCPD ( kernel = \"linear\" , min_size = 2 ) . fit ( signal ) # written in C, same class as before penalty_value = 100 # beta for ( label , algo ) in zip ( ( \"Python implementation\" , \"C implementation\" ), ( algo_python , algo_c ) ): start_time = time . time () result = algo . predict ( pen = penalty_value ) print ( f \" { label } : \\t { time . time () - start_time : .3f } s\" ) Python implementation: 0.390 s C implementation: 0.001 s Again, the speed-up is quite significant and depends on the signal size (number \\(T\\) of samples and dimension \\(d\\) ) and the penalty value \\(\\beta\\) . We remark that, for both Python and C implementations, PELT is more efficient then dynamic programming. We can also check that both methods return the same set of change points. bkps_python = algo_python . predict ( pen = penalty_value ) bkps_c = algo_c . predict ( pen = penalty_value ) print ( f \"Python implementation: \\t { bkps_python } \" ) print ( f \"C implementation: \\t { bkps_c } \" ) print ( f \"(Hausdorff distance: { hausdorff ( bkps_python , bkps_c ) : .0f } samples)\" ) Python implementation: [68, 73, 138, 207, 284, 354, 431, 500] C implementation: [68, 73, 138, 207, 284, 354, 431, 500] (Hausdorff distance: 0 samples) Note By default, Dynp and Pelt has jump=5 . In KernelCPD , jump=1 and cannot be changed. This is because, in the C implementation, changing the jump does not improve the running time significatively, while it does in the Python implementation.","title":"PELT"},{"location":"examples/kernel-cpd-performance-comparison/#gaussian-kernel","text":"The Gaussian kernel (see above) \\(k_{\\text{Gaussian}}\\) can detect changes in the distribution of an i.i.d. process. This is a feature of several kernel functions (in particular characteristics kernels; see [Gretton2012] for more information). It also corresponds to the cost function CostRbf .","title":"Gaussian kernel"},{"location":"examples/kernel-cpd-performance-comparison/#dynamic-programming_1","text":"When the number of changes to detect is known beforehand, we use dynamic programming. params = { \"gamma\" : 1e-2 } algo_python = rpt . Dynp ( model = \"rbf\" , params = params , jump = 1 , min_size = 2 ) . fit ( signal ) # written in pure python algo_c = rpt . KernelCPD ( kernel = \"rbf\" , params = params , min_size = 2 ) . fit ( signal ) # written in C for ( label , algo ) in zip ( ( \"Python implementation\" , \"C implementation\" ), ( algo_python , algo_c ) ): start_time = time . time () result = algo . predict ( n_bkps = n_bkps ) print ( f \" { label } : \\t { time . time () - start_time : .3f } s\" ) Python implementation: 7.259 s C implementation: 0.012 s Again, the speed-up is quite significant. The C implementation has a time complexity of the order \\(\\mathcal{O}(CKT^2)\\) and space complexity of the order \\(\\mathcal{O}(T)\\) , where \\(C\\) is the complexity of computing \\(k(y_s, y_t)\\) once. As to the Python implementation, the complexities in time and space are of the order \\(\\mathcal{O}(CKT^4)\\) and \\(\\mathcal{O}(T^2)\\) respectively. We can also check that both methods return the same set of change points. bkps_python = algo_python . predict ( n_bkps = n_bkps ) bkps_c = algo_c . predict ( n_bkps = n_bkps ) print ( f \"Python implementation: \\t { bkps_python } \" ) print ( f \"C implementation: \\t { bkps_c } \" ) print ( f \"(Hausdorff distance: { hausdorff ( bkps_python , bkps_c ) : .0f } samples)\" ) Python implementation: [71, 138, 207, 284, 354, 430, 500] C implementation: [71, 138, 207, 284, 354, 430, 500] (Hausdorff distance: 0 samples) Note If not provided by the user, the gamma parameter is chosen using the median heuristics, meaning that it is set to inverse of the median of all pairwise products \\(k(y_s, y_t)\\) .","title":"Dynamic programming"},{"location":"examples/kernel-cpd-performance-comparison/#pelt_1","text":"When the number of changes to detect is unknown, we resort to PELT to solve the penalized detection problem. algo_python = rpt . Pelt ( model = \"rbf\" , jump = 1 , min_size = 2 ) . fit ( signal ) # written in pure python algo_c = rpt . KernelCPD ( kernel = \"rbf\" , min_size = 2 ) . fit ( signal ) # written in C, same class as before penalty_value = 1 # beta for ( label , algo ) in zip ( ( \"Python implementation\" , \"C implementation\" ), ( algo_python , algo_c ) ): start_time = time . time () result = algo . predict ( pen = penalty_value ) print ( f \" { label } : \\t { time . time () - start_time : .3f } s\" ) Python implementation: 0.212 s C implementation: 0.002 s Again, the speed-up is quite significant and depends on the signal size (number \\(T\\) of samples and dimension \\(d\\) ) and the penalty value \\(\\beta\\) . We remark that, for both Python and C implementations, PELT is more efficient then dynamic programming. We can also check that both methods return the same set of change points. bkps_python = algo_python . predict ( pen = penalty_value ) bkps_c = algo_c . predict ( pen = penalty_value ) print ( f \"Python implementation: \\t { bkps_python } \" ) print ( f \"C implementation: \\t { bkps_c } \" ) print ( f \"(Hausdorff distance: { hausdorff ( bkps_python , bkps_c ) : .0f } samples)\" ) Python implementation: [71, 138, 207, 284, 354, 430, 500] C implementation: [71, 138, 207, 284, 354, 430, 500] (Hausdorff distance: 0 samples)","title":"PELT"},{"location":"examples/kernel-cpd-performance-comparison/#references","text":"[Gretton2012] Gretton, A., Borgwardt, K. M., Rasch, M. J., Sch\u00f6lkopf, B., & Smola, A. (2012). A kernel two-sample test. The Journal of Machine Learning Research, 13, 723\u2013773. [Killick2012] Killick, R., Fearnhead, P., & Eckley, I. (2012). Optimal detection of changepoints with a linear computational cost. Journal of the American Statistical Association, 107(500), 1590\u20131598. [Celisse2018] Celisse, A., Marot, G., Pierre-Jean, M., & Rigaill, G. (2018). New efficient algorithms for multiple change-point detection with reproducing kernels. Computational Statistics and Data Analysis, 128, 200\u2013220. [Arlot2019] Arlot, S., Celisse, A., & Harchaoui, Z. (2019). A kernel multiple change-point algorithm via model selection. Journal of Machine Learning Research, 20(162), 1\u201356.","title":"References"},{"location":"examples/music-segmentation/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Music segmentation # Info Try this notebook in an executable environment with Binder . Download this notebook here . Introduction # Music segmentation can be seen as a change point detection task and therefore can be carried out with ruptures . Roughly, it consists in finding the temporal boundaries of meaningful sections, e.g. the intro, verse, chorus and outro in a song. This is an important task in the field of music information retrieval. The adopted approach is summarized as follows: the original sound is transformed into an informative (multivariate) representation; mean shifts are detected in this new representation using a dynamic programming approach. In this example, we use the well-known tempogram representation, which is based on the onset strength envelope of the input signal, and captures tempo information [Grosche2010] . To load and manipulate sound data, we use the librosa package [McFee2015] . Setup # First, we make the necessary imports. import librosa import librosa.display import matplotlib.pyplot as plt import numpy as np from IPython.display import Audio , display import ruptures as rpt # our package We can also define a utility function. def fig_ax ( figsize = ( 15 , 5 ), dpi = 150 ): \"\"\"Return a (matplotlib) figure and ax objects with given size.\"\"\" return plt . subplots ( figsize = figsize , dpi = dpi ) Load the data # A number of music files are available in Librosa . See here for a complete list. In this example, we choose the Dance of the Sugar Plum Fairy from The Nutcracker by Tchaikovsky. We can listen to the music as well as display the sound envelope. duration = 30 # in seconds signal , sampling_rate = librosa . load ( librosa . ex ( \"nutcracker\" ), duration = duration ) # listen to the music display ( Audio ( data = signal , rate = sampling_rate )) # look at the envelope fig , ax = fig_ax () ax . plot ( np . arange ( signal . size ) / sampling_rate , signal ) ax . set_xlim ( 0 , signal . size / sampling_rate ) ax . set_xlabel ( \"Time (s)\" ) _ = ax . set ( title = \"Sound envelope\" ) Downloading file 'Kevin_MacLeod_-_P_I_Tchaikovsky_Dance_of_the_Sugar_Plum_Fairy.ogg' from 'https://librosa.org/data/audio/Kevin_MacLeod_-_P_I_Tchaikovsky_Dance_of_the_Sugar_Plum_Fairy.ogg' to '/home/runner/.cache/librosa'. Your browser does not support the audio element. Signal segmentation # Transform the signal into a tempogram # The tempogram measures the tempo (measured in Beats Per Minute, BPM) profile along the time axis. # Compute the onset strength hop_length_tempo = 256 oenv = librosa . onset . onset_strength ( y = signal , sr = sampling_rate , hop_length = hop_length_tempo ) # Compute the tempogram tempogram = librosa . feature . tempogram ( onset_envelope = oenv , sr = sampling_rate , hop_length = hop_length_tempo , ) # Display the tempogram fig , ax = fig_ax () _ = librosa . display . specshow ( tempogram , ax = ax , hop_length = hop_length_tempo , sr = sampling_rate , x_axis = \"s\" , y_axis = \"tempo\" , ) Detection algorithm # We choose to detect changes in the mean of the tempogram, which is a multivariate signal. This amounts to selecting the \\(L_2\\) cost function (see CostL2 ). To that end, two methods are available in ruptures : rpt.Dynp(model=\"l2\") rpt.KernelCPD(kernel=\"linear\") Both will return the same results but the latter is implemented in C and therefore significatively faster. Number of changes # In order to choose the number of change points, we use the elbow method. In the change point detection setting, this heuritic consists in: plotting the sum of costs for 1, 2,..., \\(K_{\\text{max}}\\) change points, picking the number of changes at the \"elbow\" of the curve. Intuitively, adding change points beyond the \"elbow\" only provides a marginal decrease of the sum of costs. Here, we set \\(K_{\\text{max}}\\) :=20. Note In rpt.Dynp and rpt.KernelCPD , whenever a segmentation with \\(K\\) changes is computed, all segmentations with 1,2,..., \\(K-1\\) are also computed and stored. Indeed, thanks to the dynamic programming approach, segmentations with less changes are avalaible for free as intermediate calculations. Therefore, users who need to compute segmentations with several numbers of changes should start with the one with the most changes. In addition, note that, in ruptures , the sum of costs of a segmentation defined by a set of change points bkps can easily be computed using: algo = rpt . KernelCPD ( kernel = \"linear\" ) . fit ( signal ) algo . cost . sum_of_costs ( bkps ) (Replace rpt.KernelCPD by the algorithm you are actually using, if different.) # Choose detection method algo = rpt . KernelCPD ( kernel = \"linear\" ) . fit ( tempogram . T ) # Choose the number of changes (elbow heuristic) n_bkps_max = 20 # K_max # Start by computing the segmentation with most changes. # After start, all segmentations with 1, 2,..., K_max-1 changes are also available for free. _ = algo . predict ( n_bkps_max ) array_of_n_bkps = np . arange ( 1 , n_bkps_max + 1 ) def get_sum_of_cost ( algo , n_bkps ) -> float : \"\"\"Return the sum of costs for the change points `bkps`\"\"\" bkps = algo . predict ( n_bkps = n_bkps ) return algo . cost . sum_of_costs ( bkps ) fig , ax = fig_ax (( 7 , 4 )) ax . plot ( array_of_n_bkps , [ get_sum_of_cost ( algo = algo , n_bkps = n_bkps ) for n_bkps in array_of_n_bkps ], \"-*\" , alpha = 0.5 , ) ax . set_xticks ( array_of_n_bkps ) ax . set_xlabel ( \"Number of change points\" ) ax . set_title ( \"Sum of costs\" ) ax . grid ( axis = \"x\" ) ax . set_xlim ( 0 , n_bkps_max + 1 ) # Visually we choose n_bkps=5 (highlighted in red on the elbow plot) n_bkps = 5 _ = ax . scatter ([ 5 ], [ get_sum_of_cost ( algo = algo , n_bkps = 5 )], color = \"r\" , s = 100 ) Visually, we choose 5 change points (highlighted in red on the elbow plot). Results # The tempogram can now be segmented into homogeous (from a tempo standpoint) portions. The results are show in the following figure. # Segmentation bkps = algo . predict ( n_bkps = n_bkps ) # Convert the estimated change points (frame counts) to actual timestamps bkps_times = librosa . frames_to_time ( bkps , sr = sampling_rate , hop_length = hop_length_tempo ) # Displaying results fig , ax = fig_ax () _ = librosa . display . specshow ( tempogram , ax = ax , x_axis = \"s\" , y_axis = \"tempo\" , hop_length = hop_length_tempo , sr = sampling_rate , ) for b in bkps_times [: - 1 ]: ax . axvline ( b , ls = \"--\" , color = \"white\" , lw = 4 ) Visually, the estimated change points indeed separate portions of signal with a relatively constant tempo profile. Going back to the original music signal, this intuition can be verified by listening to the individual segments defined by the changes points. # Compute change points corresponding indexes in original signal bkps_time_indexes = ( sampling_rate * bkps_times ) . astype ( int ) . tolist () for ( segment_number , ( start , end )) in enumerate ( rpt . utils . pairwise ([ 0 ] + bkps_time_indexes ), start = 1 ): segment = signal [ start : end ] print ( f \"Segment n\u00b0 { segment_number } (duration: { segment . size / sampling_rate : .2f } s)\" ) display ( Audio ( data = segment , rate = sampling_rate )) Segment n\u00b01 (duration: 1.76 s) Your browser does not support the audio element. Segment n\u00b02 (duration: 8.03 s) Your browser does not support the audio element. Segment n\u00b03 (duration: 8.46 s) Your browser does not support the audio element. Segment n\u00b04 (duration: 2.73 s) Your browser does not support the audio element. Segment n\u00b05 (duration: 5.97 s) Your browser does not support the audio element. Segment n\u00b06 (duration: 3.04 s) Your browser does not support the audio element. The first segment corresponds to the soundless part of the signal (visible on the plot of the signal enveloppe). The following segments correspond to different rythmic portions and the associated change points occur when various instruments enter or exit the play. Conclusion # This example shows how to apply ruptures on a music segmentation task. More precisely, we detected mean shifts on a well-suited representation (the tempogram) of a music signal. The number of changes was heuristically determined (with the \"elbow\" method) and the results agreed with visually and auditory intuition. Such results can then be used to characterize the structure of music and songs, for music classification, recommandation, instrument recognition, etc. This procedure could also be enriched with other musically relevant representations (e.g. the chromagram) to detect other types of changes. Authors # This example notebook has been authored by Olivier Boulant and edited by Charles Truong. References # [Grosche2010] Grosche, P., M\u00fcller, M., & Kurth, F. (2010). Cyclic tempogram - a mid-level tempo representation for music signals. Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 5522\u20135525. [McFee2015] McFee, B., Raffel, C., Liang, D., Ellis, D. P. W., McVicar, M., Battenberg, E., & Nieto, O. (2015). Librosa: audio and music signal analysis in Python. Proceedings of the Python in Science Conference, 8, 18\u201325.","title":"Music segmentation"},{"location":"examples/music-segmentation/#music-segmentation","text":"Info Try this notebook in an executable environment with Binder . Download this notebook here .","title":"Music segmentation"},{"location":"examples/music-segmentation/#introduction","text":"Music segmentation can be seen as a change point detection task and therefore can be carried out with ruptures . Roughly, it consists in finding the temporal boundaries of meaningful sections, e.g. the intro, verse, chorus and outro in a song. This is an important task in the field of music information retrieval. The adopted approach is summarized as follows: the original sound is transformed into an informative (multivariate) representation; mean shifts are detected in this new representation using a dynamic programming approach. In this example, we use the well-known tempogram representation, which is based on the onset strength envelope of the input signal, and captures tempo information [Grosche2010] . To load and manipulate sound data, we use the librosa package [McFee2015] .","title":"Introduction"},{"location":"examples/music-segmentation/#setup","text":"First, we make the necessary imports. import librosa import librosa.display import matplotlib.pyplot as plt import numpy as np from IPython.display import Audio , display import ruptures as rpt # our package We can also define a utility function. def fig_ax ( figsize = ( 15 , 5 ), dpi = 150 ): \"\"\"Return a (matplotlib) figure and ax objects with given size.\"\"\" return plt . subplots ( figsize = figsize , dpi = dpi )","title":"Setup"},{"location":"examples/music-segmentation/#load-the-data","text":"A number of music files are available in Librosa . See here for a complete list. In this example, we choose the Dance of the Sugar Plum Fairy from The Nutcracker by Tchaikovsky. We can listen to the music as well as display the sound envelope. duration = 30 # in seconds signal , sampling_rate = librosa . load ( librosa . ex ( \"nutcracker\" ), duration = duration ) # listen to the music display ( Audio ( data = signal , rate = sampling_rate )) # look at the envelope fig , ax = fig_ax () ax . plot ( np . arange ( signal . size ) / sampling_rate , signal ) ax . set_xlim ( 0 , signal . size / sampling_rate ) ax . set_xlabel ( \"Time (s)\" ) _ = ax . set ( title = \"Sound envelope\" ) Downloading file 'Kevin_MacLeod_-_P_I_Tchaikovsky_Dance_of_the_Sugar_Plum_Fairy.ogg' from 'https://librosa.org/data/audio/Kevin_MacLeod_-_P_I_Tchaikovsky_Dance_of_the_Sugar_Plum_Fairy.ogg' to '/home/runner/.cache/librosa'. Your browser does not support the audio element.","title":"Load the data"},{"location":"examples/music-segmentation/#signal-segmentation","text":"","title":"Signal segmentation"},{"location":"examples/music-segmentation/#transform-the-signal-into-a-tempogram","text":"The tempogram measures the tempo (measured in Beats Per Minute, BPM) profile along the time axis. # Compute the onset strength hop_length_tempo = 256 oenv = librosa . onset . onset_strength ( y = signal , sr = sampling_rate , hop_length = hop_length_tempo ) # Compute the tempogram tempogram = librosa . feature . tempogram ( onset_envelope = oenv , sr = sampling_rate , hop_length = hop_length_tempo , ) # Display the tempogram fig , ax = fig_ax () _ = librosa . display . specshow ( tempogram , ax = ax , hop_length = hop_length_tempo , sr = sampling_rate , x_axis = \"s\" , y_axis = \"tempo\" , )","title":"Transform the signal into a tempogram"},{"location":"examples/music-segmentation/#detection-algorithm","text":"We choose to detect changes in the mean of the tempogram, which is a multivariate signal. This amounts to selecting the \\(L_2\\) cost function (see CostL2 ). To that end, two methods are available in ruptures : rpt.Dynp(model=\"l2\") rpt.KernelCPD(kernel=\"linear\") Both will return the same results but the latter is implemented in C and therefore significatively faster.","title":"Detection algorithm"},{"location":"examples/music-segmentation/#number-of-changes","text":"In order to choose the number of change points, we use the elbow method. In the change point detection setting, this heuritic consists in: plotting the sum of costs for 1, 2,..., \\(K_{\\text{max}}\\) change points, picking the number of changes at the \"elbow\" of the curve. Intuitively, adding change points beyond the \"elbow\" only provides a marginal decrease of the sum of costs. Here, we set \\(K_{\\text{max}}\\) :=20. Note In rpt.Dynp and rpt.KernelCPD , whenever a segmentation with \\(K\\) changes is computed, all segmentations with 1,2,..., \\(K-1\\) are also computed and stored. Indeed, thanks to the dynamic programming approach, segmentations with less changes are avalaible for free as intermediate calculations. Therefore, users who need to compute segmentations with several numbers of changes should start with the one with the most changes. In addition, note that, in ruptures , the sum of costs of a segmentation defined by a set of change points bkps can easily be computed using: algo = rpt . KernelCPD ( kernel = \"linear\" ) . fit ( signal ) algo . cost . sum_of_costs ( bkps ) (Replace rpt.KernelCPD by the algorithm you are actually using, if different.) # Choose detection method algo = rpt . KernelCPD ( kernel = \"linear\" ) . fit ( tempogram . T ) # Choose the number of changes (elbow heuristic) n_bkps_max = 20 # K_max # Start by computing the segmentation with most changes. # After start, all segmentations with 1, 2,..., K_max-1 changes are also available for free. _ = algo . predict ( n_bkps_max ) array_of_n_bkps = np . arange ( 1 , n_bkps_max + 1 ) def get_sum_of_cost ( algo , n_bkps ) -> float : \"\"\"Return the sum of costs for the change points `bkps`\"\"\" bkps = algo . predict ( n_bkps = n_bkps ) return algo . cost . sum_of_costs ( bkps ) fig , ax = fig_ax (( 7 , 4 )) ax . plot ( array_of_n_bkps , [ get_sum_of_cost ( algo = algo , n_bkps = n_bkps ) for n_bkps in array_of_n_bkps ], \"-*\" , alpha = 0.5 , ) ax . set_xticks ( array_of_n_bkps ) ax . set_xlabel ( \"Number of change points\" ) ax . set_title ( \"Sum of costs\" ) ax . grid ( axis = \"x\" ) ax . set_xlim ( 0 , n_bkps_max + 1 ) # Visually we choose n_bkps=5 (highlighted in red on the elbow plot) n_bkps = 5 _ = ax . scatter ([ 5 ], [ get_sum_of_cost ( algo = algo , n_bkps = 5 )], color = \"r\" , s = 100 ) Visually, we choose 5 change points (highlighted in red on the elbow plot).","title":"Number of changes"},{"location":"examples/music-segmentation/#results","text":"The tempogram can now be segmented into homogeous (from a tempo standpoint) portions. The results are show in the following figure. # Segmentation bkps = algo . predict ( n_bkps = n_bkps ) # Convert the estimated change points (frame counts) to actual timestamps bkps_times = librosa . frames_to_time ( bkps , sr = sampling_rate , hop_length = hop_length_tempo ) # Displaying results fig , ax = fig_ax () _ = librosa . display . specshow ( tempogram , ax = ax , x_axis = \"s\" , y_axis = \"tempo\" , hop_length = hop_length_tempo , sr = sampling_rate , ) for b in bkps_times [: - 1 ]: ax . axvline ( b , ls = \"--\" , color = \"white\" , lw = 4 ) Visually, the estimated change points indeed separate portions of signal with a relatively constant tempo profile. Going back to the original music signal, this intuition can be verified by listening to the individual segments defined by the changes points. # Compute change points corresponding indexes in original signal bkps_time_indexes = ( sampling_rate * bkps_times ) . astype ( int ) . tolist () for ( segment_number , ( start , end )) in enumerate ( rpt . utils . pairwise ([ 0 ] + bkps_time_indexes ), start = 1 ): segment = signal [ start : end ] print ( f \"Segment n\u00b0 { segment_number } (duration: { segment . size / sampling_rate : .2f } s)\" ) display ( Audio ( data = segment , rate = sampling_rate )) Segment n\u00b01 (duration: 1.76 s) Your browser does not support the audio element. Segment n\u00b02 (duration: 8.03 s) Your browser does not support the audio element. Segment n\u00b03 (duration: 8.46 s) Your browser does not support the audio element. Segment n\u00b04 (duration: 2.73 s) Your browser does not support the audio element. Segment n\u00b05 (duration: 5.97 s) Your browser does not support the audio element. Segment n\u00b06 (duration: 3.04 s) Your browser does not support the audio element. The first segment corresponds to the soundless part of the signal (visible on the plot of the signal enveloppe). The following segments correspond to different rythmic portions and the associated change points occur when various instruments enter or exit the play.","title":"Results"},{"location":"examples/music-segmentation/#conclusion","text":"This example shows how to apply ruptures on a music segmentation task. More precisely, we detected mean shifts on a well-suited representation (the tempogram) of a music signal. The number of changes was heuristically determined (with the \"elbow\" method) and the results agreed with visually and auditory intuition. Such results can then be used to characterize the structure of music and songs, for music classification, recommandation, instrument recognition, etc. This procedure could also be enriched with other musically relevant representations (e.g. the chromagram) to detect other types of changes.","title":"Conclusion"},{"location":"examples/music-segmentation/#authors","text":"This example notebook has been authored by Olivier Boulant and edited by Charles Truong.","title":"Authors"},{"location":"examples/music-segmentation/#references","text":"[Grosche2010] Grosche, P., M\u00fcller, M., & Kurth, F. (2010). Cyclic tempogram - a mid-level tempo representation for music signals. Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 5522\u20135525. [McFee2015] McFee, B., Raffel, C., Liang, D., Ellis, D. P. W., McVicar, M., Battenberg, E., & Nieto, O. (2015). Librosa: audio and music signal analysis in Python. Proceedings of the Python in Science Conference, 8, 18\u201325.","title":"References"},{"location":"examples/text-segmentation/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Linear text segmentation # Info Try this notebook in an executable environment with Binder . Download this notebook here . Introduction # Linear text segmentation consists in dividing a text into several meaningful segments. Linear text segmentation can be seen as a change point detection task and therefore can be carried out with ruptures . This example performs exactly that on a well-known data set intoduced in [ Choi2000 ]. Setup # First we import packages and define a few utility functions. This section can be skipped at first reading. Library imports. from pathlib import Path import nltk import numpy as np import ruptures as rpt # our package from nltk.corpus import stopwords from nltk.stem import PorterStemmer from nltk.tokenize import regexp_tokenize from ruptures.base import BaseCost from sklearn.feature_extraction.text import CountVectorizer from sklearn.metrics.pairwise import cosine_similarity import matplotlib.pyplot as plt import matplotlib.cm as cm from matplotlib.colors import LogNorm nltk . download ( \"stopwords\" ) STOPWORD_SET = set ( stopwords . words ( \"english\" ) ) # set of stopwords of the English language PUNCTUATION_SET = set ( \"! \\\" #$%&'()*+,-./:;<=>?@[ \\\\ ]^_`{|}~\" ) [nltk_data] Downloading package stopwords to /home/runner/nltk_data... [nltk_data] Unzipping corpora/stopwords.zip. Utility functions. def preprocess ( list_of_sentences : list ) -> list : \"\"\"Preprocess each sentence (remove punctuation, stopwords, then stemming.)\"\"\" transformed = list () for sentence in list_of_sentences : ps = PorterStemmer () list_of_words = regexp_tokenize ( text = sentence . lower (), pattern = \"\\w+\" ) list_of_words = [ ps . stem ( word ) for word in list_of_words if word not in STOPWORD_SET ] transformed . append ( \" \" . join ( list_of_words )) return transformed def draw_square_on_ax ( start , end , ax , linewidth = 0.8 ): \"\"\"Draw a square on the given ax object.\"\"\" ax . vlines ( x = [ start - 0.5 , end - 0.5 ], ymin = start - 0.5 , ymax = end - 0.5 , linewidth = linewidth , ) ax . hlines ( y = [ start - 0.5 , end - 0.5 ], xmin = start - 0.5 , xmax = end - 0.5 , linewidth = linewidth , ) return ax Data # Description # The text to segment is a concatenation of excerpts from ten different documents randomly selected from the so-called Brown corpus (described here ). Each excerpt has nine to eleven sentences, amounting to 99 sentences in total. The complete text is shown in Appendix A . These data stem from a larger data set which is thoroughly described in [ Choi2000 ] and can be downloaded here . This is a common benchmark to evaluate text segmentation methods. # Loading the text filepath = Path ( \"../data/text-segmentation-data.txt\" ) original_text = filepath . read_text () . split ( \" \\n \" ) TRUE_BKPS = [ 11 , 20 , 30 , 40 , 49 , 59 , 69 , 80 , 90 , 99 ] # read from the data description print ( f \"There are { len ( original_text ) } sentences, from { len ( TRUE_BKPS ) } documents.\" ) There are 99 sentences, from 10 documents. The objective is to automatically recover the boundaries of the 10 excerpts, using the fact that they come from quite different documents and therefore have distinct topics. For instance, in the small extract of text printed in the following cell, an accurate text segmentation procedure would be able to detect that the first two sentences (10 and 11) and the last three sentences (12 to 14) belong to two different documents and have very different semantic fields. # print 5 sentences from the original text start , end = 9 , 14 for ( line_number , sentence ) in enumerate ( original_text [ start : end ], start = start + 1 ): sentence = sentence . strip ( \" \\n \" ) print ( f \" { line_number : >2 } : { sentence } \" ) 10: That could be easily done , but there is little reason in it . 11: It would come down to saying that Fromm paints with a broad brush , and that , after all , is not a conclusion one must work toward but an impression he has from the outset . 12: the effect of the digitalis glycosides is inhibited by a high concentration of potassium in the incubation medium and is enhanced by the absence of potassium ( Wolff , 1960 ) . 13: B. Organification of iodine The precise mechanism for organification of iodine in the thyroid is not as yet completely understood . 14: However , the formation of organically bound iodine , mainly mono-iodotyrosine , can be accomplished in cell-free systems . Preprocessing # Before performing text segmentation, the original text is preprocessed. In a nutshell (see [ Choi2000 ] for more details), the punctuation and stopwords are removed; words are reduced to their stems (e.g., \"waited\" and \"waiting\" become \"wait\"); a vector of word counts is computed. # transform text transformed_text = preprocess ( original_text ) # print original and transformed ind = 97 print ( \"Original sentence:\" ) print ( f \" \\t { original_text [ ind ] } \" ) print () print ( \"Transformed:\" ) print ( f \" \\t { transformed_text [ ind ] } \" ) Original sentence: Then Heywood Sullivan , Kansas City catcher , singled up the middle and Throneberry was across with what proved to be the winning run . Transformed: heywood sullivan kansa citi catcher singl middl throneberri across prove win run # Once the text is preprocessed, each sentence is transformed into a vector of word counts. vectorizer = CountVectorizer ( analyzer = \"word\" ) vectorized_text = vectorizer . fit_transform ( transformed_text ) msg = f \"There are { len ( vectorizer . get_feature_names ()) } different words in the corpus, e.g. { vectorizer . get_feature_names ()[ 20 : 30 ] } .\" print ( msg ) There are 842 different words in the corpus, e.g. ['acid', 'across', 'act', 'activ', 'actual', 'ad', 'adair', 'addit', 'administ', 'administr']. /opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) Note that the vectorized text representation is a (very) sparse matrix. Text segmentation # Cost function # To compare (the vectorized representation of) two sentences, [Choi2000] uses the cosine similarity \\(k_{\\text{cosine}}: \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}\\) : \\[ k_{\\text{cosine}}(x, y) := \\frac{\\langle x \\mid y \\rangle}{\\|x\\|\\|y\\|} \\] where \\(x\\) and \\(y\\) are two \\(d\\) -dimensionnal vectors of word counts. Text segmentation now amounts to a kernel change point detection (see [Truong2020] for more details). However, this particular kernel is not implemented in ruptures therefore we need to create a custom cost function . (Actually, it is implemented in ruptures but the current implementation does not exploit the sparse structure of the vectorized text representation and can therefore be slow.) Let \\(y=\\{y_0, y_1,\\dots,y_{T-1}\\}\\) be a \\(d\\) -dimensionnal signal with \\(T\\) samples. Recall that a cost function \\(c(\\cdot)\\) that derives from a kernel \\(k(\\cdot, \\cdot)\\) is such that \\[ c(y_{a..b}) = \\sum_{t=a}^{b-1} G_{t, t} - \\frac{1}{b-a} \\sum_{a \\leq s < b } \\sum_{a \\leq t < b} G_{s,t} \\] where \\(y_{a..b}\\) is the subsignal \\(\\{y_a, y_{a+1},\\dots,y_{b-1}\\}\\) and \\(G_{st}:=k(y_s, y_t)\\) (see [Truong2020] for more details). In other words, \\((G_{st})_{st}\\) is the \\(T\\times T\\) Gram matrix of \\(y\\) . Thanks to this formula, we can now implement our custom cost function (named CosineCost in the following cell). class CosineCost ( BaseCost ): \"\"\"Cost derived from the cosine similarity.\"\"\" # The 2 following attributes must be specified for compatibility. model = \"custom_cosine\" min_size = 2 def fit ( self , signal ): \"\"\"Set the internal parameter.\"\"\" self . signal = signal self . gram = cosine_similarity ( signal , dense_output = False ) return self def error ( self , start , end ) -> float : \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: segment cost Raises: NotEnoughPoints: when the segment is too short (less than `min_size` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints sub_gram = self . gram [ start : end , start : end ] val = sub_gram . diagonal () . sum () val -= sub_gram . sum () / ( end - start ) return val Compute change points # If the number \\(K\\) of change points is assumed to be known, we can use dynamic programming to search for the exact segmentation \\(\\hat{t}_1,\\dots,\\hat{t}_K\\) that minimizes the sum of segment costs: \\[ \\hat{t}_1,\\dots,\\hat{t}_K := \\text{arg}\\min_{t_1,\\dots,t_K} \\left[ c(y_{0..t_1}) + c(y_{t_1..t_2}) + \\dots + c(y_{t_K..T}) \\right]. \\] n_bkps = 9 # there are 9 change points (10 text segments) algo = rpt . Dynp ( custom_cost = CosineCost (), min_size = 2 , jump = 1 ) . fit ( vectorized_text ) predicted_bkps = algo . predict ( n_bkps = n_bkps ) print ( f \"True change points are \\t\\t { TRUE_BKPS } .\" ) print ( f \"Detected change points are \\t { predicted_bkps } .\" ) True change points are [11, 20, 30, 40, 49, 59, 69, 80, 90, 99]. Detected change points are [12, 19, 30, 40, 49, 59, 70, 78, 94, 99]. (Note that the last change point index is simply the length of the signal. This is by design.) Predicted breakpoints are quite close to the true change points. Indeed, most estimated changes are less than one sentence away from a true change. The last change is less accurately predicted with an error of 4 sentences. To overcome this issue, one solution would be to consider a richer representation (compared to the sparse word frequency vectors). Visualize segmentations # Show sentence numbers. In the following cell, the two segmentations (true and predicted) can be visually compared. For each paragraph, the sentence numbers are shown. true_segment_list = rpt . utils . pairwise ([ 0 ] + TRUE_BKPS ) predicted_segment_list = rpt . utils . pairwise ([ 0 ] + predicted_bkps ) for ( n_paragraph , ( true_segment , predicted_segment )) in enumerate ( zip ( true_segment_list , predicted_segment_list ), start = 1 ): print ( f \"Paragraph n\u00b0 { n_paragraph : 02d } \" ) start_true , end_true = true_segment start_pred , end_pred = predicted_segment start = min ( start_true , start_pred ) end = max ( end_true , end_pred ) msg = \" \" . join ( f \" { ind + 1 : 02d } \" if ( start_true <= ind < end_true ) else \" \" for ind in range ( start , end ) ) print ( f \"(true) \\t { msg } \" ) msg = \" \" . join ( f \" { ind + 1 : 02d } \" if ( start_pred <= ind < end_pred ) else \" \" for ind in range ( start , end ) ) print ( f \"(pred) \\t { msg } \" ) print () Paragraph n\u00b001 (true) 01 02 03 04 05 06 07 08 09 10 11 (pred) 01 02 03 04 05 06 07 08 09 10 11 12 Paragraph n\u00b002 (true) 12 13 14 15 16 17 18 19 20 (pred) 13 14 15 16 17 18 19 Paragraph n\u00b003 (true) 21 22 23 24 25 26 27 28 29 30 (pred) 20 21 22 23 24 25 26 27 28 29 30 Paragraph n\u00b004 (true) 31 32 33 34 35 36 37 38 39 40 (pred) 31 32 33 34 35 36 37 38 39 40 Paragraph n\u00b005 (true) 41 42 43 44 45 46 47 48 49 (pred) 41 42 43 44 45 46 47 48 49 Paragraph n\u00b006 (true) 50 51 52 53 54 55 56 57 58 59 (pred) 50 51 52 53 54 55 56 57 58 59 Paragraph n\u00b007 (true) 60 61 62 63 64 65 66 67 68 69 (pred) 60 61 62 63 64 65 66 67 68 69 70 Paragraph n\u00b008 (true) 70 71 72 73 74 75 76 77 78 79 80 (pred) 71 72 73 74 75 76 77 78 Paragraph n\u00b009 (true) 81 82 83 84 85 86 87 88 89 90 (pred) 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 Paragraph n\u00b010 (true) 91 92 93 94 95 96 97 98 99 (pred) 95 96 97 98 99 Show the Gram matrix. In addition, the text segmentation can be shown on the Gram matrix that was used to detect changes. This is done in the following cell. Most segments (represented by the blue squares) are similar between the true segmentation and the predicted segmentation, except for last two. This is mainly due to the fact that, in the penultimate excerpt, all sentences are dissimilar (with respect to the cosine measure). fig , ax_arr = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 7 , 5 ), dpi = 200 ) # plot config title_fontsize = 10 label_fontsize = 7 title_list = [ \"True text segmentation\" , \"Predicted text segmentation\" ] for ( ax , title , bkps ) in zip ( ax_arr , title_list , [ TRUE_BKPS , predicted_bkps ]): # plot gram matrix ax . imshow ( algo . cost . gram . toarray (), cmap = cm . plasma , norm = LogNorm ()) # add text segmentation for ( start , end ) in rpt . utils . pairwise ([ 0 ] + bkps ): draw_square_on_ax ( start = start , end = end , ax = ax ) # add labels and title ax . set_title ( title , fontsize = title_fontsize ) ax . set_xlabel ( \"Sentence index\" , fontsize = label_fontsize ) ax . set_ylabel ( \"Sentence index\" , fontsize = label_fontsize ) ax . tick_params ( axis = \"both\" , which = \"major\" , labelsize = label_fontsize ) Conclusion # This example shows how to apply ruptures on a text segmentation task. In detail, we detected shifts in the vocabulary of a collection of sentences using common NLP preprocessing and transformation. This task amounts to a kernel change point detection procedure where the kernel is the cosine kernel. Such results can then be used to characterize the structure of the text for subsequent NLP tasks. This procedure should certainly be enriched with more relevant and compact representations to better detect changes. Appendix A # The complete text used in this notebook is as follows. Note that the line numbers and the blank lines (added to visually mark the boundaries between excerpts) are not part of the text fed to the segmentation method. for ( start , end ) in rpt . utils . pairwise ([ 0 ] + TRUE_BKPS ): excerpt = original_text [ start : end ] for ( n_line , sentence ) in enumerate ( excerpt , start = start + 1 ): sentence = sentence . strip ( \" \\n \" ) print ( f \" { n_line : >2 } : { sentence } \" ) print () 1: The Sane Society is an ambitious work . 2: Its scope is as broad as the question : What does it mean to live in modern society ? ? 3: A work so broad , even when it is directed by a leading idea and informed by a moral vision , must necessarily `` fail '' . 4: Even a hasty reader will easily find in it numerous blind spots , errors of fact and argument , important exclusions , areas of ignorance and prejudice , undue emphases on trivia , examples of broad positions supported by flimsy evidence , and the like . 5: Such books are easy prey for critics . 6: Nor need the critic be captious . 7: A careful and orderly man , who values precision and a kind of tough intellectual responsibility , might easily be put off by such a book . 8: It is a simple matter , for one so disposed , to take a work like The Sane Society and shred it into odds and ends . 9: The thing can be made to look like the cluttered attic of a large and vigorous family -- a motley jumble of discarded objects , some outworn and some that were never useful , some once whole and bright but now chipped and tarnished , some odd pieces whose history no one remembers , here and there a gem , everything fascinating because it suggests some part of the human condition -- the whole adding up to nothing more than a glimpse into the disorderly history of the makers and users . 10: That could be easily done , but there is little reason in it . 11: It would come down to saying that Fromm paints with a broad brush , and that , after all , is not a conclusion one must work toward but an impression he has from the outset . 12: the effect of the digitalis glycosides is inhibited by a high concentration of potassium in the incubation medium and is enhanced by the absence of potassium ( Wolff , 1960 ) . 13: B. Organification of iodine The precise mechanism for organification of iodine in the thyroid is not as yet completely understood . 14: However , the formation of organically bound iodine , mainly mono-iodotyrosine , can be accomplished in cell-free systems . 15: In the absence of additions to the homogenate , the product formed is an iodinated particulate protein ( Fawcett and Kirkwood , 1953 ; ; Taurog , Potter and Chaikoff , 1955 ; ; Taurog , Potter , Tong , and Chaikoff , 1956 ; ; Serif and Kirkwood , 1958 ; ; De Groot and Carvalho , 1960 ) . 16: This iodoprotein does not appear to be the same as what is normally present in the thyroid , and there is no evidence so far that thyroglobulin can be iodinated in vitro by cell-free systems . 17: In addition , the iodoamino acid formed in largest quantity in the intact thyroid is di-iodotyrosine . 18: If tyrosine and a system generating hydrogen peroxide are added to a cell-free homogenate of the thyroid , large quantities of free mono-iodotyrosine can be formed ( Alexander , 1959 ) . 19: It is not clear whether this system bears any resemblance to the in vivo iodinating mechanism , and a system generating peroxide has not been identified in thyroid tissue . 20: On chemical grounds it seems most likely that iodide is first converted to Afj and then to Afj as the active iodinating species . 21: the statement empirical , for goodness was not a quality like red or squeaky that could be seen or heard . 22: What were they to do , then , with these awkward judgments of value ? ? 23: To find a place for them in their theory of knowledge would require them to revise the theory radically , and yet that theory was what they regarded as their most important discovery . 24: It appeared that the theory could be saved in one way only . 25: If it could be shown that judgments of good and bad were not judgments at all , that they asserted nothing true or false , but merely expressed emotions like `` Hurrah '' or `` Fiddlesticks '' , then these wayward judgments would cease from troubling and weary heads could be at rest . 26: This is the course the positivists took . 27: They explained value judgments by explaining them away . 28: Now I do not think their view will do . 29: But before discussing it , I should like to record one vote of thanks to them for the clarity with which they have stated their case . 30: It has been said of John Stuart Mill that he wrote so clearly that he could be found out . 31: Greer Garson , world-famous star of stage , screen and television , will be honored for the high standard in tasteful sophisticated fashion with which she has created a high standard in her profession . 32: As a Neiman-Marcus award winner the titian-haired Miss Garson is a personification of the individual look so important to fashion this season . 33: She will receive the 1961 `` Oscar '' at the 24th annual Neiman-Marcus Exposition , Tuesday and Wednesday in the Grand Ballroom of the Sheraton-Dallas Hotel . 34: The only woman recipient , Miss Garson will receive the award with Ferdinando Sarmi , creator of chic , beautiful women 's fashions ; ; Harry Rolnick , president of the Byer-Rolnick Hat Corporation and designer of men 's hats ; ; Sydney Wragge , creator of sophisticated casuals for women and Roger Vivier , designer of Christian Dior shoes Paris , France , whose squared toes and lowered heels have revolutionized the shoe industry . 35: The silver and ebony plaques will be presented at noon luncheons by Stanley Marcus , president of Neiman-Marcus , Beneficiary of the proceeds from the two showings will be the Dallas Society for Crippled Children Cerebral Palsy Treatment Center . 36: The attractive Greer Garson , who loves beautiful clothes and selects them as carefully as she does her professional roles , prefers timeless classical designs . 37: Occasionally she deserts the simple and elegant for a fun piece simply because `` It 's unlike me '' . 38: In private life , Miss Garson is Mrs. E.E. Fogelson and on the go most of the time commuting from Dallas , where they maintain an apartment , to their California home in Los Angeles ' suburban Bel-Air to their ranch in Pecos , New Mexico . 39: Therefore , her wardrobe is largely mobile , to be packed at a moment 's notice and to shake out without a wrinkle . 40: Her creations in fashion are from many designers because she does n't want a complete wardrobe from any one designer any more than she wants `` all of her pictures by one painter '' . 41: Wage-price policies of industry are the result of a complex of forces -- no single explanation has been found which applies to all cases . 42: The purpose of this paper is to analyze one possible force which has not been treated in the literature , but which we believe makes a significant contribution to explaining the wage-price behavior of a few very important industries . 43: While there may be several such industries to which the model of this paper is applicable , the authors make particular claim of relevance to the explanation of the course of wages and prices in the steel industry of the United States since World War 2 . 44: Indeed , the apparent stiffening of the industry 's attitude in the recent steel strike has a direct explanation in terms of the model here presented . 45: The model of this paper considers an industry which is not characterized by vigorous price competition , but which is so basic that its wage-price policies are held in check by continuous critical public scrutiny . 46: Where the industry 's product price has been kept below the `` profit-maximizing '' and `` entry-limiting '' prices due to fears of public reaction , the profit seeking producers have an interest in offering little real resistance to wage demands . 47: The contribution of this paper is a demonstration of this proposition , and an exploration of some of its implications . 48: In order to focus clearly upon the operation of this one force , which we may call the effect of `` public-limit pricing '' on `` key '' wage bargains , we deliberately simplify the model by abstracting from other forces , such as union power , which may be relevant in an actual situation . 49: For expository purposes , this is best treated as a model which spells out the conditions under which an important industry affected with the public interest would find it profitable to raise wages even in the absence of union pressures for higher wages . 50: The vast Central Valley of California is one of the most productive agricultural areas in the world . 51: During the summer of 1960 , it became the setting for a bitter and basic labor-management struggle . 52: The contestants in this economic struggle are the Agricultural Workers Organizing Committee ( AWOC ) of the AFL-CIO and the agricultural employers of the State . 53: By virtue of the legal responsibilities of the Department of Employment in the farm placement program , we necessarily found ourselves in the middle between these two forces . 54: It is not a pleasant or easy position , but one we have endeavored to maintain . 55: We have sought to be strictly neutral as between the parties , but at the same time we have been required frequently to rule on specific issues or situations as they arose . 56: Inevitably , one side was pleased and the other displeased , regardless of how we ruled . 57: Often the displeased parties interpreted our decision as implying favoritism toward the other . 58: We have consoled ourselves with the thought that this is a normal human reaction and is one of the consequences of any decision in an adversary proceeding . 59: It is disconcerting , nevertheless , to read in a labor weekly , `` Perluss knuckles down to growers '' , and then to be confronted with a growers ' publication which states , `` Perluss recognizes obviously phony and trumped-up strikes as bona fide '' . 60: Rookie Ron Nischwitz continued his pinpoint pitching Monday night as the Bears made it two straight over Indianapolis , 5-3 . 61: The husky 6-3 , 205-pound lefthander , was in command all the way before an on-the-scene audience of only 949 and countless of television viewers in the Denver area . 62: It was Nischwitz ' third straight victory of the new season and ran the Grizzlies ' winning streak to four straight . 63: They now lead Louisville by a full game on top of the American Association pack . 64: Nischwitz fanned six and walked only Charley Hinton in the third inning . 65: He has given only the one pass in his 27 innings , an unusual characteristic for a southpaw . 66: The Bears took the lead in the first inning , as they did in Sunday 's opener , and never lagged . 67: Dick McAuliffe cracked the first of his two doubles against Lefty Don Rudolph to open the Bear 's attack . 68: After Al Paschal gruonded out , Jay Cooke walked and Jim McDaniel singled home McAuliffe . 69: Alusik then moved Cooke across with a line drive to left . 70: Unemployed older workers who have no expectation of securing employment in the occupation in which they are skilled should be able to secure counseling and retraining in an occupation with a future . 71: Some vocational training schools provide such training , but the current need exceeds the facilities . 72: Current programs The present Federal program of vocational education began in 1917 with the passage of the Smith-Hughes Act , which provided a continuing annual appropriation of $ 7 million to support , on a matching basis , state-administered programs of vocational education in agriculture , trades , industrial skills and home economics . 73: Since 1917 some thirteen supplementary and related acts have extended this Federal program . 74: The George-Barden Act of 1946 raised the previous increases in annual authorizations to $ 29 million in addition to the $ 7 million under the Smith Act . 75: The Health Amendment Act of 1956 added $ 5 million for practical nurse training . 76: The latest major change in this program was introduced by the National Defense Education Act of 1958 , Title 8 , of which amended the George-Barden Act . 77: Annual authorizations of $ 15 million were added for area vocational education programs that meet national defense needs for highly skilled technicians . 78: The Federal program of vocational education merely provides financial aid to encourage the establishment of vocational education programs in public schools . 79: The initiative , administration and control remain primarily with the local school districts . 80: Even the states remain primarily in an assisting role , providing leadership and teacher training . 81: briefly , the topping configuration must be examined for its inferences . 82: Then the fact that the lower channel line was pierced had further forecasting significance . 83: And then the application of the count rules to the width ( horizontally ) of the configuration gives us an intial estimate of the probable depth of the decline . 84: The very idea of there being `` count rules '' implies that there is some sort of proportion to be expected between the amount of congestive activity and the extent of the breakaway ( run up or run down ) movement . 85: This expectation is what really `` sold '' point and figure . 86: But there is no positive and consistently demonstrable relationship in the strictest sense . 87: Experience will show that only the vaguest generalities apply , and in fine , these merely dwell upon a relationship between the durations and intensities of events . 88: After all , too much does not happen too suddenly , nor does very little take long . 89: The advantages and disadvantages of these two types of charting , bar charting and point and figure charting , remain the subject of fairly good-natured litigation among their respective professional advocates , with both methods enjoying in common , one irrevocable merit . 90: They are both trend-following methods . 91: Miami , Fla. , March 17 -- The Orioles tonight retained the distinction of being the only winless team among the eighteen Major-League clubs as they dropped their sixth straight spring exhibition decision , this one to the Kansas City Athletics by a score of 5 to 3 . 92: Indications as late as the top of the sixth were that the Birds were to end their victory draught as they coasted along with a 3-to-o advantage . 93: Siebern hits homer Over the first five frames , Jack Fisher , the big righthander who figures to be in the middle of Oriole plans for a drive on the 1961 American League pennant , held the A 's scoreless while yielding three scattered hits . 94: Then Dick Hyde , submarine-ball hurler , entered the contest and only five batters needed to face him before there existed a 3-to-3 deadlock . 95: A two-run homer by Norm Siebern and a solo blast by Bill Tuttle tied the game , and single runs in the eighth and ninth gave the Athletics their fifth victory in eight starts . 96: House throws wild With one down in the eighth , Marv Throneberry drew a walk and stole second as Hyde fanned Tuttle . 97: Catcher Frank House 's throw in an effort to nab Throneberry was wide and in the dirt . 98: Then Heywood Sullivan , Kansas City catcher , singled up the middle and Throneberry was across with what proved to be the winning run . 99: Rookie southpaw George Stepanovich relieved Hyde at the start of the ninth and gave up the A 's fifth tally on a walk to second baseman Dick Howser , a wild pitch , and Frank Cipriani 's single under Shortstop Jerry Adair 's glove into center . Authors # This example notebook has been authored by Olivier Boulant and edited by Charles Truong. References # [Choi2000] Choi, F. Y. Y. (2000). Advances in domain independent linear text segmentation. Proceedings of the North American Chapter of the Association for Computational Linguistics Conference (NAACL), 26\u201333. [Truong2020] Truong, C., Oudre, L., & Vayatis, N. (2020). Selective review of offline change point detection methods. Signal Processing, 167.","title":"Text segmentation"},{"location":"examples/text-segmentation/#linear-text-segmentation","text":"Info Try this notebook in an executable environment with Binder . Download this notebook here .","title":"Linear text segmentation"},{"location":"examples/text-segmentation/#introduction","text":"Linear text segmentation consists in dividing a text into several meaningful segments. Linear text segmentation can be seen as a change point detection task and therefore can be carried out with ruptures . This example performs exactly that on a well-known data set intoduced in [ Choi2000 ].","title":"Introduction"},{"location":"examples/text-segmentation/#setup","text":"First we import packages and define a few utility functions. This section can be skipped at first reading. Library imports. from pathlib import Path import nltk import numpy as np import ruptures as rpt # our package from nltk.corpus import stopwords from nltk.stem import PorterStemmer from nltk.tokenize import regexp_tokenize from ruptures.base import BaseCost from sklearn.feature_extraction.text import CountVectorizer from sklearn.metrics.pairwise import cosine_similarity import matplotlib.pyplot as plt import matplotlib.cm as cm from matplotlib.colors import LogNorm nltk . download ( \"stopwords\" ) STOPWORD_SET = set ( stopwords . words ( \"english\" ) ) # set of stopwords of the English language PUNCTUATION_SET = set ( \"! \\\" #$%&'()*+,-./:;<=>?@[ \\\\ ]^_`{|}~\" ) [nltk_data] Downloading package stopwords to /home/runner/nltk_data... [nltk_data] Unzipping corpora/stopwords.zip. Utility functions. def preprocess ( list_of_sentences : list ) -> list : \"\"\"Preprocess each sentence (remove punctuation, stopwords, then stemming.)\"\"\" transformed = list () for sentence in list_of_sentences : ps = PorterStemmer () list_of_words = regexp_tokenize ( text = sentence . lower (), pattern = \"\\w+\" ) list_of_words = [ ps . stem ( word ) for word in list_of_words if word not in STOPWORD_SET ] transformed . append ( \" \" . join ( list_of_words )) return transformed def draw_square_on_ax ( start , end , ax , linewidth = 0.8 ): \"\"\"Draw a square on the given ax object.\"\"\" ax . vlines ( x = [ start - 0.5 , end - 0.5 ], ymin = start - 0.5 , ymax = end - 0.5 , linewidth = linewidth , ) ax . hlines ( y = [ start - 0.5 , end - 0.5 ], xmin = start - 0.5 , xmax = end - 0.5 , linewidth = linewidth , ) return ax","title":"Setup"},{"location":"examples/text-segmentation/#data","text":"","title":"Data"},{"location":"examples/text-segmentation/#description","text":"The text to segment is a concatenation of excerpts from ten different documents randomly selected from the so-called Brown corpus (described here ). Each excerpt has nine to eleven sentences, amounting to 99 sentences in total. The complete text is shown in Appendix A . These data stem from a larger data set which is thoroughly described in [ Choi2000 ] and can be downloaded here . This is a common benchmark to evaluate text segmentation methods. # Loading the text filepath = Path ( \"../data/text-segmentation-data.txt\" ) original_text = filepath . read_text () . split ( \" \\n \" ) TRUE_BKPS = [ 11 , 20 , 30 , 40 , 49 , 59 , 69 , 80 , 90 , 99 ] # read from the data description print ( f \"There are { len ( original_text ) } sentences, from { len ( TRUE_BKPS ) } documents.\" ) There are 99 sentences, from 10 documents. The objective is to automatically recover the boundaries of the 10 excerpts, using the fact that they come from quite different documents and therefore have distinct topics. For instance, in the small extract of text printed in the following cell, an accurate text segmentation procedure would be able to detect that the first two sentences (10 and 11) and the last three sentences (12 to 14) belong to two different documents and have very different semantic fields. # print 5 sentences from the original text start , end = 9 , 14 for ( line_number , sentence ) in enumerate ( original_text [ start : end ], start = start + 1 ): sentence = sentence . strip ( \" \\n \" ) print ( f \" { line_number : >2 } : { sentence } \" ) 10: That could be easily done , but there is little reason in it . 11: It would come down to saying that Fromm paints with a broad brush , and that , after all , is not a conclusion one must work toward but an impression he has from the outset . 12: the effect of the digitalis glycosides is inhibited by a high concentration of potassium in the incubation medium and is enhanced by the absence of potassium ( Wolff , 1960 ) . 13: B. Organification of iodine The precise mechanism for organification of iodine in the thyroid is not as yet completely understood . 14: However , the formation of organically bound iodine , mainly mono-iodotyrosine , can be accomplished in cell-free systems .","title":"Description"},{"location":"examples/text-segmentation/#preprocessing","text":"Before performing text segmentation, the original text is preprocessed. In a nutshell (see [ Choi2000 ] for more details), the punctuation and stopwords are removed; words are reduced to their stems (e.g., \"waited\" and \"waiting\" become \"wait\"); a vector of word counts is computed. # transform text transformed_text = preprocess ( original_text ) # print original and transformed ind = 97 print ( \"Original sentence:\" ) print ( f \" \\t { original_text [ ind ] } \" ) print () print ( \"Transformed:\" ) print ( f \" \\t { transformed_text [ ind ] } \" ) Original sentence: Then Heywood Sullivan , Kansas City catcher , singled up the middle and Throneberry was across with what proved to be the winning run . Transformed: heywood sullivan kansa citi catcher singl middl throneberri across prove win run # Once the text is preprocessed, each sentence is transformed into a vector of word counts. vectorizer = CountVectorizer ( analyzer = \"word\" ) vectorized_text = vectorizer . fit_transform ( transformed_text ) msg = f \"There are { len ( vectorizer . get_feature_names ()) } different words in the corpus, e.g. { vectorizer . get_feature_names ()[ 20 : 30 ] } .\" print ( msg ) There are 842 different words in the corpus, e.g. ['acid', 'across', 'act', 'activ', 'actual', 'ad', 'adair', 'addit', 'administ', 'administr']. /opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) Note that the vectorized text representation is a (very) sparse matrix.","title":"Preprocessing"},{"location":"examples/text-segmentation/#text-segmentation","text":"","title":"Text segmentation"},{"location":"examples/text-segmentation/#cost-function","text":"To compare (the vectorized representation of) two sentences, [Choi2000] uses the cosine similarity \\(k_{\\text{cosine}}: \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}\\) : \\[ k_{\\text{cosine}}(x, y) := \\frac{\\langle x \\mid y \\rangle}{\\|x\\|\\|y\\|} \\] where \\(x\\) and \\(y\\) are two \\(d\\) -dimensionnal vectors of word counts. Text segmentation now amounts to a kernel change point detection (see [Truong2020] for more details). However, this particular kernel is not implemented in ruptures therefore we need to create a custom cost function . (Actually, it is implemented in ruptures but the current implementation does not exploit the sparse structure of the vectorized text representation and can therefore be slow.) Let \\(y=\\{y_0, y_1,\\dots,y_{T-1}\\}\\) be a \\(d\\) -dimensionnal signal with \\(T\\) samples. Recall that a cost function \\(c(\\cdot)\\) that derives from a kernel \\(k(\\cdot, \\cdot)\\) is such that \\[ c(y_{a..b}) = \\sum_{t=a}^{b-1} G_{t, t} - \\frac{1}{b-a} \\sum_{a \\leq s < b } \\sum_{a \\leq t < b} G_{s,t} \\] where \\(y_{a..b}\\) is the subsignal \\(\\{y_a, y_{a+1},\\dots,y_{b-1}\\}\\) and \\(G_{st}:=k(y_s, y_t)\\) (see [Truong2020] for more details). In other words, \\((G_{st})_{st}\\) is the \\(T\\times T\\) Gram matrix of \\(y\\) . Thanks to this formula, we can now implement our custom cost function (named CosineCost in the following cell). class CosineCost ( BaseCost ): \"\"\"Cost derived from the cosine similarity.\"\"\" # The 2 following attributes must be specified for compatibility. model = \"custom_cosine\" min_size = 2 def fit ( self , signal ): \"\"\"Set the internal parameter.\"\"\" self . signal = signal self . gram = cosine_similarity ( signal , dense_output = False ) return self def error ( self , start , end ) -> float : \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: segment cost Raises: NotEnoughPoints: when the segment is too short (less than `min_size` samples). \"\"\" if end - start < self . min_size : raise NotEnoughPoints sub_gram = self . gram [ start : end , start : end ] val = sub_gram . diagonal () . sum () val -= sub_gram . sum () / ( end - start ) return val","title":"Cost function"},{"location":"examples/text-segmentation/#compute-change-points","text":"If the number \\(K\\) of change points is assumed to be known, we can use dynamic programming to search for the exact segmentation \\(\\hat{t}_1,\\dots,\\hat{t}_K\\) that minimizes the sum of segment costs: \\[ \\hat{t}_1,\\dots,\\hat{t}_K := \\text{arg}\\min_{t_1,\\dots,t_K} \\left[ c(y_{0..t_1}) + c(y_{t_1..t_2}) + \\dots + c(y_{t_K..T}) \\right]. \\] n_bkps = 9 # there are 9 change points (10 text segments) algo = rpt . Dynp ( custom_cost = CosineCost (), min_size = 2 , jump = 1 ) . fit ( vectorized_text ) predicted_bkps = algo . predict ( n_bkps = n_bkps ) print ( f \"True change points are \\t\\t { TRUE_BKPS } .\" ) print ( f \"Detected change points are \\t { predicted_bkps } .\" ) True change points are [11, 20, 30, 40, 49, 59, 69, 80, 90, 99]. Detected change points are [12, 19, 30, 40, 49, 59, 70, 78, 94, 99]. (Note that the last change point index is simply the length of the signal. This is by design.) Predicted breakpoints are quite close to the true change points. Indeed, most estimated changes are less than one sentence away from a true change. The last change is less accurately predicted with an error of 4 sentences. To overcome this issue, one solution would be to consider a richer representation (compared to the sparse word frequency vectors).","title":"Compute change points"},{"location":"examples/text-segmentation/#visualize-segmentations","text":"Show sentence numbers. In the following cell, the two segmentations (true and predicted) can be visually compared. For each paragraph, the sentence numbers are shown. true_segment_list = rpt . utils . pairwise ([ 0 ] + TRUE_BKPS ) predicted_segment_list = rpt . utils . pairwise ([ 0 ] + predicted_bkps ) for ( n_paragraph , ( true_segment , predicted_segment )) in enumerate ( zip ( true_segment_list , predicted_segment_list ), start = 1 ): print ( f \"Paragraph n\u00b0 { n_paragraph : 02d } \" ) start_true , end_true = true_segment start_pred , end_pred = predicted_segment start = min ( start_true , start_pred ) end = max ( end_true , end_pred ) msg = \" \" . join ( f \" { ind + 1 : 02d } \" if ( start_true <= ind < end_true ) else \" \" for ind in range ( start , end ) ) print ( f \"(true) \\t { msg } \" ) msg = \" \" . join ( f \" { ind + 1 : 02d } \" if ( start_pred <= ind < end_pred ) else \" \" for ind in range ( start , end ) ) print ( f \"(pred) \\t { msg } \" ) print () Paragraph n\u00b001 (true) 01 02 03 04 05 06 07 08 09 10 11 (pred) 01 02 03 04 05 06 07 08 09 10 11 12 Paragraph n\u00b002 (true) 12 13 14 15 16 17 18 19 20 (pred) 13 14 15 16 17 18 19 Paragraph n\u00b003 (true) 21 22 23 24 25 26 27 28 29 30 (pred) 20 21 22 23 24 25 26 27 28 29 30 Paragraph n\u00b004 (true) 31 32 33 34 35 36 37 38 39 40 (pred) 31 32 33 34 35 36 37 38 39 40 Paragraph n\u00b005 (true) 41 42 43 44 45 46 47 48 49 (pred) 41 42 43 44 45 46 47 48 49 Paragraph n\u00b006 (true) 50 51 52 53 54 55 56 57 58 59 (pred) 50 51 52 53 54 55 56 57 58 59 Paragraph n\u00b007 (true) 60 61 62 63 64 65 66 67 68 69 (pred) 60 61 62 63 64 65 66 67 68 69 70 Paragraph n\u00b008 (true) 70 71 72 73 74 75 76 77 78 79 80 (pred) 71 72 73 74 75 76 77 78 Paragraph n\u00b009 (true) 81 82 83 84 85 86 87 88 89 90 (pred) 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 Paragraph n\u00b010 (true) 91 92 93 94 95 96 97 98 99 (pred) 95 96 97 98 99 Show the Gram matrix. In addition, the text segmentation can be shown on the Gram matrix that was used to detect changes. This is done in the following cell. Most segments (represented by the blue squares) are similar between the true segmentation and the predicted segmentation, except for last two. This is mainly due to the fact that, in the penultimate excerpt, all sentences are dissimilar (with respect to the cosine measure). fig , ax_arr = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 7 , 5 ), dpi = 200 ) # plot config title_fontsize = 10 label_fontsize = 7 title_list = [ \"True text segmentation\" , \"Predicted text segmentation\" ] for ( ax , title , bkps ) in zip ( ax_arr , title_list , [ TRUE_BKPS , predicted_bkps ]): # plot gram matrix ax . imshow ( algo . cost . gram . toarray (), cmap = cm . plasma , norm = LogNorm ()) # add text segmentation for ( start , end ) in rpt . utils . pairwise ([ 0 ] + bkps ): draw_square_on_ax ( start = start , end = end , ax = ax ) # add labels and title ax . set_title ( title , fontsize = title_fontsize ) ax . set_xlabel ( \"Sentence index\" , fontsize = label_fontsize ) ax . set_ylabel ( \"Sentence index\" , fontsize = label_fontsize ) ax . tick_params ( axis = \"both\" , which = \"major\" , labelsize = label_fontsize )","title":"Visualize segmentations"},{"location":"examples/text-segmentation/#conclusion","text":"This example shows how to apply ruptures on a text segmentation task. In detail, we detected shifts in the vocabulary of a collection of sentences using common NLP preprocessing and transformation. This task amounts to a kernel change point detection procedure where the kernel is the cosine kernel. Such results can then be used to characterize the structure of the text for subsequent NLP tasks. This procedure should certainly be enriched with more relevant and compact representations to better detect changes.","title":"Conclusion"},{"location":"examples/text-segmentation/#appendix-a","text":"The complete text used in this notebook is as follows. Note that the line numbers and the blank lines (added to visually mark the boundaries between excerpts) are not part of the text fed to the segmentation method. for ( start , end ) in rpt . utils . pairwise ([ 0 ] + TRUE_BKPS ): excerpt = original_text [ start : end ] for ( n_line , sentence ) in enumerate ( excerpt , start = start + 1 ): sentence = sentence . strip ( \" \\n \" ) print ( f \" { n_line : >2 } : { sentence } \" ) print () 1: The Sane Society is an ambitious work . 2: Its scope is as broad as the question : What does it mean to live in modern society ? ? 3: A work so broad , even when it is directed by a leading idea and informed by a moral vision , must necessarily `` fail '' . 4: Even a hasty reader will easily find in it numerous blind spots , errors of fact and argument , important exclusions , areas of ignorance and prejudice , undue emphases on trivia , examples of broad positions supported by flimsy evidence , and the like . 5: Such books are easy prey for critics . 6: Nor need the critic be captious . 7: A careful and orderly man , who values precision and a kind of tough intellectual responsibility , might easily be put off by such a book . 8: It is a simple matter , for one so disposed , to take a work like The Sane Society and shred it into odds and ends . 9: The thing can be made to look like the cluttered attic of a large and vigorous family -- a motley jumble of discarded objects , some outworn and some that were never useful , some once whole and bright but now chipped and tarnished , some odd pieces whose history no one remembers , here and there a gem , everything fascinating because it suggests some part of the human condition -- the whole adding up to nothing more than a glimpse into the disorderly history of the makers and users . 10: That could be easily done , but there is little reason in it . 11: It would come down to saying that Fromm paints with a broad brush , and that , after all , is not a conclusion one must work toward but an impression he has from the outset . 12: the effect of the digitalis glycosides is inhibited by a high concentration of potassium in the incubation medium and is enhanced by the absence of potassium ( Wolff , 1960 ) . 13: B. Organification of iodine The precise mechanism for organification of iodine in the thyroid is not as yet completely understood . 14: However , the formation of organically bound iodine , mainly mono-iodotyrosine , can be accomplished in cell-free systems . 15: In the absence of additions to the homogenate , the product formed is an iodinated particulate protein ( Fawcett and Kirkwood , 1953 ; ; Taurog , Potter and Chaikoff , 1955 ; ; Taurog , Potter , Tong , and Chaikoff , 1956 ; ; Serif and Kirkwood , 1958 ; ; De Groot and Carvalho , 1960 ) . 16: This iodoprotein does not appear to be the same as what is normally present in the thyroid , and there is no evidence so far that thyroglobulin can be iodinated in vitro by cell-free systems . 17: In addition , the iodoamino acid formed in largest quantity in the intact thyroid is di-iodotyrosine . 18: If tyrosine and a system generating hydrogen peroxide are added to a cell-free homogenate of the thyroid , large quantities of free mono-iodotyrosine can be formed ( Alexander , 1959 ) . 19: It is not clear whether this system bears any resemblance to the in vivo iodinating mechanism , and a system generating peroxide has not been identified in thyroid tissue . 20: On chemical grounds it seems most likely that iodide is first converted to Afj and then to Afj as the active iodinating species . 21: the statement empirical , for goodness was not a quality like red or squeaky that could be seen or heard . 22: What were they to do , then , with these awkward judgments of value ? ? 23: To find a place for them in their theory of knowledge would require them to revise the theory radically , and yet that theory was what they regarded as their most important discovery . 24: It appeared that the theory could be saved in one way only . 25: If it could be shown that judgments of good and bad were not judgments at all , that they asserted nothing true or false , but merely expressed emotions like `` Hurrah '' or `` Fiddlesticks '' , then these wayward judgments would cease from troubling and weary heads could be at rest . 26: This is the course the positivists took . 27: They explained value judgments by explaining them away . 28: Now I do not think their view will do . 29: But before discussing it , I should like to record one vote of thanks to them for the clarity with which they have stated their case . 30: It has been said of John Stuart Mill that he wrote so clearly that he could be found out . 31: Greer Garson , world-famous star of stage , screen and television , will be honored for the high standard in tasteful sophisticated fashion with which she has created a high standard in her profession . 32: As a Neiman-Marcus award winner the titian-haired Miss Garson is a personification of the individual look so important to fashion this season . 33: She will receive the 1961 `` Oscar '' at the 24th annual Neiman-Marcus Exposition , Tuesday and Wednesday in the Grand Ballroom of the Sheraton-Dallas Hotel . 34: The only woman recipient , Miss Garson will receive the award with Ferdinando Sarmi , creator of chic , beautiful women 's fashions ; ; Harry Rolnick , president of the Byer-Rolnick Hat Corporation and designer of men 's hats ; ; Sydney Wragge , creator of sophisticated casuals for women and Roger Vivier , designer of Christian Dior shoes Paris , France , whose squared toes and lowered heels have revolutionized the shoe industry . 35: The silver and ebony plaques will be presented at noon luncheons by Stanley Marcus , president of Neiman-Marcus , Beneficiary of the proceeds from the two showings will be the Dallas Society for Crippled Children Cerebral Palsy Treatment Center . 36: The attractive Greer Garson , who loves beautiful clothes and selects them as carefully as she does her professional roles , prefers timeless classical designs . 37: Occasionally she deserts the simple and elegant for a fun piece simply because `` It 's unlike me '' . 38: In private life , Miss Garson is Mrs. E.E. Fogelson and on the go most of the time commuting from Dallas , where they maintain an apartment , to their California home in Los Angeles ' suburban Bel-Air to their ranch in Pecos , New Mexico . 39: Therefore , her wardrobe is largely mobile , to be packed at a moment 's notice and to shake out without a wrinkle . 40: Her creations in fashion are from many designers because she does n't want a complete wardrobe from any one designer any more than she wants `` all of her pictures by one painter '' . 41: Wage-price policies of industry are the result of a complex of forces -- no single explanation has been found which applies to all cases . 42: The purpose of this paper is to analyze one possible force which has not been treated in the literature , but which we believe makes a significant contribution to explaining the wage-price behavior of a few very important industries . 43: While there may be several such industries to which the model of this paper is applicable , the authors make particular claim of relevance to the explanation of the course of wages and prices in the steel industry of the United States since World War 2 . 44: Indeed , the apparent stiffening of the industry 's attitude in the recent steel strike has a direct explanation in terms of the model here presented . 45: The model of this paper considers an industry which is not characterized by vigorous price competition , but which is so basic that its wage-price policies are held in check by continuous critical public scrutiny . 46: Where the industry 's product price has been kept below the `` profit-maximizing '' and `` entry-limiting '' prices due to fears of public reaction , the profit seeking producers have an interest in offering little real resistance to wage demands . 47: The contribution of this paper is a demonstration of this proposition , and an exploration of some of its implications . 48: In order to focus clearly upon the operation of this one force , which we may call the effect of `` public-limit pricing '' on `` key '' wage bargains , we deliberately simplify the model by abstracting from other forces , such as union power , which may be relevant in an actual situation . 49: For expository purposes , this is best treated as a model which spells out the conditions under which an important industry affected with the public interest would find it profitable to raise wages even in the absence of union pressures for higher wages . 50: The vast Central Valley of California is one of the most productive agricultural areas in the world . 51: During the summer of 1960 , it became the setting for a bitter and basic labor-management struggle . 52: The contestants in this economic struggle are the Agricultural Workers Organizing Committee ( AWOC ) of the AFL-CIO and the agricultural employers of the State . 53: By virtue of the legal responsibilities of the Department of Employment in the farm placement program , we necessarily found ourselves in the middle between these two forces . 54: It is not a pleasant or easy position , but one we have endeavored to maintain . 55: We have sought to be strictly neutral as between the parties , but at the same time we have been required frequently to rule on specific issues or situations as they arose . 56: Inevitably , one side was pleased and the other displeased , regardless of how we ruled . 57: Often the displeased parties interpreted our decision as implying favoritism toward the other . 58: We have consoled ourselves with the thought that this is a normal human reaction and is one of the consequences of any decision in an adversary proceeding . 59: It is disconcerting , nevertheless , to read in a labor weekly , `` Perluss knuckles down to growers '' , and then to be confronted with a growers ' publication which states , `` Perluss recognizes obviously phony and trumped-up strikes as bona fide '' . 60: Rookie Ron Nischwitz continued his pinpoint pitching Monday night as the Bears made it two straight over Indianapolis , 5-3 . 61: The husky 6-3 , 205-pound lefthander , was in command all the way before an on-the-scene audience of only 949 and countless of television viewers in the Denver area . 62: It was Nischwitz ' third straight victory of the new season and ran the Grizzlies ' winning streak to four straight . 63: They now lead Louisville by a full game on top of the American Association pack . 64: Nischwitz fanned six and walked only Charley Hinton in the third inning . 65: He has given only the one pass in his 27 innings , an unusual characteristic for a southpaw . 66: The Bears took the lead in the first inning , as they did in Sunday 's opener , and never lagged . 67: Dick McAuliffe cracked the first of his two doubles against Lefty Don Rudolph to open the Bear 's attack . 68: After Al Paschal gruonded out , Jay Cooke walked and Jim McDaniel singled home McAuliffe . 69: Alusik then moved Cooke across with a line drive to left . 70: Unemployed older workers who have no expectation of securing employment in the occupation in which they are skilled should be able to secure counseling and retraining in an occupation with a future . 71: Some vocational training schools provide such training , but the current need exceeds the facilities . 72: Current programs The present Federal program of vocational education began in 1917 with the passage of the Smith-Hughes Act , which provided a continuing annual appropriation of $ 7 million to support , on a matching basis , state-administered programs of vocational education in agriculture , trades , industrial skills and home economics . 73: Since 1917 some thirteen supplementary and related acts have extended this Federal program . 74: The George-Barden Act of 1946 raised the previous increases in annual authorizations to $ 29 million in addition to the $ 7 million under the Smith Act . 75: The Health Amendment Act of 1956 added $ 5 million for practical nurse training . 76: The latest major change in this program was introduced by the National Defense Education Act of 1958 , Title 8 , of which amended the George-Barden Act . 77: Annual authorizations of $ 15 million were added for area vocational education programs that meet national defense needs for highly skilled technicians . 78: The Federal program of vocational education merely provides financial aid to encourage the establishment of vocational education programs in public schools . 79: The initiative , administration and control remain primarily with the local school districts . 80: Even the states remain primarily in an assisting role , providing leadership and teacher training . 81: briefly , the topping configuration must be examined for its inferences . 82: Then the fact that the lower channel line was pierced had further forecasting significance . 83: And then the application of the count rules to the width ( horizontally ) of the configuration gives us an intial estimate of the probable depth of the decline . 84: The very idea of there being `` count rules '' implies that there is some sort of proportion to be expected between the amount of congestive activity and the extent of the breakaway ( run up or run down ) movement . 85: This expectation is what really `` sold '' point and figure . 86: But there is no positive and consistently demonstrable relationship in the strictest sense . 87: Experience will show that only the vaguest generalities apply , and in fine , these merely dwell upon a relationship between the durations and intensities of events . 88: After all , too much does not happen too suddenly , nor does very little take long . 89: The advantages and disadvantages of these two types of charting , bar charting and point and figure charting , remain the subject of fairly good-natured litigation among their respective professional advocates , with both methods enjoying in common , one irrevocable merit . 90: They are both trend-following methods . 91: Miami , Fla. , March 17 -- The Orioles tonight retained the distinction of being the only winless team among the eighteen Major-League clubs as they dropped their sixth straight spring exhibition decision , this one to the Kansas City Athletics by a score of 5 to 3 . 92: Indications as late as the top of the sixth were that the Birds were to end their victory draught as they coasted along with a 3-to-o advantage . 93: Siebern hits homer Over the first five frames , Jack Fisher , the big righthander who figures to be in the middle of Oriole plans for a drive on the 1961 American League pennant , held the A 's scoreless while yielding three scattered hits . 94: Then Dick Hyde , submarine-ball hurler , entered the contest and only five batters needed to face him before there existed a 3-to-3 deadlock . 95: A two-run homer by Norm Siebern and a solo blast by Bill Tuttle tied the game , and single runs in the eighth and ninth gave the Athletics their fifth victory in eight starts . 96: House throws wild With one down in the eighth , Marv Throneberry drew a walk and stole second as Hyde fanned Tuttle . 97: Catcher Frank House 's throw in an effort to nab Throneberry was wide and in the dirt . 98: Then Heywood Sullivan , Kansas City catcher , singled up the middle and Throneberry was across with what proved to be the winning run . 99: Rookie southpaw George Stepanovich relieved Hyde at the start of the ninth and gave up the A 's fifth tally on a walk to second baseman Dick Howser , a wild pitch , and Frank Cipriani 's single under Shortstop Jerry Adair 's glove into center .","title":"Appendix A"},{"location":"examples/text-segmentation/#authors","text":"This example notebook has been authored by Olivier Boulant and edited by Charles Truong.","title":"Authors"},{"location":"examples/text-segmentation/#references","text":"[Choi2000] Choi, F. Y. Y. (2000). Advances in domain independent linear text segmentation. Proceedings of the North American Chapter of the Association for Computational Linguistics Conference (NAACL), 26\u201333. [Truong2020] Truong, C., Oudre, L., & Vayatis, N. (2020). Selective review of offline change point detection methods. Signal Processing, 167.","title":"References"},{"location":"getting-started/basic-usage/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Basic usage # Info Try this notebook in an executable environment with Binder . Download this notebook here . Let us start with a simple example to illustrate the use of ruptures : generate a 3-dimensional piecewise constant signal with noise and estimate the change points. Setup # First, we make the necessary imports. import matplotlib.pyplot as plt # for display purposes import ruptures as rpt # our package Generate and display the signal # Let us generate a 3-dimensional piecewise constant signal with Gaussian noise. n_samples , n_dims , sigma = 1000 , 3 , 2 n_bkps = 4 # number of breakpoints signal , bkps = rpt . pw_constant ( n_samples , n_dims , n_bkps , noise_std = sigma ) The true change points of this synthetic signal are available in the bkps variable. print ( bkps ) [182, 367, 582, 804, 1000] Note that the first four element are change point indexes while the last is simply the number of samples. (This is a technical convention so that functions in ruptures always know the length of the signal at hand.) It is also possible to plot our \\(\\mathbb{R}^3\\) -valued signal along with the true change points with the rpt.display function. In the following image, the color changes whenever the mean of the signal shifts. fig , ax_array = rpt . display ( signal , bkps ) Change point detection # We can now perform change point detection, meaning that we find the indexes where the signal mean changes. To that end, we minimize the sum of squared errors when approximating the signal by a piecewise constant signal. Formally, for a signal \\(y_0,y_1,\\dots,y_{T-1}\\) ( \\(T\\) samples), we solve the following optimization problem, over all possible change positions \\(t_1<t_2<\\dots<t_K\\) (where the number \\(K\\) of changes is defined by the user): \\[ \\hat{t}_1, \\hat{t}_2,\\dots,\\hat{t}_K = \\arg\\min_{t_1,\\dots,t_K} V(t_1,t_2,\\dots,t_K) \\] with \\[ V(t_1,t_2,\\dots,t_K) := \\sum_{k=0}^K\\sum_{t=t_k}^{t_{k+1}-1} \\|y_t-\\bar{y}_{t_k..t_{k+1}}\\|^2 \\] where \\(\\bar{y}_{t_k..t_{k+1}}\\) is the empirical mean of the sub-signal \\(y_{t_k}, y_{t_k+1},\\dots,y_{t_{k+1}-1}\\) . (By convention \\(t_0=0\\) and \\(t_{K+1}=T\\) .) This optimization is solved with dynamic programming, using the Dynp class. (More information in the section What is change point detection? and the User guide .) # detection algo = rpt . Dynp ( model = \"l2\" ) . fit ( signal ) result = algo . predict ( n_bkps = 4 ) print ( result ) [185, 365, 580, 805, 1000] Again the first elements are change point indexes and the last is the number of samples. Display the results # To visualy compare the true segmentation ( bkps ) and the estimated one ( result ), we can resort to rpt.display a second time. In the following image, the alternating colors indicate the true breakpoints and the dashed vertical lines, the estimated breakpoints. # display rpt . display ( signal , bkps , result ) plt . show () In this simple example, both are quite similar and almost undistinguishable.","title":"Basic usage"},{"location":"getting-started/basic-usage/#basic-usage","text":"Info Try this notebook in an executable environment with Binder . Download this notebook here . Let us start with a simple example to illustrate the use of ruptures : generate a 3-dimensional piecewise constant signal with noise and estimate the change points.","title":"Basic usage"},{"location":"getting-started/basic-usage/#setup","text":"First, we make the necessary imports. import matplotlib.pyplot as plt # for display purposes import ruptures as rpt # our package","title":"Setup"},{"location":"getting-started/basic-usage/#generate-and-display-the-signal","text":"Let us generate a 3-dimensional piecewise constant signal with Gaussian noise. n_samples , n_dims , sigma = 1000 , 3 , 2 n_bkps = 4 # number of breakpoints signal , bkps = rpt . pw_constant ( n_samples , n_dims , n_bkps , noise_std = sigma ) The true change points of this synthetic signal are available in the bkps variable. print ( bkps ) [182, 367, 582, 804, 1000] Note that the first four element are change point indexes while the last is simply the number of samples. (This is a technical convention so that functions in ruptures always know the length of the signal at hand.) It is also possible to plot our \\(\\mathbb{R}^3\\) -valued signal along with the true change points with the rpt.display function. In the following image, the color changes whenever the mean of the signal shifts. fig , ax_array = rpt . display ( signal , bkps )","title":"Generate and display the signal"},{"location":"getting-started/basic-usage/#change-point-detection","text":"We can now perform change point detection, meaning that we find the indexes where the signal mean changes. To that end, we minimize the sum of squared errors when approximating the signal by a piecewise constant signal. Formally, for a signal \\(y_0,y_1,\\dots,y_{T-1}\\) ( \\(T\\) samples), we solve the following optimization problem, over all possible change positions \\(t_1<t_2<\\dots<t_K\\) (where the number \\(K\\) of changes is defined by the user): \\[ \\hat{t}_1, \\hat{t}_2,\\dots,\\hat{t}_K = \\arg\\min_{t_1,\\dots,t_K} V(t_1,t_2,\\dots,t_K) \\] with \\[ V(t_1,t_2,\\dots,t_K) := \\sum_{k=0}^K\\sum_{t=t_k}^{t_{k+1}-1} \\|y_t-\\bar{y}_{t_k..t_{k+1}}\\|^2 \\] where \\(\\bar{y}_{t_k..t_{k+1}}\\) is the empirical mean of the sub-signal \\(y_{t_k}, y_{t_k+1},\\dots,y_{t_{k+1}-1}\\) . (By convention \\(t_0=0\\) and \\(t_{K+1}=T\\) .) This optimization is solved with dynamic programming, using the Dynp class. (More information in the section What is change point detection? and the User guide .) # detection algo = rpt . Dynp ( model = \"l2\" ) . fit ( signal ) result = algo . predict ( n_bkps = 4 ) print ( result ) [185, 365, 580, 805, 1000] Again the first elements are change point indexes and the last is the number of samples.","title":"Change point detection"},{"location":"getting-started/basic-usage/#display-the-results","text":"To visualy compare the true segmentation ( bkps ) and the estimated one ( result ), we can resort to rpt.display a second time. In the following image, the alternating colors indicate the true breakpoints and the dashed vertical lines, the estimated breakpoints. # display rpt . display ( signal , bkps , result ) plt . show () In this simple example, both are quite similar and almost undistinguishable.","title":"Display the results"},{"location":"user-guide/","text":"User guide # This section describes the algorithms and utility functions of ruptures . Each entry of the user guide is linked to a companion entry in the Code reference section, where the API is detailed.","title":"User guide"},{"location":"user-guide/#user-guide","text":"This section describes the algorithms and utility functions of ruptures . Each entry of the user guide is linked to a companion entry in the Code reference section, where the API is detailed.","title":"User guide"},{"location":"user-guide/evaluation/","text":"Evaluation and visualization #","title":"Evaluation and visualization"},{"location":"user-guide/evaluation/#evaluation-and-visualization","text":"","title":"Evaluation and visualization"},{"location":"user-guide/costs/costautoregressive/","text":"Autoregressive model change ( CostAR ) # Description # Let \\(0<t_1<t_2<\\dots<n\\) be unknown change points indexes. Consider the following piecewise autoregressive model \\[ y_t = z_t' \\delta_j + \\varepsilon_t, \\quad \\forall t=t_j,\\dots,t_{j+1}-1 \\] where \\(j>1\\) is the segment number, \\(z_t=[y_{t-1}, y_{t-2},\\dots,y_{t-p}]\\) is the lag vector,and \\(p>0\\) is the order of the process. The least-squares estimates of the break dates is obtained by minimizing the sum of squared residuals [Bai2000] . Formally, the associated cost function on an interval \\(I\\) is \\[ c(y_{I}) = \\min_{\\delta\\in\\mathbb{R}^p} \\sum_{t\\in I} \\|y_t - \\delta' z_t \\|_2^2. \\] Currently, this function is limited to 1D signals. Usage # Start with the usual imports and create a signal with piecewise linear trends. from itertools import cycle import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n = 2000 n_bkps , sigma = 4 , 0.5 # number of change points, noise standart deviation bkps = [ 400 , 1000 , 1300 , 1800 , n ] f1 = np . array ([ 0.075 , 0.1 ]) f2 = np . array ([ 0.1 , 0.125 ]) freqs = np . zeros (( n , 2 )) for sub , val in zip ( np . split ( freqs , bkps [: - 1 ]), cycle ([ f1 , f2 ])): sub += val tt = np . arange ( n ) signal = np . sum (( np . sin ( 2 * np . pi * tt * f ) for f in freqs . T )) signal += np . random . normal ( scale = sigma , size = signal . shape ) # display signal rpt . show . display ( signal , bkps , figsize = ( 10 , 6 )) plt . show () Then create a CostAR instance and print the cost of the sub-signal signal[50:150] . The autoregressive order can be specified through the keyword 'order' . c = rpt . costs . CostAR ( order = 10 ) . fit ( signal ) print ( c . error ( 50 , 150 )) You can also compute the sum of costs for a given list of change points. print ( c . sum_of_costs ( bkps )) print ( c . sum_of_costs ([ 10 , 100 , 200 , 250 , n ])) In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator ), either pass a CostAR instance (through the argument 'custom_cost' ) or set model=\"ar\" . Additional parameters can be passed to the cost instance through the keyword 'params' . c = rpt . costs . CostAR ( order = 10 ) algo = rpt . Dynp ( custom_cost = c ) # is equivalent to algo = rpt . Dynp ( model = \"ar\" , params = { \"order\" : 10 }) Reference # [Bai2000] Bai, J. (2000). Vector autoregressive models with structural changes in regression coefficients and in variance\u2013covariance matrices. Annals of Economics and Finance, 1(2), 301\u2013336.","title":"CostAR"},{"location":"user-guide/costs/costautoregressive/#autoregressive-model-change-costar","text":"","title":"Autoregressive model change (CostAR)"},{"location":"user-guide/costs/costautoregressive/#description","text":"Let \\(0<t_1<t_2<\\dots<n\\) be unknown change points indexes. Consider the following piecewise autoregressive model \\[ y_t = z_t' \\delta_j + \\varepsilon_t, \\quad \\forall t=t_j,\\dots,t_{j+1}-1 \\] where \\(j>1\\) is the segment number, \\(z_t=[y_{t-1}, y_{t-2},\\dots,y_{t-p}]\\) is the lag vector,and \\(p>0\\) is the order of the process. The least-squares estimates of the break dates is obtained by minimizing the sum of squared residuals [Bai2000] . Formally, the associated cost function on an interval \\(I\\) is \\[ c(y_{I}) = \\min_{\\delta\\in\\mathbb{R}^p} \\sum_{t\\in I} \\|y_t - \\delta' z_t \\|_2^2. \\] Currently, this function is limited to 1D signals.","title":"Description"},{"location":"user-guide/costs/costautoregressive/#usage","text":"Start with the usual imports and create a signal with piecewise linear trends. from itertools import cycle import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n = 2000 n_bkps , sigma = 4 , 0.5 # number of change points, noise standart deviation bkps = [ 400 , 1000 , 1300 , 1800 , n ] f1 = np . array ([ 0.075 , 0.1 ]) f2 = np . array ([ 0.1 , 0.125 ]) freqs = np . zeros (( n , 2 )) for sub , val in zip ( np . split ( freqs , bkps [: - 1 ]), cycle ([ f1 , f2 ])): sub += val tt = np . arange ( n ) signal = np . sum (( np . sin ( 2 * np . pi * tt * f ) for f in freqs . T )) signal += np . random . normal ( scale = sigma , size = signal . shape ) # display signal rpt . show . display ( signal , bkps , figsize = ( 10 , 6 )) plt . show () Then create a CostAR instance and print the cost of the sub-signal signal[50:150] . The autoregressive order can be specified through the keyword 'order' . c = rpt . costs . CostAR ( order = 10 ) . fit ( signal ) print ( c . error ( 50 , 150 )) You can also compute the sum of costs for a given list of change points. print ( c . sum_of_costs ( bkps )) print ( c . sum_of_costs ([ 10 , 100 , 200 , 250 , n ])) In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator ), either pass a CostAR instance (through the argument 'custom_cost' ) or set model=\"ar\" . Additional parameters can be passed to the cost instance through the keyword 'params' . c = rpt . costs . CostAR ( order = 10 ) algo = rpt . Dynp ( custom_cost = c ) # is equivalent to algo = rpt . Dynp ( model = \"ar\" , params = { \"order\" : 10 })","title":"Usage"},{"location":"user-guide/costs/costautoregressive/#reference","text":"[Bai2000] Bai, J. (2000). Vector autoregressive models with structural changes in regression coefficients and in variance\u2013covariance matrices. Annals of Economics and Finance, 1(2), 301\u2013336.","title":"Reference"},{"location":"user-guide/costs/costclinear/","text":"Continuous linear change ( CostCLinear ) # Description # For a given set of indexes (also called knots) \\(t_k\\) ( \\(k=1,\\dots,K\\) ), a linear spline \\(f\\) is such that: \\(f\\) is affine on each interval \\(t_k..t_{k+1}\\) , i.e. \\(f(t)=\\alpha_k (t-t_k) + \\beta_k\\) ( \\(\\alpha_k, \\beta_k \\in \\mathbb{R}^d\\) ) for all \\(t=t_k,t_k+1,\\dots,t_{k+1}-1\\) ; \\(f\\) is continuous. The cost function CostCLinear measures the error when approximating the signal with a linear spline. Formally, it is defined for \\(0<a<b\\leq T\\) by \\[ c(y_{a..b}) := \\sum_{t=a}^{b-1} \\left\\lVert y_t - y_{a-1} - \\frac{t-a+1}{b-a}(y_{b-1}-y_{a-1}) \\right\\rVert^2 \\] and \\(c(y_{0..b}):=c(y_{1..b})\\) (by convention). Usage # Start with the usual imports and create a signal with piecewise linear trends. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n_samples , n_dims = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standard deviation signal , bkps = rpt . pw_constant ( n_samples , n_dims , n_bkps , noise_std = sigma ) signal = np . cumsum ( signal , axis = 1 ) Then create a CostCLinear instance and print the cost of the sub-signal signal[50:150] . c = rpt . costs . CostCLinear () . fit ( signal ) print ( c . error ( 50 , 150 )) You can also compute the sum of costs for a given list of change points. print ( c . sum_of_costs ( bkps )) print ( c . sum_of_costs ([ 10 , 100 , 200 , 250 , n ])) In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator ), either pass a CostCLinear instance (through the argument custom_cost ) or set model=\"clinear\" . c = rpt . costs . CostCLinear () algo = rpt . Dynp ( custom_cost = c ) # is equivalent to algo = rpt . Dynp ( model = \"clinear\" )","title":"CostCLinear"},{"location":"user-guide/costs/costclinear/#continuous-linear-change-costclinear","text":"","title":"Continuous linear change (CostCLinear)"},{"location":"user-guide/costs/costclinear/#description","text":"For a given set of indexes (also called knots) \\(t_k\\) ( \\(k=1,\\dots,K\\) ), a linear spline \\(f\\) is such that: \\(f\\) is affine on each interval \\(t_k..t_{k+1}\\) , i.e. \\(f(t)=\\alpha_k (t-t_k) + \\beta_k\\) ( \\(\\alpha_k, \\beta_k \\in \\mathbb{R}^d\\) ) for all \\(t=t_k,t_k+1,\\dots,t_{k+1}-1\\) ; \\(f\\) is continuous. The cost function CostCLinear measures the error when approximating the signal with a linear spline. Formally, it is defined for \\(0<a<b\\leq T\\) by \\[ c(y_{a..b}) := \\sum_{t=a}^{b-1} \\left\\lVert y_t - y_{a-1} - \\frac{t-a+1}{b-a}(y_{b-1}-y_{a-1}) \\right\\rVert^2 \\] and \\(c(y_{0..b}):=c(y_{1..b})\\) (by convention).","title":"Description"},{"location":"user-guide/costs/costclinear/#usage","text":"Start with the usual imports and create a signal with piecewise linear trends. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n_samples , n_dims = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standard deviation signal , bkps = rpt . pw_constant ( n_samples , n_dims , n_bkps , noise_std = sigma ) signal = np . cumsum ( signal , axis = 1 ) Then create a CostCLinear instance and print the cost of the sub-signal signal[50:150] . c = rpt . costs . CostCLinear () . fit ( signal ) print ( c . error ( 50 , 150 )) You can also compute the sum of costs for a given list of change points. print ( c . sum_of_costs ( bkps )) print ( c . sum_of_costs ([ 10 , 100 , 200 , 250 , n ])) In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator ), either pass a CostCLinear instance (through the argument custom_cost ) or set model=\"clinear\" . c = rpt . costs . CostCLinear () algo = rpt . Dynp ( custom_cost = c ) # is equivalent to algo = rpt . Dynp ( model = \"clinear\" )","title":"Usage"},{"location":"user-guide/costs/costcosine/","text":"Kernelized mean change ( CostCosine ) # Description # Given a positive semi-definite kernel \\(k(\\cdot, \\cdot) : \\mathbb{R}^d\\times \\mathbb{R}^d \\mapsto \\mathbb{R}\\) and its associated feature map \\(\\Phi:\\mathbb{R}^d \\mapsto \\mathcal{H}\\) (where \\(\\mathcal{H}\\) is an appropriate Hilbert space), this cost function detects changes in the mean of the embedded signal \\(\\{\\Phi(y_t)\\}_t\\) [ Arlot2019 ]. Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\(I\\) , \\[ c(y_{a..b}) = \\sum_{t=a}^{b-1} \\| \\Phi(y_t) - \\bar{\\mu} \\|_{\\mathcal{H}}^2 \\] where \\(\\bar{\\mu}_{a..b}\\) is the empirical mean of the embedded sub-signal \\(\\{\\Phi(y_t)\\}_{a\\leq t < b-1}\\) . Here the kernel is the cosine similarity: \\[ k(x, y) = \\frac{\\langle x\\mid y\\rangle}{\\|x\\|\\|y\\|} \\] where \\(\\langle \\cdot\\mid\\cdot \\rangle\\) and \\(\\| \\cdot \\|\\) are the Euclidean scalar product and norm respectively. In other words, it is equal to the L2-normalized dot product of vectors. This cost function has been used for music segmentation tasks [ Cooper2002 ] and topic segmentation of text [ Hearst1994 ]. Usage # Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) Then create a CostCosine instance and print the cost of the sub-signal signal[50:150] . c = rpt . costs . CostCosine () . fit ( signal ) print ( c . error ( 50 , 150 )) You can also compute the sum of costs for a given list of change points. print ( c . sum_of_costs ( bkps )) print ( c . sum_of_costs ([ 10 , 100 , 200 , 250 , n ])) In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator ), either pass a CostCosine instance (through the argument custom_cost ) or set model=\"cosine\" . c = rpt . costs . CostCosine () algo = rpt . Dynp ( custom_cost = c ) # is equivalent to algo = rpt . Dynp ( model = \"cosine\" ) References # [Hearst1994] Hearst, M. A. (1994). Multi-paragraph segmentation of expository text. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (pp. 9\u201316). Las Cruces, New Mexico, USA. [Cooper2002] Cooper, M., & Foote, J. (2002). Automatic music summarization via similarity analysis. In Proceedings of the International Conference on Music Information Retrieval (ISMIR) (pp. 81\u201385). Paris, France. [Arlot2019] Arlot, S., Celisse, A., & Harchaoui, Z. (2019). A kernel multiple change-point algorithm via model selection. Journal of Machine Learning Research, 20(162), 1\u201356.","title":"CostCosine"},{"location":"user-guide/costs/costcosine/#kernelized-mean-change-costcosine","text":"","title":"Kernelized mean change (CostCosine)"},{"location":"user-guide/costs/costcosine/#description","text":"Given a positive semi-definite kernel \\(k(\\cdot, \\cdot) : \\mathbb{R}^d\\times \\mathbb{R}^d \\mapsto \\mathbb{R}\\) and its associated feature map \\(\\Phi:\\mathbb{R}^d \\mapsto \\mathcal{H}\\) (where \\(\\mathcal{H}\\) is an appropriate Hilbert space), this cost function detects changes in the mean of the embedded signal \\(\\{\\Phi(y_t)\\}_t\\) [ Arlot2019 ]. Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\(I\\) , \\[ c(y_{a..b}) = \\sum_{t=a}^{b-1} \\| \\Phi(y_t) - \\bar{\\mu} \\|_{\\mathcal{H}}^2 \\] where \\(\\bar{\\mu}_{a..b}\\) is the empirical mean of the embedded sub-signal \\(\\{\\Phi(y_t)\\}_{a\\leq t < b-1}\\) . Here the kernel is the cosine similarity: \\[ k(x, y) = \\frac{\\langle x\\mid y\\rangle}{\\|x\\|\\|y\\|} \\] where \\(\\langle \\cdot\\mid\\cdot \\rangle\\) and \\(\\| \\cdot \\|\\) are the Euclidean scalar product and norm respectively. In other words, it is equal to the L2-normalized dot product of vectors. This cost function has been used for music segmentation tasks [ Cooper2002 ] and topic segmentation of text [ Hearst1994 ].","title":"Description"},{"location":"user-guide/costs/costcosine/#usage","text":"Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) Then create a CostCosine instance and print the cost of the sub-signal signal[50:150] . c = rpt . costs . CostCosine () . fit ( signal ) print ( c . error ( 50 , 150 )) You can also compute the sum of costs for a given list of change points. print ( c . sum_of_costs ( bkps )) print ( c . sum_of_costs ([ 10 , 100 , 200 , 250 , n ])) In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator ), either pass a CostCosine instance (through the argument custom_cost ) or set model=\"cosine\" . c = rpt . costs . CostCosine () algo = rpt . Dynp ( custom_cost = c ) # is equivalent to algo = rpt . Dynp ( model = \"cosine\" )","title":"Usage"},{"location":"user-guide/costs/costcosine/#references","text":"[Hearst1994] Hearst, M. A. (1994). Multi-paragraph segmentation of expository text. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (pp. 9\u201316). Las Cruces, New Mexico, USA. [Cooper2002] Cooper, M., & Foote, J. (2002). Automatic music summarization via similarity analysis. In Proceedings of the International Conference on Music Information Retrieval (ISMIR) (pp. 81\u201385). Paris, France. [Arlot2019] Arlot, S., Celisse, A., & Harchaoui, Z. (2019). A kernel multiple change-point algorithm via model selection. Journal of Machine Learning Research, 20(162), 1\u201356.","title":"References"},{"location":"user-guide/costs/costcustom/","text":"Custom cost class # Users who are interested in detecting a specific type of change can easily do so by creating a custom cost function. Provided, they subclass the base cost function BaseCost , they will be able to seamlessly run the algorithms implemented in ruptures . Important The custom cost class must at least implement the two following methods: .fit(signal) and .error(start, end) (see user guide ). Example # Let \\(\\{y_t\\}_t\\) denote a 1D piecewise stationary random process. Assume that the \\(y_t\\) are independent and exponentially distributed with a scale parameter that shifts at some unknown instants \\(t_1,t_2,\\dots\\) The change points estimates are the minimizers of the negative log-likelihood, and the associated cost function is given by \\[ c(y_I) = |I| \\log \\bar{\\mu}_I \\] where \\(I,\\, y_I\\) and \\(\\bar{\\mu}_I\\) are respectively an interval, the sub-signal on this interval and the empirical mean of this sub-signal. The following code implements this cost function: from math import log from ruptures.base import BaseCost class MyCost ( BaseCost ): \"\"\"Custom cost for exponential signals.\"\"\" # The 2 following attributes must be specified for compatibility. model = \"\" min_size = 2 def fit ( self , signal ): \"\"\"Set the internal parameter.\"\"\" self . signal = signal return self def error ( self , start , end ): \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: float: segment cost \"\"\" sub = self . signal [ start : end ] return ( end - start ) * log ( sub . mean ()) Warning For compatibility reasons, the static attributes model and min_size must be explicitly specified: model is simply a string containing the name of the cost function (can be empty); min_size is a positive integer that indicates the minimum segment size (in number of samples) on which the cost function can be applied. This cost function can now be used with all algorithms from ruptures . For instance, import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data a = np . random . exponential ( scale = 1 , size = 100 ) b = np . random . exponential ( scale = 2 , size = 200 ) signal , bkps = np . r_ [ a , b , a ], [ 100 , 300 , 400 ] # cost algo = rpt . Pelt ( custom_cost = MyCost ()) . fit ( signal ) my_bkps = algo . predict ( pen = 10 ) # display rpt . display ( signal , bkps , my_bkps ) plt . show ()","title":"Custom cost"},{"location":"user-guide/costs/costcustom/#custom-cost-class","text":"Users who are interested in detecting a specific type of change can easily do so by creating a custom cost function. Provided, they subclass the base cost function BaseCost , they will be able to seamlessly run the algorithms implemented in ruptures . Important The custom cost class must at least implement the two following methods: .fit(signal) and .error(start, end) (see user guide ).","title":"Custom cost class"},{"location":"user-guide/costs/costcustom/#example","text":"Let \\(\\{y_t\\}_t\\) denote a 1D piecewise stationary random process. Assume that the \\(y_t\\) are independent and exponentially distributed with a scale parameter that shifts at some unknown instants \\(t_1,t_2,\\dots\\) The change points estimates are the minimizers of the negative log-likelihood, and the associated cost function is given by \\[ c(y_I) = |I| \\log \\bar{\\mu}_I \\] where \\(I,\\, y_I\\) and \\(\\bar{\\mu}_I\\) are respectively an interval, the sub-signal on this interval and the empirical mean of this sub-signal. The following code implements this cost function: from math import log from ruptures.base import BaseCost class MyCost ( BaseCost ): \"\"\"Custom cost for exponential signals.\"\"\" # The 2 following attributes must be specified for compatibility. model = \"\" min_size = 2 def fit ( self , signal ): \"\"\"Set the internal parameter.\"\"\" self . signal = signal return self def error ( self , start , end ): \"\"\"Return the approximation cost on the segment [start:end]. Args: start (int): start of the segment end (int): end of the segment Returns: float: segment cost \"\"\" sub = self . signal [ start : end ] return ( end - start ) * log ( sub . mean ()) Warning For compatibility reasons, the static attributes model and min_size must be explicitly specified: model is simply a string containing the name of the cost function (can be empty); min_size is a positive integer that indicates the minimum segment size (in number of samples) on which the cost function can be applied. This cost function can now be used with all algorithms from ruptures . For instance, import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data a = np . random . exponential ( scale = 1 , size = 100 ) b = np . random . exponential ( scale = 2 , size = 200 ) signal , bkps = np . r_ [ a , b , a ], [ 100 , 300 , 400 ] # cost algo = rpt . Pelt ( custom_cost = MyCost ()) . fit ( signal ) my_bkps = algo . predict ( pen = 10 ) # display rpt . display ( signal , bkps , my_bkps ) plt . show ()","title":"Example"},{"location":"user-guide/costs/costl1/","text":"Least absolute deviation ( CostL1 ) # Description # This cost function detects changes in the median of a signal. Overall, it is a robust estimator of a shift in the central point (mean, median, mode) of a distribution [Bai1995] . Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\(I\\) , \\[ c(y_{I}) = \\sum_{t\\in I} \\|y_t - \\bar{y}\\|_1 \\] where \\(\\bar{y}\\) is the componentwise median of \\(\\{y_t\\}_{t\\in I}\\) . Usage # Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) Then create a CostL1 instance and print the cost of the sub-signal signal[50:150] . c = rpt . costs . CostL1 () . fit ( signal ) print ( c . error ( 50 , 150 )) You can also compute the sum of costs for a given list of change points. print ( c . sum_of_costs ( bkps )) print ( c . sum_of_costs ([ 10 , 100 , 200 , 250 , n ])) In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator , either pass a CostL1 instance (through the argument custom_cost ) or set model=\"l1\" . c = rpt . costs . CostL1 () algo = rpt . Dynp ( custom_cost = c ) # is equivalent to algo = rpt . Dynp ( model = \"l1\" ) References # [Bai1995] Bai, J. (1995). Least absolute deviation of a shift. Econometric Theory, 11(3), 403\u2013436.","title":"CostL1"},{"location":"user-guide/costs/costl1/#least-absolute-deviation-costl1","text":"","title":"Least absolute deviation (CostL1)"},{"location":"user-guide/costs/costl1/#description","text":"This cost function detects changes in the median of a signal. Overall, it is a robust estimator of a shift in the central point (mean, median, mode) of a distribution [Bai1995] . Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\(I\\) , \\[ c(y_{I}) = \\sum_{t\\in I} \\|y_t - \\bar{y}\\|_1 \\] where \\(\\bar{y}\\) is the componentwise median of \\(\\{y_t\\}_{t\\in I}\\) .","title":"Description"},{"location":"user-guide/costs/costl1/#usage","text":"Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) Then create a CostL1 instance and print the cost of the sub-signal signal[50:150] . c = rpt . costs . CostL1 () . fit ( signal ) print ( c . error ( 50 , 150 )) You can also compute the sum of costs for a given list of change points. print ( c . sum_of_costs ( bkps )) print ( c . sum_of_costs ([ 10 , 100 , 200 , 250 , n ])) In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator , either pass a CostL1 instance (through the argument custom_cost ) or set model=\"l1\" . c = rpt . costs . CostL1 () algo = rpt . Dynp ( custom_cost = c ) # is equivalent to algo = rpt . Dynp ( model = \"l1\" )","title":"Usage"},{"location":"user-guide/costs/costl1/#references","text":"[Bai1995] Bai, J. (1995). Least absolute deviation of a shift. Econometric Theory, 11(3), 403\u2013436.","title":"References"},{"location":"user-guide/costs/costl2/","text":"Least squared deviation ( CostL2 ) # Description # This cost function detects mean-shifts in a signal. Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\(I\\) , \\[ c(y_{I}) = \\sum_{t\\in I} \\|y_t - \\bar{y}\\|_2^2 \\] where \\(\\bar{y}\\) is the mean of \\(\\{y_t\\}_{t\\in I}\\) . Usage # Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) Then create a CostL2 instance and print the cost of the sub-signal signal[50:150] . c = rpt . costs . CostL2 () . fit ( signal ) print ( c . error ( 50 , 150 )) You can also compute the sum of costs for a given list of change points. print ( c . sum_of_costs ( bkps )) print ( c . sum_of_costs ([ 10 , 100 , 200 , 250 , n ])) In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator ), either pass a CostL2 instance (through the argument custom_cost ) or set model=\"l2\" . c = rpt . costs . CostL2 () algo = rpt . Dynp ( custom_cost = c ) # is equivalent to algo = rpt . Dynp ( model = \"l2\" )","title":"CostL2"},{"location":"user-guide/costs/costl2/#least-squared-deviation-costl2","text":"","title":"Least squared deviation (CostL2)"},{"location":"user-guide/costs/costl2/#description","text":"This cost function detects mean-shifts in a signal. Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\(I\\) , \\[ c(y_{I}) = \\sum_{t\\in I} \\|y_t - \\bar{y}\\|_2^2 \\] where \\(\\bar{y}\\) is the mean of \\(\\{y_t\\}_{t\\in I}\\) .","title":"Description"},{"location":"user-guide/costs/costl2/#usage","text":"Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) Then create a CostL2 instance and print the cost of the sub-signal signal[50:150] . c = rpt . costs . CostL2 () . fit ( signal ) print ( c . error ( 50 , 150 )) You can also compute the sum of costs for a given list of change points. print ( c . sum_of_costs ( bkps )) print ( c . sum_of_costs ([ 10 , 100 , 200 , 250 , n ])) In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator ), either pass a CostL2 instance (through the argument custom_cost ) or set model=\"l2\" . c = rpt . costs . CostL2 () algo = rpt . Dynp ( custom_cost = c ) # is equivalent to algo = rpt . Dynp ( model = \"l2\" )","title":"Usage"},{"location":"user-guide/costs/costlinear/","text":"Linear model change ( CostLinear ) # Description # Let \\(0 < t_1 < t_2 < \\dots < n\\) be unknown change points indexes. Consider the following multiple linear regression model \\[ y_t = x_t' \\delta_j + \\varepsilon_t, \\quad \\forall t=t_j,\\dots,t_{j+1}-1 \\] for \\(j>1\\) . Here, the observed dependant variable is \\(y_t\\in\\mathbb{R}\\) , the covariate vector is \\(x_t \\in\\mathbb{R}^p\\) , the disturbance is \\(\\varepsilon_t\\in\\mathbb{R}\\) . The vectors \\(\\delta_j\\in\\mathbb{R}^p\\) are the parameter vectors (or regression coefficients). The least-squares estimates of the break dates is obtained by minimizing the sum of squared residuals [Bai2003] . Formally, the associated cost function on an interval \\(I\\) is \\[ c(y_{I}) = \\min_{\\delta\\in\\mathbb{R}^p} \\sum_{t\\in I} \\|y_t - \\delta' x_t \\|_2^2. \\] Usage # Start with the usual imports and create a signal with piecewise linear trends. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , n_reg = 2000 , 3 # number of samples, number of regressors (including intercept) n_bkps = 3 # number of change points # regressors tt = np . linspace ( 0 , 10 * np . pi , n ) X = np . vstack (( np . sin ( tt ), np . sin ( 5 * tt ), np . ones ( n ))) . T # parameter vectors deltas , bkps = rpt . pw_constant ( n , n_reg , n_bkps , noise_std = None , delta = ( 1 , 3 )) # observed signal y = np . sum ( X * deltas , axis = 1 ) y += np . random . normal ( size = y . shape ) # display signal rpt . show . display ( y , bkps , figsize = ( 10 , 6 )) plt . show () Then create a CostLinear instance and print the cost of the sub-signal signal[50:150] . # stack observed signal and regressors. # first dimension is the observed signal. signal = np . column_stack (( y . reshape ( - 1 , 1 ), X )) c = rpt . costs . CostLinear () . fit ( signal ) print ( c . error ( 50 , 150 )) You can also compute the sum of costs for a given list of change points. print ( c . sum_of_costs ( bkps )) print ( c . sum_of_costs ([ 10 , 100 , 200 , 250 , n ])) In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator ), either pass a CostLinear instance (through the argument custom_cost ) or set model=\"linear\" . c = rpt . costs . CostLinear () algo = rpt . Dynp ( custom_cost = c ) # is equivalent to algo = rpt . Dynp ( model = \"linear\" ) References # [Bai2003] J. Bai and P. Perron. Critical values for multiple structural change tests. Econometrics Journal, 6(1):72\u201378, 2003.","title":"CostLinear"},{"location":"user-guide/costs/costlinear/#linear-model-change-costlinear","text":"","title":"Linear model change (CostLinear)"},{"location":"user-guide/costs/costlinear/#description","text":"Let \\(0 < t_1 < t_2 < \\dots < n\\) be unknown change points indexes. Consider the following multiple linear regression model \\[ y_t = x_t' \\delta_j + \\varepsilon_t, \\quad \\forall t=t_j,\\dots,t_{j+1}-1 \\] for \\(j>1\\) . Here, the observed dependant variable is \\(y_t\\in\\mathbb{R}\\) , the covariate vector is \\(x_t \\in\\mathbb{R}^p\\) , the disturbance is \\(\\varepsilon_t\\in\\mathbb{R}\\) . The vectors \\(\\delta_j\\in\\mathbb{R}^p\\) are the parameter vectors (or regression coefficients). The least-squares estimates of the break dates is obtained by minimizing the sum of squared residuals [Bai2003] . Formally, the associated cost function on an interval \\(I\\) is \\[ c(y_{I}) = \\min_{\\delta\\in\\mathbb{R}^p} \\sum_{t\\in I} \\|y_t - \\delta' x_t \\|_2^2. \\]","title":"Description"},{"location":"user-guide/costs/costlinear/#usage","text":"Start with the usual imports and create a signal with piecewise linear trends. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , n_reg = 2000 , 3 # number of samples, number of regressors (including intercept) n_bkps = 3 # number of change points # regressors tt = np . linspace ( 0 , 10 * np . pi , n ) X = np . vstack (( np . sin ( tt ), np . sin ( 5 * tt ), np . ones ( n ))) . T # parameter vectors deltas , bkps = rpt . pw_constant ( n , n_reg , n_bkps , noise_std = None , delta = ( 1 , 3 )) # observed signal y = np . sum ( X * deltas , axis = 1 ) y += np . random . normal ( size = y . shape ) # display signal rpt . show . display ( y , bkps , figsize = ( 10 , 6 )) plt . show () Then create a CostLinear instance and print the cost of the sub-signal signal[50:150] . # stack observed signal and regressors. # first dimension is the observed signal. signal = np . column_stack (( y . reshape ( - 1 , 1 ), X )) c = rpt . costs . CostLinear () . fit ( signal ) print ( c . error ( 50 , 150 )) You can also compute the sum of costs for a given list of change points. print ( c . sum_of_costs ( bkps )) print ( c . sum_of_costs ([ 10 , 100 , 200 , 250 , n ])) In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator ), either pass a CostLinear instance (through the argument custom_cost ) or set model=\"linear\" . c = rpt . costs . CostLinear () algo = rpt . Dynp ( custom_cost = c ) # is equivalent to algo = rpt . Dynp ( model = \"linear\" )","title":"Usage"},{"location":"user-guide/costs/costlinear/#references","text":"[Bai2003] J. Bai and P. Perron. Critical values for multiple structural change tests. Econometrics Journal, 6(1):72\u201378, 2003.","title":"References"},{"location":"user-guide/costs/costml/","text":"Change detection with a Mahalanobis-type metric ( CostMl ) # Description # Given a positive semi-definite matrix \\(M\\in\\mathbb{R}^{d\\times d}\\) , this cost function detects changes in the mean of the embedded signal defined by the pseudo-metric \\[ \\| x - y \\|_M^2 = (x-y)^t M (x-y). \\] Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\(I\\) , the cost function is equal to \\[ c(y_{I}) = \\sum_{t\\in I} \\| y_t - \\bar{\\mu} \\|_{M}^2 \\] where \\(\\bar{\\mu}\\) is the empirical mean of the sub-signal \\(\\{y_t\\}_{t\\in I}\\) . The matrix \\(M\\) can for instance be the result of a similarity learning algorithm [ Xing2003 , Truong2019 ] or the inverse of the empirical covariance matrix (yielding the Mahalanobis distance). Usage # Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) Then create a CostMl instance and print the cost of the sub-signal signal[50:150] . M = np . eye ( dim ) c = rpt . costs . CostMl ( metric = M ) . fit ( signal ) print ( c . error ( 50 , 150 )) You can also compute the sum of costs for a given list of change points. print ( c . sum_of_costs ( bkps )) print ( c . sum_of_costs ([ 10 , 100 , 200 , 250 , n ])) In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator ), either pass a CostMl instance (through the argument custom_cost ) or set model=\"mahalanobis\" . c = rpt . costs . CostMl ( metric = M ) algo = rpt . Dynp ( custom_cost = c ) # is equivalent to algo = rpt . Dynp ( model = \"mahalanobis\" , params = { \"metric\" : M }) References # [Xing2003] Xing, E. P., Jordan, M. I., & Russell, S. J. (2003). Distance metric learning, with application to clustering with side-Information. Advances in Neural Information Processing Systems (NIPS), 521\u2013528. [Truong2019] Truong, C., Oudre, L., & Vayatis, N. (2019). Supervised kernel change point detection with partial annotations. Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1\u20135.","title":"CostMl"},{"location":"user-guide/costs/costml/#change-detection-with-a-mahalanobis-type-metric-costml","text":"","title":"Change detection with a Mahalanobis-type metric (CostMl)"},{"location":"user-guide/costs/costml/#description","text":"Given a positive semi-definite matrix \\(M\\in\\mathbb{R}^{d\\times d}\\) , this cost function detects changes in the mean of the embedded signal defined by the pseudo-metric \\[ \\| x - y \\|_M^2 = (x-y)^t M (x-y). \\] Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\(I\\) , the cost function is equal to \\[ c(y_{I}) = \\sum_{t\\in I} \\| y_t - \\bar{\\mu} \\|_{M}^2 \\] where \\(\\bar{\\mu}\\) is the empirical mean of the sub-signal \\(\\{y_t\\}_{t\\in I}\\) . The matrix \\(M\\) can for instance be the result of a similarity learning algorithm [ Xing2003 , Truong2019 ] or the inverse of the empirical covariance matrix (yielding the Mahalanobis distance).","title":"Description"},{"location":"user-guide/costs/costml/#usage","text":"Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) Then create a CostMl instance and print the cost of the sub-signal signal[50:150] . M = np . eye ( dim ) c = rpt . costs . CostMl ( metric = M ) . fit ( signal ) print ( c . error ( 50 , 150 )) You can also compute the sum of costs for a given list of change points. print ( c . sum_of_costs ( bkps )) print ( c . sum_of_costs ([ 10 , 100 , 200 , 250 , n ])) In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator ), either pass a CostMl instance (through the argument custom_cost ) or set model=\"mahalanobis\" . c = rpt . costs . CostMl ( metric = M ) algo = rpt . Dynp ( custom_cost = c ) # is equivalent to algo = rpt . Dynp ( model = \"mahalanobis\" , params = { \"metric\" : M })","title":"Usage"},{"location":"user-guide/costs/costml/#references","text":"[Xing2003] Xing, E. P., Jordan, M. I., & Russell, S. J. (2003). Distance metric learning, with application to clustering with side-Information. Advances in Neural Information Processing Systems (NIPS), 521\u2013528. [Truong2019] Truong, C., Oudre, L., & Vayatis, N. (2019). Supervised kernel change point detection with partial annotations. Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1\u20135.","title":"References"},{"location":"user-guide/costs/costnormal/","text":"Gaussian process change ( CostNormal ) # Description # This cost function detects changes in the mean and covariance matrix of a sequence of multivariate Gaussian random variables. Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\(I\\) , $$ c(y_{I}) = |I| \\log\\det(\\widehat{\\Sigma}_I + \\epsilon\\text{Id}) $$ where \\(\\widehat{\\Sigma}_I\\) is the empirical covariance matrix of the sub-signal \\(\\{y_t\\}_{t\\in I}\\) and \\(\\epsilon>0\\) is a small constant added to cope with badly conditioned covariance matrices (new in version 1.1.5, see Issue 196 ). It is robust to strongly dependant processes; for more information, see [Lavielle1999] (univariate case) and [Lavielle2006] (multivariate case). Usage # Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) Then create a CostNormal instance and print the cost of the sub-signal signal[50:150] . c = rpt . costs . CostNormal () . fit ( signal ) print ( c . error ( 50 , 150 )) You can also compute the sum of costs for a given list of change points. print ( c . sum_of_costs ( bkps )) print ( c . sum_of_costs ([ 10 , 100 , 200 , 250 , n ])) In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator ), either pass a CostNormal instance (through the argument custom_cost ) or set model=\"normal\" . c = rpt . costs . CostNormal () algo = rpt . Dynp ( custom_cost = c ) # is equivalent to algo = rpt . Dynp ( model = \"normal\" ) To set the small diagonal bias to 0 (default behaviour in versions 1.1.4 and before), simply do the following (change Dynp by the search method you need). c = rpt . costs . CostNormal ( add_small_diag = False ) algo = rpt . Dynp ( custom_cost = c ) # or, equivalently, algo = rpt . Dynp ( model = \"normal\" , params = { \"add_small_diag\" : False }) References # [Lavielle1999] Lavielle, M. (1999). Detection of multiples changes in a sequence of dependant variables. Stochastic Processes and Their Applications, 83(1), 79\u2013102. [Lavielle2006] Lavielle, M., & Teyssi\u00e8re, G. (2006). Detection of multiple change-points in multivariate time series. Lithuanian Mathematical Journal, 46(3).","title":"CostNormal"},{"location":"user-guide/costs/costnormal/#gaussian-process-change-costnormal","text":"","title":"Gaussian process change (CostNormal)"},{"location":"user-guide/costs/costnormal/#description","text":"This cost function detects changes in the mean and covariance matrix of a sequence of multivariate Gaussian random variables. Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\(I\\) , $$ c(y_{I}) = |I| \\log\\det(\\widehat{\\Sigma}_I + \\epsilon\\text{Id}) $$ where \\(\\widehat{\\Sigma}_I\\) is the empirical covariance matrix of the sub-signal \\(\\{y_t\\}_{t\\in I}\\) and \\(\\epsilon>0\\) is a small constant added to cope with badly conditioned covariance matrices (new in version 1.1.5, see Issue 196 ). It is robust to strongly dependant processes; for more information, see [Lavielle1999] (univariate case) and [Lavielle2006] (multivariate case).","title":"Description"},{"location":"user-guide/costs/costnormal/#usage","text":"Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) Then create a CostNormal instance and print the cost of the sub-signal signal[50:150] . c = rpt . costs . CostNormal () . fit ( signal ) print ( c . error ( 50 , 150 )) You can also compute the sum of costs for a given list of change points. print ( c . sum_of_costs ( bkps )) print ( c . sum_of_costs ([ 10 , 100 , 200 , 250 , n ])) In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator ), either pass a CostNormal instance (through the argument custom_cost ) or set model=\"normal\" . c = rpt . costs . CostNormal () algo = rpt . Dynp ( custom_cost = c ) # is equivalent to algo = rpt . Dynp ( model = \"normal\" ) To set the small diagonal bias to 0 (default behaviour in versions 1.1.4 and before), simply do the following (change Dynp by the search method you need). c = rpt . costs . CostNormal ( add_small_diag = False ) algo = rpt . Dynp ( custom_cost = c ) # or, equivalently, algo = rpt . Dynp ( model = \"normal\" , params = { \"add_small_diag\" : False })","title":"Usage"},{"location":"user-guide/costs/costnormal/#references","text":"[Lavielle1999] Lavielle, M. (1999). Detection of multiples changes in a sequence of dependant variables. Stochastic Processes and Their Applications, 83(1), 79\u2013102. [Lavielle2006] Lavielle, M., & Teyssi\u00e8re, G. (2006). Detection of multiple change-points in multivariate time series. Lithuanian Mathematical Journal, 46(3).","title":"References"},{"location":"user-guide/costs/costrank/","text":"Rank-based cost function ( CostRank ) # Description # This cost function detects general distribution changes in multivariate signals, using a rank transformation [Lung-Yut-Fong2015] . Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\([a, b)\\) , \\[ c_{rank}(a, b) = -(b - a) \\bar{r}_{a..b}' \\hat{\\Sigma}_r^{-1} \\bar{r}_{a..b} \\] where \\(\\bar{r}_{a..b}\\) is the empirical mean of the sub-signal \\(\\{r_t\\}_{t=a+1}^b\\) , and \\(\\hat{\\Sigma}_r\\) is the covariance matrix of the complete rank signal \\(r\\) . Usage # Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standard deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) Then create a CostRank instance and print the cost of the sub-signal signal[50:150] . c = rpt . costs . CostRank () . fit ( signal ) print ( c . error ( 50 , 150 )) You can also compute the sum of costs for a given list of change points. print ( c . sum_of_costs ( bkps )) print ( c . sum_of_costs ([ 10 , 100 , 200 , 250 , n ])) In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator ), either pass a CostRank instance (through the argument custom_cost ) or set model=\"rank\" . c = rpt . costs . CostRank () algo = rpt . Dynp ( custom_cost = c ) # is equivalent to algo = rpt . Dynp ( model = \"rank\" ) References # [Lung-Yut-Fong2015] Lung-Yut-Fong, A., L\u00e9vy-Leduc, C., & Capp\u00e9, O. (2015). Homogeneity and change-point detection tests for multivariate data using rank statistics. Journal de La Soci\u00e9t\u00e9 Fran\u00e7aise de Statistique, 156(4), 133\u2013162.","title":"CostRank"},{"location":"user-guide/costs/costrank/#rank-based-cost-function-costrank","text":"","title":"Rank-based cost function (CostRank)"},{"location":"user-guide/costs/costrank/#description","text":"This cost function detects general distribution changes in multivariate signals, using a rank transformation [Lung-Yut-Fong2015] . Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\([a, b)\\) , \\[ c_{rank}(a, b) = -(b - a) \\bar{r}_{a..b}' \\hat{\\Sigma}_r^{-1} \\bar{r}_{a..b} \\] where \\(\\bar{r}_{a..b}\\) is the empirical mean of the sub-signal \\(\\{r_t\\}_{t=a+1}^b\\) , and \\(\\hat{\\Sigma}_r\\) is the covariance matrix of the complete rank signal \\(r\\) .","title":"Description"},{"location":"user-guide/costs/costrank/#usage","text":"Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standard deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) Then create a CostRank instance and print the cost of the sub-signal signal[50:150] . c = rpt . costs . CostRank () . fit ( signal ) print ( c . error ( 50 , 150 )) You can also compute the sum of costs for a given list of change points. print ( c . sum_of_costs ( bkps )) print ( c . sum_of_costs ([ 10 , 100 , 200 , 250 , n ])) In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator ), either pass a CostRank instance (through the argument custom_cost ) or set model=\"rank\" . c = rpt . costs . CostRank () algo = rpt . Dynp ( custom_cost = c ) # is equivalent to algo = rpt . Dynp ( model = \"rank\" )","title":"Usage"},{"location":"user-guide/costs/costrank/#references","text":"[Lung-Yut-Fong2015] Lung-Yut-Fong, A., L\u00e9vy-Leduc, C., & Capp\u00e9, O. (2015). Homogeneity and change-point detection tests for multivariate data using rank statistics. Journal de La Soci\u00e9t\u00e9 Fran\u00e7aise de Statistique, 156(4), 133\u2013162.","title":"References"},{"location":"user-guide/costs/costrbf/","text":"Kernelized mean change ( CostRbf ) # Description # Given a positive semi-definite kernel \\(k(\\cdot, \\cdot) : \\mathbb{R}^d\\times \\mathbb{R}^d \\mapsto \\mathbb{R}\\) and its associated feature map \\(\\Phi:\\mathbb{R}^d \\mapsto \\mathcal{H}\\) (where \\(\\mathcal{H}\\) is an appropriate Hilbert space), this cost function detects changes in the mean of the embedded signal \\(\\{\\Phi(y_t)\\}_t\\) [ Garreau2018 , Arlot2019 ]. Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\(I\\) , \\[ c(y_{I}) = \\sum_{t\\in I} \\| \\Phi(y_t) - \\bar{\\mu} \\|_{\\mathcal{H}}^2 \\] where \\(\\bar{\\mu}\\) is the empirical mean of the embedded sub-signal \\(\\{\\Phi(y_t)\\}_{t\\in I}\\) . Here the kernel is the radial basis function (rbf): \\[ k(x, y) = \\exp(-\\gamma \\| x - y \\|^2 ) \\] where \\(\\| \\cdot \\|\\) is the Euclidean norm and \\(\\gamma>0\\) is the so-called bandwidth parameter and is determined according to median heuristics (i.e. equal to the inverse of median of all pairwise distances). In a nutshell, this cost function is able to detect changes in the distribution of an iid sequence of random variables. Because it is non-parametric, it is performs reasonably well on a wide range of tasks. Usage # Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) Then create a CostRbf instance and print the cost of the sub-signal signal[50:150] . c = rpt . costs . CostRbf () . fit ( signal ) print ( c . error ( 50 , 150 )) You can also compute the sum of costs for a given list of change points. print ( c . sum_of_costs ( bkps )) print ( c . sum_of_costs ([ 10 , 100 , 200 , 250 , n ])) In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator ), either pass a CostRbf instance (through the argument custom_cost ) or set model=\"rbf\" . c = rpt . costs . CostRbf () algo = rpt . Dynp ( custom_cost = c ) # is equivalent to algo = rpt . Dynp ( model = \"rbf\" ) References # [Garreau2018] Garreau, D., & Arlot, S. (2018). Consistent change-point detection with kernels. Electronic Journal of Statistics, 12(2), 4440\u20134486. [Arlot2019] Arlot, S., Celisse, A., & Harchaoui, Z. (2019). A kernel multiple change-point algorithm via model selection. Journal of Machine Learning Research, 20(162), 1\u201356.","title":"CostRbf"},{"location":"user-guide/costs/costrbf/#kernelized-mean-change-costrbf","text":"","title":"Kernelized mean change (CostRbf)"},{"location":"user-guide/costs/costrbf/#description","text":"Given a positive semi-definite kernel \\(k(\\cdot, \\cdot) : \\mathbb{R}^d\\times \\mathbb{R}^d \\mapsto \\mathbb{R}\\) and its associated feature map \\(\\Phi:\\mathbb{R}^d \\mapsto \\mathcal{H}\\) (where \\(\\mathcal{H}\\) is an appropriate Hilbert space), this cost function detects changes in the mean of the embedded signal \\(\\{\\Phi(y_t)\\}_t\\) [ Garreau2018 , Arlot2019 ]. Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\(I\\) , \\[ c(y_{I}) = \\sum_{t\\in I} \\| \\Phi(y_t) - \\bar{\\mu} \\|_{\\mathcal{H}}^2 \\] where \\(\\bar{\\mu}\\) is the empirical mean of the embedded sub-signal \\(\\{\\Phi(y_t)\\}_{t\\in I}\\) . Here the kernel is the radial basis function (rbf): \\[ k(x, y) = \\exp(-\\gamma \\| x - y \\|^2 ) \\] where \\(\\| \\cdot \\|\\) is the Euclidean norm and \\(\\gamma>0\\) is the so-called bandwidth parameter and is determined according to median heuristics (i.e. equal to the inverse of median of all pairwise distances). In a nutshell, this cost function is able to detect changes in the distribution of an iid sequence of random variables. Because it is non-parametric, it is performs reasonably well on a wide range of tasks.","title":"Description"},{"location":"user-guide/costs/costrbf/#usage","text":"Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) Then create a CostRbf instance and print the cost of the sub-signal signal[50:150] . c = rpt . costs . CostRbf () . fit ( signal ) print ( c . error ( 50 , 150 )) You can also compute the sum of costs for a given list of change points. print ( c . sum_of_costs ( bkps )) print ( c . sum_of_costs ([ 10 , 100 , 200 , 250 , n ])) In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator ), either pass a CostRbf instance (through the argument custom_cost ) or set model=\"rbf\" . c = rpt . costs . CostRbf () algo = rpt . Dynp ( custom_cost = c ) # is equivalent to algo = rpt . Dynp ( model = \"rbf\" )","title":"Usage"},{"location":"user-guide/costs/costrbf/#references","text":"[Garreau2018] Garreau, D., & Arlot, S. (2018). Consistent change-point detection with kernels. Electronic Journal of Statistics, 12(2), 4440\u20134486. [Arlot2019] Arlot, S., Celisse, A., & Harchaoui, Z. (2019). A kernel multiple change-point algorithm via model selection. Journal of Machine Learning Research, 20(162), 1\u201356.","title":"References"},{"location":"user-guide/datasets/pw_constant/","text":"Piecewise constant ( pw_constant ) # Description # For a given number of samples \\(T\\) , number \\(K\\) of change points and noise variance \\(\\sigma^2\\) , the function pw_constant generates change point dexes \\(0 < t_1 < \\dots < t_K < T\\) and a piecewise constant signal \\(\\{y_t\\}_t\\) with additive Gaussian noise. Usage # Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standard deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) rpt . display ( signal , bkps ) The mean shift amplitude is uniformly drawn from an interval that can be changed through the keyword delta . signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma , delta = ( 1 , 10 ))","title":"Piecewise constant"},{"location":"user-guide/datasets/pw_constant/#piecewise-constant-pw_constant","text":"","title":"Piecewise constant (pw_constant)"},{"location":"user-guide/datasets/pw_constant/#description","text":"For a given number of samples \\(T\\) , number \\(K\\) of change points and noise variance \\(\\sigma^2\\) , the function pw_constant generates change point dexes \\(0 < t_1 < \\dots < t_K < T\\) and a piecewise constant signal \\(\\{y_t\\}_t\\) with additive Gaussian noise.","title":"Description"},{"location":"user-guide/datasets/pw_constant/#usage","text":"Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standard deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) rpt . display ( signal , bkps ) The mean shift amplitude is uniformly drawn from an interval that can be changed through the keyword delta . signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma , delta = ( 1 , 10 ))","title":"Usage"},{"location":"user-guide/datasets/pw_linear/","text":"Piecewise linear ( pw_linear ) # Description # This function pw_linear simulates a piecewise linear model (see Cost linear ). The covariates are standard Gaussian random variables. The response variable is a (piecewise) linear combination of the covariates. Usage # Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension of the covariates n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_linear ( n , dim , n_bkps , noise_std = sigma ) rpt . display ( signal , bkps )","title":"Piecewise linear"},{"location":"user-guide/datasets/pw_linear/#piecewise-linear-pw_linear","text":"","title":"Piecewise linear (pw_linear)"},{"location":"user-guide/datasets/pw_linear/#description","text":"This function pw_linear simulates a piecewise linear model (see Cost linear ). The covariates are standard Gaussian random variables. The response variable is a (piecewise) linear combination of the covariates.","title":"Description"},{"location":"user-guide/datasets/pw_linear/#usage","text":"Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension of the covariates n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_linear ( n , dim , n_bkps , noise_std = sigma ) rpt . display ( signal , bkps )","title":"Usage"},{"location":"user-guide/datasets/pw_normal/","text":"Piecewise 2D Gaussian process ( pw_normal ) # Description # The function pw_normal simulates a 2D signal of Gaussian i.i.d. random variables with zero mean and covariance matrix alternating between \\([[1, 0.9], [0.9, 1]]\\) and \\([[1, -0.9], [-0.9, 1]]\\) at every change point. Top and middle: 2D signal example. Bottom: Scatter plot for each regime type Usage # Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n = 500 , 3 # number of samples n_bkps = 3 # number of change points, noise standart deviation signal , bkps = rpt . pw_normal ( n , n_bkps ) rpt . display ( signal , bkps )","title":"Piecewise Gaussian"},{"location":"user-guide/datasets/pw_normal/#piecewise-2d-gaussian-process-pw_normal","text":"","title":"Piecewise 2D Gaussian process (pw_normal)"},{"location":"user-guide/datasets/pw_normal/#description","text":"The function pw_normal simulates a 2D signal of Gaussian i.i.d. random variables with zero mean and covariance matrix alternating between \\([[1, 0.9], [0.9, 1]]\\) and \\([[1, -0.9], [-0.9, 1]]\\) at every change point. Top and middle: 2D signal example. Bottom: Scatter plot for each regime type","title":"Description"},{"location":"user-guide/datasets/pw_normal/#usage","text":"Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n = 500 , 3 # number of samples n_bkps = 3 # number of change points, noise standart deviation signal , bkps = rpt . pw_normal ( n , n_bkps ) rpt . display ( signal , bkps )","title":"Usage"},{"location":"user-guide/datasets/pw_wavy/","text":"Piecewise sinusoidal signal ( pw_wavy ) # Description # The function pw_wavy simulates a sum-of-sine signal \\(y_t=\\sin(2\\pi f_1 t)+\\sin(2\\pi f_2 t)\\) where \\(t=0,\\dots,T-1\\) . The frequency vector \\([f_1, f_2]\\) alternates between \\([0.075, 0.1]\\) and \\([0.1, 0.125]\\) at each change point index. Gaussian white noise can be added to the signal. Top: signal example. Bottom: associated spectrogram. Usage # Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_wavy ( n , n_bkps , noise_std = sigma ) rpt . display ( signal , bkps )","title":"Piecewise sinusoidal"},{"location":"user-guide/datasets/pw_wavy/#piecewise-sinusoidal-signal-pw_wavy","text":"","title":"Piecewise sinusoidal signal (pw_wavy)"},{"location":"user-guide/datasets/pw_wavy/#description","text":"The function pw_wavy simulates a sum-of-sine signal \\(y_t=\\sin(2\\pi f_1 t)+\\sin(2\\pi f_2 t)\\) where \\(t=0,\\dots,T-1\\) . The frequency vector \\([f_1, f_2]\\) alternates between \\([0.075, 0.1]\\) and \\([0.1, 0.125]\\) at each change point index. Gaussian white noise can be added to the signal. Top: signal example. Bottom: associated spectrogram.","title":"Description"},{"location":"user-guide/datasets/pw_wavy/#usage","text":"Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_wavy ( n , n_bkps , noise_std = sigma ) rpt . display ( signal , bkps )","title":"Usage"},{"location":"user-guide/detection/binseg/","text":"Binary segmentation ( Binseg ) # Description # Binary change point detection is used to perform fast signal segmentation and is implemented in Binseg . It is a sequential approach: first, one change point is detected in the complete input signal, then series is split around this change point, then the operation is repeated on the two resulting sub-signals. For a theoretical and algorithmic analysis of Binseg , see for instance [Bai1997] and [Fryzlewicz2014] . The benefits of binary segmentation includes low complexity (of the order of \\(\\mathcal{O}(Cn\\log n)\\) , where \\(n\\) is the number of samples and \\(C\\) the complexity of calling the considered cost function on one sub-signal), the fact that it can extend any single change point detection method to detect multiple changes points and that it can work whether the number of regimes is known beforehand or not. Schematic view of the binary segmentation algorithm Usage # Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n = 500 # number of samples n_bkps , sigma = 3 , 5 # number of change points, noise standard deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) To perform a binary segmentation of a signal, initialize a BinSeg instance. # change point detection model = \"l2\" # \"l1\", \"rbf\", \"linear\", \"normal\", \"ar\",... algo = rpt . Binseg ( model = model ) . fit ( signal ) my_bkps = algo . predict ( n_bkps = 3 ) # show results rpt . show . display ( signal , bkps , my_bkps , figsize = ( 10 , 6 )) plt . show () In the situation in which the number of change points is unknown, one can specify a penalty using the pen parameter or a threshold on the residual norm using epsilon . my_bkps = algo . predict ( pen = np . log ( n ) * dim * sigma ** 2 ) # or my_bkps = algo . predict ( epsilon = 3 * n * sigma ** 2 ) For faster predictions, one can modify the jump parameter during initialization. The higher it is, the faster the prediction is achieved (at the expense of precision). algo = rpt . Binseg ( model = model , jump = 10 ) . fit ( signal ) References # [Bai1997] Bai, J. (1997). Estimating multiple breaks one at a time. Econometric Theory, 13(3), 315\u2013352. [Fryzlewicz2014] Fryzlewicz, P. (2014). Wild binary segmentation for multiple change-point detection. The Annals of Statistics, 42(6), 2243\u20132281.","title":"Binary segmentation"},{"location":"user-guide/detection/binseg/#binary-segmentation-binseg","text":"","title":"Binary segmentation (Binseg)"},{"location":"user-guide/detection/binseg/#description","text":"Binary change point detection is used to perform fast signal segmentation and is implemented in Binseg . It is a sequential approach: first, one change point is detected in the complete input signal, then series is split around this change point, then the operation is repeated on the two resulting sub-signals. For a theoretical and algorithmic analysis of Binseg , see for instance [Bai1997] and [Fryzlewicz2014] . The benefits of binary segmentation includes low complexity (of the order of \\(\\mathcal{O}(Cn\\log n)\\) , where \\(n\\) is the number of samples and \\(C\\) the complexity of calling the considered cost function on one sub-signal), the fact that it can extend any single change point detection method to detect multiple changes points and that it can work whether the number of regimes is known beforehand or not. Schematic view of the binary segmentation algorithm","title":"Description"},{"location":"user-guide/detection/binseg/#usage","text":"Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n = 500 # number of samples n_bkps , sigma = 3 , 5 # number of change points, noise standard deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) To perform a binary segmentation of a signal, initialize a BinSeg instance. # change point detection model = \"l2\" # \"l1\", \"rbf\", \"linear\", \"normal\", \"ar\",... algo = rpt . Binseg ( model = model ) . fit ( signal ) my_bkps = algo . predict ( n_bkps = 3 ) # show results rpt . show . display ( signal , bkps , my_bkps , figsize = ( 10 , 6 )) plt . show () In the situation in which the number of change points is unknown, one can specify a penalty using the pen parameter or a threshold on the residual norm using epsilon . my_bkps = algo . predict ( pen = np . log ( n ) * dim * sigma ** 2 ) # or my_bkps = algo . predict ( epsilon = 3 * n * sigma ** 2 ) For faster predictions, one can modify the jump parameter during initialization. The higher it is, the faster the prediction is achieved (at the expense of precision). algo = rpt . Binseg ( model = model , jump = 10 ) . fit ( signal )","title":"Usage"},{"location":"user-guide/detection/binseg/#references","text":"[Bai1997] Bai, J. (1997). Estimating multiple breaks one at a time. Econometric Theory, 13(3), 315\u2013352. [Fryzlewicz2014] Fryzlewicz, P. (2014). Wild binary segmentation for multiple change-point detection. The Annals of Statistics, 42(6), 2243\u20132281.","title":"References"},{"location":"user-guide/detection/bottomup/","text":"Bottom-up segmentation ( BottomUp ) # Description # Bottom-up change point detection is used to perform fast signal segmentation and is implemented in BottomUp in a sequential manner. Contrary to binary segmentation, which is a greedy procedure, bottom-up segmentation is generous: it starts with many change points and successively deletes the less significant ones. First, the signal is divided in many sub-signals along a regular grid. Then contiguous segments are successively merged according to a measure of how similar they are. See for instance [Keogh2001] or [Fryzlewicz2007] for an algorithmic analysis of BottomUp . The benefits of bottom-up segmentation includes low complexity (of the order of \\(\\mathcal{O}(n\\log n)\\) , where \\(n\\) is the number of samples), the fact that it can extend any single change point detection method to detect multiple changes points and that it can work whether the number of regimes is known beforehand or not. Schematic view of the bottom-up segmentation algorithm Usage # Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) To perform a bottom-up segmentation of a signal, initialize a BottomUp instance. # change point detection model = \"l2\" # \"l1\", \"rbf\", \"linear\", \"normal\", \"ar\" algo = rpt . BottomUp ( model = model ) . fit ( signal ) my_bkps = algo . predict ( n_bkps = 3 ) # show results rpt . show . display ( signal , bkps , my_bkps , figsize = ( 10 , 6 )) plt . show () In the situation in which the number of change points is unknown, one can specify a penalty using the pen parameter or a threshold on the residual norm using epsilon . my_bkps = algo . predict ( pen = np . log ( n ) * dim * sigma ** 2 ) # or my_bkps = algo . predict ( epsilon = 3 * n * sigma ** 2 ) For faster predictions, one can modify the jump parameter during initialization. The higher it is, the faster the prediction is achieved (at the expense of precision). algo = rpt . BottomUp ( model = model , jump = 10 ) . fit ( signal ) References # [Keogh2001] Keogh, E., Chu, S., Hart, D., & Pazzani, M. (2001). An online algorithm for segmenting time series. Proceedings of the IEEE International Conference on Data Mining (ICDM), 289\u2013296. [Fryzlewicz2007] Fryzlewicz, P. (2007). Unbalanced Haar technique for nonparametric function estimation. Journal of the American Statistical Association, 102(480), 1318\u20131327.","title":"Bottom-up segmentation"},{"location":"user-guide/detection/bottomup/#bottom-up-segmentation-bottomup","text":"","title":"Bottom-up segmentation (BottomUp)"},{"location":"user-guide/detection/bottomup/#description","text":"Bottom-up change point detection is used to perform fast signal segmentation and is implemented in BottomUp in a sequential manner. Contrary to binary segmentation, which is a greedy procedure, bottom-up segmentation is generous: it starts with many change points and successively deletes the less significant ones. First, the signal is divided in many sub-signals along a regular grid. Then contiguous segments are successively merged according to a measure of how similar they are. See for instance [Keogh2001] or [Fryzlewicz2007] for an algorithmic analysis of BottomUp . The benefits of bottom-up segmentation includes low complexity (of the order of \\(\\mathcal{O}(n\\log n)\\) , where \\(n\\) is the number of samples), the fact that it can extend any single change point detection method to detect multiple changes points and that it can work whether the number of regimes is known beforehand or not. Schematic view of the bottom-up segmentation algorithm","title":"Description"},{"location":"user-guide/detection/bottomup/#usage","text":"Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) To perform a bottom-up segmentation of a signal, initialize a BottomUp instance. # change point detection model = \"l2\" # \"l1\", \"rbf\", \"linear\", \"normal\", \"ar\" algo = rpt . BottomUp ( model = model ) . fit ( signal ) my_bkps = algo . predict ( n_bkps = 3 ) # show results rpt . show . display ( signal , bkps , my_bkps , figsize = ( 10 , 6 )) plt . show () In the situation in which the number of change points is unknown, one can specify a penalty using the pen parameter or a threshold on the residual norm using epsilon . my_bkps = algo . predict ( pen = np . log ( n ) * dim * sigma ** 2 ) # or my_bkps = algo . predict ( epsilon = 3 * n * sigma ** 2 ) For faster predictions, one can modify the jump parameter during initialization. The higher it is, the faster the prediction is achieved (at the expense of precision). algo = rpt . BottomUp ( model = model , jump = 10 ) . fit ( signal )","title":"Usage"},{"location":"user-guide/detection/bottomup/#references","text":"[Keogh2001] Keogh, E., Chu, S., Hart, D., & Pazzani, M. (2001). An online algorithm for segmenting time series. Proceedings of the IEEE International Conference on Data Mining (ICDM), 289\u2013296. [Fryzlewicz2007] Fryzlewicz, P. (2007). Unbalanced Haar technique for nonparametric function estimation. Journal of the American Statistical Association, 102(480), 1318\u20131327.","title":"References"},{"location":"user-guide/detection/dynp/","text":"Dynamic programming ( Dynp ) # Description # The method is implemented in both Dynp , which is a full native python implementation for which the user can choose any cost functions defined in ruptures.costs It finds the (exact) minimum of the sum of costs by computing the cost of all subsequences of a given signal. It is called \"dynamic programming\" because the search over all possible segmentations is ordered using a dynamic programming approach. In order to work, the user must specify in advance the number of changes to detect . (Consider using penalized methods when this number is unknown.) The complexity of the dynamic programming approach is of the order \\(\\mathcal{O}(CKn^2)\\) , where \\(K\\) is the number of change points to detect, \\(n\\) the number of samples and \\(C\\) the complexity of calling the considered cost function on one sub-signal. Consequently, piecewise constant models ( model=l2 ) are significantly faster than linear or autoregressive models. To reduce the computational cost, you can consider only a subsample of possible change point indexes, by changing the min_size and jump arguments when instantiating Dynp : min_size controls the minimum distance between change points; for instance, if min_size=10 , all change points will be at least 10 samples apart. jump controls the grid of possible change points; for instance, if jump=k , only changes at k, 2*k, 3*k,... are considered. Usage # import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 n_bkps , sigma = 3 , 5 signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) # change point detection model = \"l1\" # \"l2\", \"rbf\" algo = rpt . Dynp ( model = model , min_size = 3 , jump = 5 ) . fit ( signal ) my_bkps = algo . predict ( n_bkps = 3 ) # show results rpt . show . display ( signal , bkps , my_bkps , figsize = ( 10 , 6 )) plt . show ()","title":"Dynamic programming"},{"location":"user-guide/detection/dynp/#dynamic-programming-dynp","text":"","title":"Dynamic programming (Dynp)"},{"location":"user-guide/detection/dynp/#description","text":"The method is implemented in both Dynp , which is a full native python implementation for which the user can choose any cost functions defined in ruptures.costs It finds the (exact) minimum of the sum of costs by computing the cost of all subsequences of a given signal. It is called \"dynamic programming\" because the search over all possible segmentations is ordered using a dynamic programming approach. In order to work, the user must specify in advance the number of changes to detect . (Consider using penalized methods when this number is unknown.) The complexity of the dynamic programming approach is of the order \\(\\mathcal{O}(CKn^2)\\) , where \\(K\\) is the number of change points to detect, \\(n\\) the number of samples and \\(C\\) the complexity of calling the considered cost function on one sub-signal. Consequently, piecewise constant models ( model=l2 ) are significantly faster than linear or autoregressive models. To reduce the computational cost, you can consider only a subsample of possible change point indexes, by changing the min_size and jump arguments when instantiating Dynp : min_size controls the minimum distance between change points; for instance, if min_size=10 , all change points will be at least 10 samples apart. jump controls the grid of possible change points; for instance, if jump=k , only changes at k, 2*k, 3*k,... are considered.","title":"Description"},{"location":"user-guide/detection/dynp/#usage","text":"import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 n_bkps , sigma = 3 , 5 signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) # change point detection model = \"l1\" # \"l2\", \"rbf\" algo = rpt . Dynp ( model = model , min_size = 3 , jump = 5 ) . fit ( signal ) my_bkps = algo . predict ( n_bkps = 3 ) # show results rpt . show . display ( signal , bkps , my_bkps , figsize = ( 10 , 6 )) plt . show ()","title":"Usage"},{"location":"user-guide/detection/kernelcpd/","text":"Kernel change point detection # Problem formulation # In this section, the kernel change point detection setting is briefly described. The interested reader can refer to [ Celisse2018 , Arlot2019 ] for a more complete introduction. Let \\(y = \\{y_0,y_1,\\dots,y_{T-1}\\}\\) denote a \\(\\mathbb{R}^d\\) -valued signal with \\(T\\) samples. This signal is mapped onto a reproducing Hilbert space (rkhs) \\(\\mathcal{H}\\) associated with a user-defined kernel function \\(k(\\cdot, \\cdot):\\mathbb{R}^d\\times\\mathbb{R}^d\\rightarrow\\mathbb{R}\\) . The mapping function \\(\\phi:\\mathbb{R}^d\\rightarrow\\mathcal{H}\\) onto this rkhs is implicitly defined by \\(\\phi(y_t) = k(y_t, \\cdot)\\in\\mathcal{H}\\) resulting in the following inner-product and norm: \\[ \\langle\\phi(y_s)\\mid\\phi(y_t)\\rangle_{\\mathcal{H}} = k(y_s,y_t) \\] and \\[ \\|\\phi(y_t)\\|_{\\mathcal{H}}^2 = k(y_t,y_t) \\] for any samples \\(y_s,y_t\\in\\mathbb{R}^d\\) . Kernel change point detection consists in finding mean-shifts in the mapped signal \\(\\phi(y)\\) by minimizing \\(V(\\cdot)\\) where \\[ V(t_1,\\dots,t_K) := \\sum_{k=0}^K\\sum_{t=t_k}^{t_{k+1}-1} \\|\\phi(y_t)-\\bar{\\mu}_{t_k..t_{k+1}}\\|^2_{\\mathcal{H}} \\] where \\(\\bar{\\mu}_{t_k..t_{k+1}}\\) is the empirical mean of the sub-signal \\(\\phi(y_{t_k}), \\phi(y_{t_k+1}),\\dots,\\phi(y_{t_{k+1}-1})\\) , and \\(t_1,t_2,\\dots,t_K\\) are change point indexes, in increasing order. (By convention \\(t_0=0\\) and \\(t_{K+1}=T\\) .) If the number of changes is known beforehand , we solve the following optimization problem, over all possible change positions \\(t_1<t_2<\\dots<t_K\\) (where the number \\(K\\) of changes is provided by the user): \\[ \\hat{t}_1,\\dots,\\hat{t}_K := \\arg\\min_{t_1,\\dots,t_K} V(t_1,\\dots,t_K). \\] The exact optimization procedure is described in [Celisse2018] . If the number of changes is not known , we solve the following penalized optimization problem \\[ \\hat{K}, \\{\\hat{t}_1,\\dots,\\hat{t}_{\\hat{K}}\\} := \\arg\\min_{K, \\{t_1,\\dots, t_K\\}} V(t_1,\\dots, t_K) + \\beta K \\] where \\(\\beta>0\\) is the smoothing parameter (provided by the user) and \\(\\hat{K}\\) is the estimated number of change points. Higher values of \\(\\beta\\) produce lower \\(\\hat{K}\\) . The exact optimization procedure is described in [Killick2012] . Available kernels # We list below a number of kernels that are already implemented in ruptures . In the following, \\(u\\) and \\(v\\) are two d-dimensional vectors and \\(\\|\\cdot\\|\\) is the Euclidean norm. Kernel Description Cost function Linear model=\"linear\" \\(k_{\\text{linear}}(u, v) = u^T v\\) . CostL2 Gaussian model=\"rbf\" \\(k_{\\text{Gaussian}}(u,v)=\\exp(-\\gamma \\|u-v\\|^2)\\) where \\(\\gamma>0\\) is a user-defined parameter. CostRbf Cosine model=\"cosine\" \\(k_{\\text{cosine}}(u, v) = (u^T v)/(\\|u\\|\\|v\\|)\\) CostCosine Implementation and usage # Kernel change point detection is implemented in the class KernelCPD , which is a C implementation of dynamic programming and PELT. To see it in action, please look at the gallery of examples, in particular: Kernel change point detection: a performance comparison The exact class API is available here . References # [Gretton2012] Gretton, A., Borgwardt, K. M., Rasch, M. J., Sch\u00f6lkopf, B., & Smola, A. (2012). A kernel two-sample test. The Journal of Machine Learning Research, 13, 723\u2013773. [Killick2012] Killick, R., Fearnhead, P., & Eckley, I. (2012). Optimal detection of changepoints with a linear computational cost. Journal of the American Statistical Association, 107(500), 1590\u20131598. [Celisse2018] Celisse, A., Marot, G., Pierre-Jean, M., & Rigaill, G. (2018). New efficient algorithms for multiple change-point detection with reproducing kernels. Computational Statistics and Data Analysis, 128, 200\u2013220. [Arlot2019] Arlot, S., Celisse, A., & Harchaoui, Z. (2019). A kernel multiple change-point algorithm via model selection. Journal of Machine Learning Research, 20(162), 1\u201356.","title":"Kernel change detection"},{"location":"user-guide/detection/kernelcpd/#kernel-change-point-detection","text":"","title":"Kernel change point detection"},{"location":"user-guide/detection/kernelcpd/#problem-formulation","text":"In this section, the kernel change point detection setting is briefly described. The interested reader can refer to [ Celisse2018 , Arlot2019 ] for a more complete introduction. Let \\(y = \\{y_0,y_1,\\dots,y_{T-1}\\}\\) denote a \\(\\mathbb{R}^d\\) -valued signal with \\(T\\) samples. This signal is mapped onto a reproducing Hilbert space (rkhs) \\(\\mathcal{H}\\) associated with a user-defined kernel function \\(k(\\cdot, \\cdot):\\mathbb{R}^d\\times\\mathbb{R}^d\\rightarrow\\mathbb{R}\\) . The mapping function \\(\\phi:\\mathbb{R}^d\\rightarrow\\mathcal{H}\\) onto this rkhs is implicitly defined by \\(\\phi(y_t) = k(y_t, \\cdot)\\in\\mathcal{H}\\) resulting in the following inner-product and norm: \\[ \\langle\\phi(y_s)\\mid\\phi(y_t)\\rangle_{\\mathcal{H}} = k(y_s,y_t) \\] and \\[ \\|\\phi(y_t)\\|_{\\mathcal{H}}^2 = k(y_t,y_t) \\] for any samples \\(y_s,y_t\\in\\mathbb{R}^d\\) . Kernel change point detection consists in finding mean-shifts in the mapped signal \\(\\phi(y)\\) by minimizing \\(V(\\cdot)\\) where \\[ V(t_1,\\dots,t_K) := \\sum_{k=0}^K\\sum_{t=t_k}^{t_{k+1}-1} \\|\\phi(y_t)-\\bar{\\mu}_{t_k..t_{k+1}}\\|^2_{\\mathcal{H}} \\] where \\(\\bar{\\mu}_{t_k..t_{k+1}}\\) is the empirical mean of the sub-signal \\(\\phi(y_{t_k}), \\phi(y_{t_k+1}),\\dots,\\phi(y_{t_{k+1}-1})\\) , and \\(t_1,t_2,\\dots,t_K\\) are change point indexes, in increasing order. (By convention \\(t_0=0\\) and \\(t_{K+1}=T\\) .) If the number of changes is known beforehand , we solve the following optimization problem, over all possible change positions \\(t_1<t_2<\\dots<t_K\\) (where the number \\(K\\) of changes is provided by the user): \\[ \\hat{t}_1,\\dots,\\hat{t}_K := \\arg\\min_{t_1,\\dots,t_K} V(t_1,\\dots,t_K). \\] The exact optimization procedure is described in [Celisse2018] . If the number of changes is not known , we solve the following penalized optimization problem \\[ \\hat{K}, \\{\\hat{t}_1,\\dots,\\hat{t}_{\\hat{K}}\\} := \\arg\\min_{K, \\{t_1,\\dots, t_K\\}} V(t_1,\\dots, t_K) + \\beta K \\] where \\(\\beta>0\\) is the smoothing parameter (provided by the user) and \\(\\hat{K}\\) is the estimated number of change points. Higher values of \\(\\beta\\) produce lower \\(\\hat{K}\\) . The exact optimization procedure is described in [Killick2012] .","title":"Problem formulation"},{"location":"user-guide/detection/kernelcpd/#available-kernels","text":"We list below a number of kernels that are already implemented in ruptures . In the following, \\(u\\) and \\(v\\) are two d-dimensional vectors and \\(\\|\\cdot\\|\\) is the Euclidean norm. Kernel Description Cost function Linear model=\"linear\" \\(k_{\\text{linear}}(u, v) = u^T v\\) . CostL2 Gaussian model=\"rbf\" \\(k_{\\text{Gaussian}}(u,v)=\\exp(-\\gamma \\|u-v\\|^2)\\) where \\(\\gamma>0\\) is a user-defined parameter. CostRbf Cosine model=\"cosine\" \\(k_{\\text{cosine}}(u, v) = (u^T v)/(\\|u\\|\\|v\\|)\\) CostCosine","title":"Available kernels"},{"location":"user-guide/detection/kernelcpd/#implementation-and-usage","text":"Kernel change point detection is implemented in the class KernelCPD , which is a C implementation of dynamic programming and PELT. To see it in action, please look at the gallery of examples, in particular: Kernel change point detection: a performance comparison The exact class API is available here .","title":"Implementation and usage"},{"location":"user-guide/detection/kernelcpd/#references","text":"[Gretton2012] Gretton, A., Borgwardt, K. M., Rasch, M. J., Sch\u00f6lkopf, B., & Smola, A. (2012). A kernel two-sample test. The Journal of Machine Learning Research, 13, 723\u2013773. [Killick2012] Killick, R., Fearnhead, P., & Eckley, I. (2012). Optimal detection of changepoints with a linear computational cost. Journal of the American Statistical Association, 107(500), 1590\u20131598. [Celisse2018] Celisse, A., Marot, G., Pierre-Jean, M., & Rigaill, G. (2018). New efficient algorithms for multiple change-point detection with reproducing kernels. Computational Statistics and Data Analysis, 128, 200\u2013220. [Arlot2019] Arlot, S., Celisse, A., & Harchaoui, Z. (2019). A kernel multiple change-point algorithm via model selection. Journal of Machine Learning Research, 20(162), 1\u201356.","title":"References"},{"location":"user-guide/detection/pelt/","text":"Linearly penalized segmentation ( Pelt ) # Description # The method is implemented in Pelt . Because the enumeration of all possible partitions impossible, the algorithm relies on a pruning rule. Many indexes are discarded, greatly reducing the computational cost while retaining the ability to find the optimal segmentation. The implementation follows [Killick2012] . In addition, under certain conditions on the change point repartition, the avarage computational complexity is of the order of \\(\\mathcal{O}(CKn)\\) , where \\(K\\) is the number of change points to detect, \\(n\\) the number of samples and \\(C\\) the complexity of calling the considered cost function on one sub-signal. Consequently, piecewise constant models ( model=l2 ) are significantly faster than linear or autoregressive models. To reduce the computational cost, you can consider only a subsample of possible change point indexes, by changing the min_size and jump arguments when instantiating Pelt : min_size controls the minimum distance between change points; for instance, if min_size=10 , all change points will be at least 10 samples apart. jump controls the grid of possible change points; for instance, if jump=k , only changes at k, 2*k, 3*k,... are considered. Usage # import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 n_bkps , sigma = 3 , 1 signal , b = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) # change point detection model = \"l1\" # \"l2\", \"rbf\" algo = rpt . Pelt ( model = model , min_size = 3 , jump = 5 ) . fit ( signal ) my_bkps = algo . predict ( pen = 3 ) # show results fig , ( ax ,) = rpt . display ( signal , bkps , my_bkps , figsize = ( 10 , 6 )) plt . show () References # [Killick2012] Killick, R., Fearnhead, P., & Eckley, I. (2012). Optimal detection of changepoints with a linear computational cost. Journal of the American Statistical Association, 107(500), 1590\u20131598.","title":"Pelt"},{"location":"user-guide/detection/pelt/#linearly-penalized-segmentation-pelt","text":"","title":"Linearly penalized segmentation (Pelt)"},{"location":"user-guide/detection/pelt/#description","text":"The method is implemented in Pelt . Because the enumeration of all possible partitions impossible, the algorithm relies on a pruning rule. Many indexes are discarded, greatly reducing the computational cost while retaining the ability to find the optimal segmentation. The implementation follows [Killick2012] . In addition, under certain conditions on the change point repartition, the avarage computational complexity is of the order of \\(\\mathcal{O}(CKn)\\) , where \\(K\\) is the number of change points to detect, \\(n\\) the number of samples and \\(C\\) the complexity of calling the considered cost function on one sub-signal. Consequently, piecewise constant models ( model=l2 ) are significantly faster than linear or autoregressive models. To reduce the computational cost, you can consider only a subsample of possible change point indexes, by changing the min_size and jump arguments when instantiating Pelt : min_size controls the minimum distance between change points; for instance, if min_size=10 , all change points will be at least 10 samples apart. jump controls the grid of possible change points; for instance, if jump=k , only changes at k, 2*k, 3*k,... are considered.","title":"Description"},{"location":"user-guide/detection/pelt/#usage","text":"import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 n_bkps , sigma = 3 , 1 signal , b = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) # change point detection model = \"l1\" # \"l2\", \"rbf\" algo = rpt . Pelt ( model = model , min_size = 3 , jump = 5 ) . fit ( signal ) my_bkps = algo . predict ( pen = 3 ) # show results fig , ( ax ,) = rpt . display ( signal , bkps , my_bkps , figsize = ( 10 , 6 )) plt . show ()","title":"Usage"},{"location":"user-guide/detection/pelt/#references","text":"[Killick2012] Killick, R., Fearnhead, P., & Eckley, I. (2012). Optimal detection of changepoints with a linear computational cost. Journal of the American Statistical Association, 107(500), 1590\u20131598.","title":"References"},{"location":"user-guide/detection/window/","text":"Window-based change point detection ( Window ) # Description # Window-based change point detection is used to perform fast signal segmentation and is implemented in Window . The algorithm uses two windows which slide along the data stream. The statistical properties of the signals within each window are compared with a discrepancy measure. For a given cost function \\(c(\\cdot)\\) , a discrepancy measure is derived \\(d(\\cdot,\\cdot)\\) as follows: \\[ d(y_{u..v}, y_{v..w}) = c(y_{u..w}) - c(y_{u..v}) - c(y_{v..w}) \\] where \\(\\{y_t\\}_t\\) is the input signal and \\(u < v < w\\) are indexes. The discrepancy is the cost gain of splitting the sub-signal \\(y_{u..w}\\) at the index \\(v\\) . If the sliding windows \\(u..v\\) and \\(v..w\\) both fall into a segment, their statistical properties are similar and the discrepancy between the first window and the second window is low. If the sliding windows fall into two dissimilar segments, the discrepancy is significantly higher, suggesting that the boundary between windows is a change point. The discrepancy curve is the curve, defined for all indexes \\(t\\) between \\(w/2\\) and \\(n-w/2\\) ( \\(n\\) is the number of samples), \\[ \\big(t, d(y_{t-w/2..t}, y_{t..t+w/2})\\big) \\] where \\(w\\) is the window length. A sequential peak search is performed on the discrepancy curve in order to detect change points. The benefits of window-based segmentation includes low complexity (of the order of \\(\\mathcal{O}(n w)\\) , where \\(n\\) is the number of samples), the fact that it can extend any single change point detection method to detect multiple changes points and that it can work whether the number of regimes is known beforehand or not. Schematic view of the window sliding segmentation algorithm Usage # Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) To perform a binary segmentation of a signal, initialize a Window instance. # change point detection model = \"l2\" # \"l1\", \"rbf\", \"linear\", \"normal\", \"ar\" algo = rpt . Window ( width = 40 , model = model ) . fit ( signal ) my_bkps = algo . predict ( n_bkps = 3 ) # show results rpt . show . display ( signal , bkps , my_bkps , figsize = ( 10 , 6 )) plt . show () The window length (in number of samples) is modified through the argument width . Usual methods assume that the window length is smaller than the smallest regime length. In the situation in which the number of change points is unknown, one can specify a penalty using the pen parameter or a threshold on the residual norm using epsilon . my_bkps = algo . predict ( pen = np . log ( n ) * dim * sigma ** 2 ) # or my_bkps = algo . predict ( epsilon = 3 * n * sigma ** 2 ) For faster predictions, one can modify the jump parameter during initialization. The higher it is, the faster the prediction is achieved (at the expense of precision). algo = rpt . Window ( model = model , jump = 10 ) . fit ( signal )","title":"Window sliding segmentation"},{"location":"user-guide/detection/window/#window-based-change-point-detection-window","text":"","title":"Window-based change point detection (Window)"},{"location":"user-guide/detection/window/#description","text":"Window-based change point detection is used to perform fast signal segmentation and is implemented in Window . The algorithm uses two windows which slide along the data stream. The statistical properties of the signals within each window are compared with a discrepancy measure. For a given cost function \\(c(\\cdot)\\) , a discrepancy measure is derived \\(d(\\cdot,\\cdot)\\) as follows: \\[ d(y_{u..v}, y_{v..w}) = c(y_{u..w}) - c(y_{u..v}) - c(y_{v..w}) \\] where \\(\\{y_t\\}_t\\) is the input signal and \\(u < v < w\\) are indexes. The discrepancy is the cost gain of splitting the sub-signal \\(y_{u..w}\\) at the index \\(v\\) . If the sliding windows \\(u..v\\) and \\(v..w\\) both fall into a segment, their statistical properties are similar and the discrepancy between the first window and the second window is low. If the sliding windows fall into two dissimilar segments, the discrepancy is significantly higher, suggesting that the boundary between windows is a change point. The discrepancy curve is the curve, defined for all indexes \\(t\\) between \\(w/2\\) and \\(n-w/2\\) ( \\(n\\) is the number of samples), \\[ \\big(t, d(y_{t-w/2..t}, y_{t..t+w/2})\\big) \\] where \\(w\\) is the window length. A sequential peak search is performed on the discrepancy curve in order to detect change points. The benefits of window-based segmentation includes low complexity (of the order of \\(\\mathcal{O}(n w)\\) , where \\(n\\) is the number of samples), the fact that it can extend any single change point detection method to detect multiple changes points and that it can work whether the number of regimes is known beforehand or not. Schematic view of the window sliding segmentation algorithm","title":"Description"},{"location":"user-guide/detection/window/#usage","text":"Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 3 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) To perform a binary segmentation of a signal, initialize a Window instance. # change point detection model = \"l2\" # \"l1\", \"rbf\", \"linear\", \"normal\", \"ar\" algo = rpt . Window ( width = 40 , model = model ) . fit ( signal ) my_bkps = algo . predict ( n_bkps = 3 ) # show results rpt . show . display ( signal , bkps , my_bkps , figsize = ( 10 , 6 )) plt . show () The window length (in number of samples) is modified through the argument width . Usual methods assume that the window length is smaller than the smallest regime length. In the situation in which the number of change points is unknown, one can specify a penalty using the pen parameter or a threshold on the residual norm using epsilon . my_bkps = algo . predict ( pen = np . log ( n ) * dim * sigma ** 2 ) # or my_bkps = algo . predict ( epsilon = 3 * n * sigma ** 2 ) For faster predictions, one can modify the jump parameter during initialization. The higher it is, the faster the prediction is achieved (at the expense of precision). algo = rpt . Window ( model = model , jump = 10 ) . fit ( signal )","title":"Usage"},{"location":"user-guide/metrics/hausdorff/","text":"Hausdorff metric ( hausdorff ) # Description # The hausdorff function computes the Hausdorff metric which measures the worst prediction error. Assume a set of change point indexes \\(t_1,t_2,\\dots\\) and their estimates \\(\\hat{t}_1, \\hat{t}_2,\\dots\\) . The Hausdorff metric is then equal to \\[ \\text{Hausdorff}(\\{t_k\\}_k, \\{\\hat{t}_k\\}_k) := \\max \\{ \\max_k \\min_l |t_k - \\hat{t}_l| \\, , \\max_k \\min_l |\\hat{t}_k - t_l|\\}. \\] Schematic example: true segmentation in gray, estimated segmentation in dashed lines. Here, Hausdorff is equal to \\(\\max(\\Delta t_1, \\Delta t_2, \\Delta t_3)\\) . Usage # Start with the usual imports and create two segmentations to compare. from ruptures.metrics import hausdorff bkps1 , bkps2 = [ 100 , 200 , 500 ], [ 105 , 115 , 350 , 400 , 500 ] print ( hausdorff ( bkps1 , bkps2 ))","title":"Hausdorff metric"},{"location":"user-guide/metrics/hausdorff/#hausdorff-metric-hausdorff","text":"","title":"Hausdorff metric (hausdorff)"},{"location":"user-guide/metrics/hausdorff/#description","text":"The hausdorff function computes the Hausdorff metric which measures the worst prediction error. Assume a set of change point indexes \\(t_1,t_2,\\dots\\) and their estimates \\(\\hat{t}_1, \\hat{t}_2,\\dots\\) . The Hausdorff metric is then equal to \\[ \\text{Hausdorff}(\\{t_k\\}_k, \\{\\hat{t}_k\\}_k) := \\max \\{ \\max_k \\min_l |t_k - \\hat{t}_l| \\, , \\max_k \\min_l |\\hat{t}_k - t_l|\\}. \\] Schematic example: true segmentation in gray, estimated segmentation in dashed lines. Here, Hausdorff is equal to \\(\\max(\\Delta t_1, \\Delta t_2, \\Delta t_3)\\) .","title":"Description"},{"location":"user-guide/metrics/hausdorff/#usage","text":"Start with the usual imports and create two segmentations to compare. from ruptures.metrics import hausdorff bkps1 , bkps2 = [ 100 , 200 , 500 ], [ 105 , 115 , 350 , 400 , 500 ] print ( hausdorff ( bkps1 , bkps2 ))","title":"Usage"},{"location":"user-guide/metrics/precisionrecall/","text":"Precision and recall ( precision_recall ) # Description # The precision and recall of an estimated segmentation is computed by the function precision_recall as follows. A true change point is declared \"detected\" (or positive) if there is at least one computed change point at less than \"margin\" points from it. Formally, assume a set of change point indexes \\(t_1,t_2,\\dots\\) and their estimates \\(\\hat{t}_1, \\hat{t}_2,\\dots\\) In the context of change point detection, precision and recall are defined as follows: \\[ \\text{precision}:=|\\text{TP}|/|\\{\\hat{t}_l\\}_l| \\quad \\text{and}\\quad\\text{recall}:=|\\text{TP}|/|\\{t_k\\}_k| \\] where, for a given margin \\(M\\) , true positives \\(\\text{TP}\\) are true change points for which there is an estimated one at less than \\(M\\) samples, i.e. \\[ \\text{TP}:= \\{t_k\\,|\\, \\exists\\, \\hat{t}_l\\,\\, \\text{s.t.}\\, |\\hat{t}_l - t_k|<M \\}. \\] Schematic example: true segmentation in gray, estimated segmentation in dashed lines and margin in dashed areas. Here, precision is 2/3 and recall is 2/2. Usage # Start with the usual imports and create two change point sets to compare. from ruptures.metrics import precision_recall bkps1 , bkps2 = [ 100 , 200 , 500 ], [ 105 , 115 , 350 , 400 , 500 ] p , r = precision_recall ( bkps1 , bkps2 ) print (( p , r )) The margin parameter \\(M\\) can be changed through the keyword margin (default is 10 samples). p , r = precision_recall ( bkps1 , bkps2 , margin = 10 ) print (( p , r )) p , r = precision_recall ( bkps1 , bkps2 , margin = 20 ) print (( p , r ))","title":"Precision and recall"},{"location":"user-guide/metrics/precisionrecall/#precision-and-recall-precision_recall","text":"","title":"Precision and recall (precision_recall)"},{"location":"user-guide/metrics/precisionrecall/#description","text":"The precision and recall of an estimated segmentation is computed by the function precision_recall as follows. A true change point is declared \"detected\" (or positive) if there is at least one computed change point at less than \"margin\" points from it. Formally, assume a set of change point indexes \\(t_1,t_2,\\dots\\) and their estimates \\(\\hat{t}_1, \\hat{t}_2,\\dots\\) In the context of change point detection, precision and recall are defined as follows: \\[ \\text{precision}:=|\\text{TP}|/|\\{\\hat{t}_l\\}_l| \\quad \\text{and}\\quad\\text{recall}:=|\\text{TP}|/|\\{t_k\\}_k| \\] where, for a given margin \\(M\\) , true positives \\(\\text{TP}\\) are true change points for which there is an estimated one at less than \\(M\\) samples, i.e. \\[ \\text{TP}:= \\{t_k\\,|\\, \\exists\\, \\hat{t}_l\\,\\, \\text{s.t.}\\, |\\hat{t}_l - t_k|<M \\}. \\] Schematic example: true segmentation in gray, estimated segmentation in dashed lines and margin in dashed areas. Here, precision is 2/3 and recall is 2/2.","title":"Description"},{"location":"user-guide/metrics/precisionrecall/#usage","text":"Start with the usual imports and create two change point sets to compare. from ruptures.metrics import precision_recall bkps1 , bkps2 = [ 100 , 200 , 500 ], [ 105 , 115 , 350 , 400 , 500 ] p , r = precision_recall ( bkps1 , bkps2 ) print (( p , r )) The margin parameter \\(M\\) can be changed through the keyword margin (default is 10 samples). p , r = precision_recall ( bkps1 , bkps2 , margin = 10 ) print (( p , r )) p , r = precision_recall ( bkps1 , bkps2 , margin = 20 ) print (( p , r ))","title":"Usage"},{"location":"user-guide/metrics/randindex/","text":"Rand index ( randindex ) # Description # The Rand index ( \\(RI\\) ) measures the similarity between two segmentations and is equal to the proportion of aggreement between two partitions. Formally, for \\(\\mathcal{T}_1\\) and \\(\\mathcal{T}_2\\) two partitions of \\(\\{1, 2,\\dots,T\\}\\) , \\[ RI := \\frac{N_0 + N_1}{T(T+1)/2} \\] where \\(N_0\\) is the number of pairs of samples that belong to the same segment according to \\(\\mathcal{T}_1\\) and \\(\\mathcal{T}_2\\) , \\(N_1\\) is the number of pairs of samples that belong to different segments according to \\(\\mathcal{T}_1\\) and \\(\\mathcal{T}_2\\) . \\(RI\\) is between 0 (total disagreement) and 1 (total agreement). It is available in the randindex function which uses the efficient implementation of [Prates2021] . Usage # Start with the usual imports and create two segmentations to compare. from ruptures.metrics import randindex bkps1 , bkps2 = [ 100 , 200 , 500 ], [ 105 , 115 , 350 , 400 , 500 ] print ( randindex ( bkps1 , bkps2 )) References # [Prates2021] Prates, L. (2021). A more efficient algorithm to compute the Rand Index for change-point problems. ArXiv:2112.03738.","title":"Rand index"},{"location":"user-guide/metrics/randindex/#rand-index-randindex","text":"","title":"Rand index (randindex)"},{"location":"user-guide/metrics/randindex/#description","text":"The Rand index ( \\(RI\\) ) measures the similarity between two segmentations and is equal to the proportion of aggreement between two partitions. Formally, for \\(\\mathcal{T}_1\\) and \\(\\mathcal{T}_2\\) two partitions of \\(\\{1, 2,\\dots,T\\}\\) , \\[ RI := \\frac{N_0 + N_1}{T(T+1)/2} \\] where \\(N_0\\) is the number of pairs of samples that belong to the same segment according to \\(\\mathcal{T}_1\\) and \\(\\mathcal{T}_2\\) , \\(N_1\\) is the number of pairs of samples that belong to different segments according to \\(\\mathcal{T}_1\\) and \\(\\mathcal{T}_2\\) . \\(RI\\) is between 0 (total disagreement) and 1 (total agreement). It is available in the randindex function which uses the efficient implementation of [Prates2021] .","title":"Description"},{"location":"user-guide/metrics/randindex/#usage","text":"Start with the usual imports and create two segmentations to compare. from ruptures.metrics import randindex bkps1 , bkps2 = [ 100 , 200 , 500 ], [ 105 , 115 , 350 , 400 , 500 ] print ( randindex ( bkps1 , bkps2 ))","title":"Usage"},{"location":"user-guide/metrics/randindex/#references","text":"[Prates2021] Prates, L. (2021). A more efficient algorithm to compute the Rand Index for change-point problems. ArXiv:2112.03738.","title":"References"},{"location":"user-guide/show/display/","text":"Display ( display ) # Description # The function display displays a signal and the change points provided in alternating colors. If another set of change point indexes is provided, they are displayed with dashed vertical dashed lines. Usage # Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 2 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) rpt . display ( signal , bkps ) If we computed another set of change points, for instance [110, 150, 320, 500] , we can easily compare the two segmentations. rpt . display ( signal , bkps , [ 110 , 150 , 320 , 500 ]) Example output of the function display .","title":"Display"},{"location":"user-guide/show/display/#display-display","text":"","title":"Display (display)"},{"location":"user-guide/show/display/#description","text":"The function display displays a signal and the change points provided in alternating colors. If another set of change point indexes is provided, they are displayed with dashed vertical dashed lines.","title":"Description"},{"location":"user-guide/show/display/#usage","text":"Start with the usual imports and create a signal. import numpy as np import matplotlib.pylab as plt import ruptures as rpt # creation of data n , dim = 500 , 2 # number of samples, dimension n_bkps , sigma = 3 , 5 # number of change points, noise standart deviation signal , bkps = rpt . pw_constant ( n , dim , n_bkps , noise_std = sigma ) rpt . display ( signal , bkps ) If we computed another set of change points, for instance [110, 150, 320, 500] , we can easily compare the two segmentations. rpt . display ( signal , bkps , [ 110 , 150 , 320 , 500 ]) Example output of the function display .","title":"Usage"}]}