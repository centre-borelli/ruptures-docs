{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ruptures","text":"<p><code>ruptures</code> is a Python library for off-line change point detection. This package provides methods for the analysis and segmentation of non-stationary signals.  Implemented algorithms include exact and approximate detection for various parametric and non-parametric models. <code>ruptures</code> focuses on ease of use by providing a well-documented and consistent interface. In addition, thanks to its modular structure, different algorithms and models can be connected and extended within this package.</p> <p>How to cite. If you use <code>ruptures</code> in a scientific publication, we would appreciate citations to the following paper:</p> <ul> <li>C. Truong, L. Oudre, N. Vayatis. Selective review of offline change point detection methods. Signal Processing, 167:107299, 2020. [journal] [pdf]</li> </ul>"},{"location":"#basic-usage","title":"Basic usage","text":"<p>(Please refer to the documentation for more advanced use.)</p> <p>The following snippet creates a noisy piecewise constant signal, performs a penalized kernel change point detection and displays the results (alternating colors mark true regimes and dashed lines mark estimated change points).</p> <pre><code>import matplotlib.pyplot as plt\nimport ruptures as rpt\n\n# generate signal\nn_samples, dim, sigma = 1000, 3, 4\nn_bkps = 4  # number of breakpoints\nsignal, bkps = rpt.pw_constant(n_samples, dim, n_bkps, noise_std=sigma)\n\n# detection\nalgo = rpt.Pelt(model=\"rbf\").fit(signal)\nresult = algo.predict(pen=10)\n\n# display\nrpt.display(signal, bkps, result)\nplt.show()\n</code></pre> <p></p>"},{"location":"#general-information","title":"General information","text":""},{"location":"#contact","title":"Contact","text":"<p>Concerning this package, its use and bugs, use the issue page of the ruptures repository. For other inquiries, you can contact me here.</p>"},{"location":"#important-links","title":"Important links","text":"<ul> <li>Documentation</li> <li>Pypi package index</li> </ul>"},{"location":"#dependencies-and-install","title":"Dependencies and install","text":"<p>Installation instructions can be found here.</p>"},{"location":"#changelog","title":"Changelog","text":"<p>See the changelog for a history of notable changes to <code>ruptures</code>.</p>"},{"location":"#thanks-to-all-our-contributors","title":"Thanks to all our contributors","text":""},{"location":"#license","title":"License","text":"<p>This project is under BSD license.</p> <pre><code>BSD 2-Clause License\n\nCopyright (c) 2017-2022, ENS Paris-Saclay, CNRS\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#before-contributing","title":"Before contributing","text":"<p>In all following steps, it is highly recommended to use a virtual environment. Build and installation are performed using <code>pip</code> so be sure to have the latest version available.</p> <pre><code>python -m pip install --upgrade pip\n</code></pre>"},{"location":"contributing/#install-the-development-version","title":"Install the development version","text":"<p>It is important that you contribute to the latest version of the code. To that end, start by cloning the Github repository.</p> <pre><code>git clone https://github.com/deepcharles/ruptures\ncd ruptures\n</code></pre> <p>Then install the downloaded package with <code>pip</code>.</p> <pre><code>python -m pip install --editable .[dev]\n</code></pre> <p>Note that <code>python -m</code> can be omitted most of the times, but within virtualenvs, it can prevent certain errors. Also, in certain terminals (such as <code>zsh</code>), the square brackets must be escaped, e.g. replace <code>.[dev]</code> by <code>.\\[dev\\]</code>.</p> <p>In addition to <code>numpy</code>, <code>scipy</code> and <code>ruptures</code>, this command will install all packages needed to develop <code>ruptures</code>. The exact list of librairies can be found in the <code>setup.cfg</code> file (section <code>[options.extras_require]</code>).</p>"},{"location":"contributing/#pre-commit-hooks","title":"Pre-commit hooks","text":"<p>We use <code>pre-commit</code> to run Git hooks before submitting the code to review. These hook scripts perform simple tasks before each commit (code formatting mostly). To activate the hooks, simply run the following command in your terminal.</p> <pre><code>pre-commit install\n</code></pre> <p>If you try to commit a non-compliant (i.e. badly formatted) file, <code>pre-commit</code> will modify this file and make the commit fail. However you need to stage the new changes yourself as <code>pre-commit</code> will not do that for you (this is by design; see here or here). Fortunately, <code>pre-commit</code> outputs useful messages.</p> <p>The list of hooks (and their options) can be found in <code>.pre-commit-config.yaml</code>. For more information, see their website. If you want to manually run all pre-commit hooks on a repository, run <code>pre-commit run --all-files</code>. To run individual hooks use <code>pre-commit run &lt;hook_id&gt;</code>.</p>"},{"location":"contributing/#contribute-to-the-code","title":"Contribute to the code","text":""},{"location":"contributing/#write-tests","title":"Write tests","text":"<p>The following command executes the test suite.</p> <pre><code>python -m pytest\n</code></pre>"},{"location":"contributing/#write-docstrings","title":"Write docstrings","text":""},{"location":"contributing/#contribute-to-the-documentation","title":"Contribute to the documentation","text":"<p>Use MkDocs.</p> <p>Use <code>mkdocs serve</code> to preview your changes. Once you are satisfied, no need to build the documentation, the CI will take care of that and publish it online at the next release of the package (if the pull request has been merged).</p>"},{"location":"contributing/#add-examples-to-the-gallery","title":"Add examples to the gallery","text":"<p>An easy way to showcase your work with <code>ruptures</code> is to write a narrative example. To that, simply put a Jupyter notebook in the <code>docs/examples</code> folder. To make it appear in the documentation, add a reference in <code>mkdocs.yml</code> (<code>nav &gt; Gallery of examples</code>): if the notebook's name is <code>my_notebook.ipynb</code>, it will be available as <code>examples/my_notebook.ipynb</code>. It will be rendered automatically when MkDocs builds the documentation.</p> <p>Note</p> <p>To automatically add a Binder link and a download link to your notebook, simply add the following line of code. <pre><code>&lt;!-- {{ add_binder_block(page) }} --&gt;\n</code></pre> Ideally, place this code below the title of the notebook (same cell) and it will be rendered as in here.</p> <p>We welcome any interesting work about a new cost function, algorithm, data, calibration method, etc. Any other package can be used in combination with <code>ruptures</code>. However, each example should be clearly explained with text and figures. The amount of raw code should also remain limited for readability.</p>"},{"location":"contributing/#miscellaneous","title":"Miscellaneous","text":""},{"location":"contributing/#naming-convention","title":"Naming convention","text":"<p>We try to follow (roughly) a consistent naming convention of modules, classes, functions, etc. When in doubt, you can refer to the PEP 8 style guide for Python code.</p>"},{"location":"custom-cost-function/","title":"Creating a custom cost function","text":"<p>In order to define custom cost functions, simply create a class that inherits from <code>ruptures.base.BaseCost</code> and implement the methods <code>.fit(signal)</code> and <code>.error(start, end)</code>:</p> <ul> <li>The method <code>.fit(signal)</code> takes a signal as input and sets parameters. It returns <code>'self'</code>.</li> <li>The method <code>.error(start, end)</code> takes two indexes <code>'start'</code> and <code>'end'</code>  and returns the cost on the segment start:end.</li> </ul> <p>Example</p> <p>See this custom cost example.</p>"},{"location":"fit-and-predict/","title":"Fitting and prediction: estimator basics","text":"<p><code>ruptures</code> has an object-oriented modelling approach (largely inspired by scikit-learn): change point detection algorithms are broken down into two conceptual objects that inherits from base classes: <code>BaseEstimator</code> and <code>BaseCost</code>.</p>"},{"location":"fit-and-predict/#initializing-a-new-estimator","title":"Initializing a new estimator","text":"<p>Each change point detection algorithm inherits from the base class <code>ruptures.base.BaseEstimator</code>. When a class that inherits from the base estimator is created, the <code>.__init__()</code> method initializes an estimator with the following arguments:</p> <ul> <li><code>model</code>: \"l1\", \"l2\", \"normal\", \"rbf\", \"linear\", etc. Cost function to use to compute the approximation error.</li> <li><code>cost</code>: a custom cost function to the detection algorithm. Should be a <code>BaseCost</code> instance.</li> <li><code>jump</code>: reduce the set of possible change point indexes; predicted change points can only be a multiple of <code>jump</code>.</li> <li><code>min_size</code>: minimum number of samples between two change points.</li> </ul>"},{"location":"fit-and-predict/#making-a-prediction","title":"Making a prediction","text":"<p>The main methods are <code>.fit()</code>, <code>.predict()</code>, <code>.fit_predict()</code>:</p> <ul> <li><code>.fit()</code>: generally takes a signal as input and fit the algorithm to the data.</li> <li><code>.predict()</code>: performs the change point detection. This method returns a list of indexes corresponding to the end of each regimes. By design, the last element of this list is the number of samples.</li> <li><code>.fit_predict()</code>: helper method which calls <code>.fit()</code> and <code>.predict()</code> successively.</li> </ul>"},{"location":"install/","title":"Installation","text":"<p>This library requires Python &gt;=3.6 and the following packages: <code>numpy</code>, <code>scipy</code> and <code>matplotlib</code> (the last one is optional and only for display purposes). You can either install the latest stable release or the development version.</p>"},{"location":"install/#stable-release","title":"Stable release","text":"<p>To install the latest stable release, use <code>pip</code> or <code>conda</code>.</p> With pip <pre><code>python -m pip install ruptures\n</code></pre> With conda <p><code>ruptures</code> can be installed from the <code>conda-forge</code> channel (run <code>conda config --add channels conda-forge</code> to add it): <pre><code>conda install ruptures\n</code></pre></p>"},{"location":"install/#development-release","title":"Development release","text":"<p>Alternatively, you can install the development version of <code>ruptures</code> which can contain features that have not yet been integrated to the stable release. To that end, refer to the contributing guide.</p>"},{"location":"install/#upgrade","title":"Upgrade","text":"<p>Show the current version of the package.</p> <pre><code>python -m pip show ruptures\n</code></pre> <p>In order to upgrade to the version, use the following command.</p> <pre><code>python -m pip install -U ruptures\n</code></pre>"},{"location":"license/","title":"License","text":"<p>This project is under BSD license.</p> <pre><code>BSD 2-Clause License\n\nCopyright (c) 2017-2021, ENS Paris-Saclay, CNRS\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n</code></pre>"},{"location":"release-notes/","title":"Changelog","text":"<p>The latest release notes are available directly in Github: ruptures/releases.</p> <p>Earlier releases are documented below \u2b07\ufe0f.</p>"},{"location":"release-notes/#112-2020-12-01","title":"1.1.2 - 2020-12-01","text":""},{"location":"release-notes/#added","title":"Added","text":"<ul> <li>12cbc9e feat: add piecewise linear cpd (#91)</li> <li>a12b215 test: add code coverage badge (#97)</li> <li>2e9b17f docs: add binder for notebooks (#94)</li> <li>da7544f docs(costcosine): add entry for CostCosine in docs (#93)</li> <li>8c9aa35 build(setup.py/cfg):  add build_ext to setup.py (#88)</li> <li>10ef8e8 build(python39): add py39 to supported versions (#87)</li> </ul>"},{"location":"release-notes/#changed","title":"Changed","text":"<ul> <li>069bd41 fix(kernelcpd): bug fix in pelt (#95)</li> <li>b4abc34 fix: memory leak in KernelCPD (#89)</li> </ul>"},{"location":"release-notes/#111-2020-11-26","title":"1.1.1 - 2020-11-26","text":"<p>No change to the code compared to the previous version. The package was only partly published to Pypi because of the failure of one provider in the CI. Since Pypi's policy prevents re-uploading twice the same version, we have to increment the version number.</p>"},{"location":"release-notes/#110-2020-11-23","title":"1.1.0 - 2020-11-23","text":""},{"location":"release-notes/#added_1","title":"Added","text":"<ul> <li>modify publishing process to Pypi PR#83</li> <li>add cosine kernel (cost function and in KernelCPD)PR#74</li> <li>add faster kernel change point detection (<code>KernelCPD</code>, C implementation) PR#74</li> <li>add manual trigger to publish to Pypi PR#72</li> </ul>"},{"location":"release-notes/#changed_1","title":"Changed","text":""},{"location":"release-notes/#106-2020-10-23","title":"1.0.6 - 2020-10-23","text":""},{"location":"release-notes/#added_2","title":"Added","text":"<ul> <li>Correct minor error in Dynp (about min_size) PR#74</li> <li>Fix legacy formatting errors PR#69</li> <li>New documentation (from Sphinx to Mkdocs) PR#64</li> <li>Separate requirements.txt and requirements-dev.txt PR#64</li> <li>A changelog file (link)</li> <li>New Github actions for automatic generation of documentation</li> <li>Pre-commit code formatting using black</li> </ul>"},{"location":"release-notes/#changed_2","title":"Changed","text":"<ul> <li>Correction of display function test #64</li> <li>Add badges in the README (Github repo) PR#62: pypi version, python version, code style, contributor list</li> <li>Typo in documentation (PR#60) by @gjaeger</li> <li>Documentation theme</li> <li>Documentation site</li> </ul>"},{"location":"release-notes/#105-2020-07-22","title":"1.0.5 - 2020-07-22","text":""},{"location":"release-notes/#changed_3","title":"Changed","text":"<ul> <li>Link to documentation in PyPi description</li> </ul>"},{"location":"what-is-cpd/","title":"Getting started","text":""},{"location":"what-is-cpd/#what-is-change-point-detection","title":"What is change point detection?","text":"<p>Under construction. In the meantime, you can refer to the associated review of methods [Truong2020].</p>"},{"location":"what-is-cpd/#references","title":"References","text":"<p>[Truong2020] Truong, C., Oudre, L., &amp; Vayatis, N. (2020). Selective review of offline change point detection methods. Signal Processing, 167. [abstract] [doi] [pdf]</p>"},{"location":"code-reference/","title":"Introduction","text":"<p>This section describes the API of all functions and classes in the <code>ruptures</code> package. For a more intuitive description of each method, please refer to the User guide.</p> <p>Roughly, each module corresponds to a certain type of procedure:</p> <ul> <li><code>ruptures.base</code>: base classes;</li> <li><code>ruptures.detection</code>: search methods;</li> <li><code>ruptures.costs</code>: costs functions;</li> <li><code>ruptures.datasets</code>: data set generating utilities;</li> <li><code>ruptures.metrics</code>: evaluation metrics;</li> <li><code>ruptures.show</code>: display functions.</li> </ul>"},{"location":"code-reference/base-reference/","title":"Base classes (ruptures.base)","text":"<p>All estimators and cost functions are subclasses of.</p> <p><code>BaseEstimator</code> and <code>BaseCost</code> respectively.</p>"},{"location":"code-reference/base-reference/#ruptures.base.BaseCost","title":"<code>BaseCost</code>","text":"<p>             Bases: <code>object</code></p> <p>Base class for all segment cost classes.</p> Notes <p>All classes should specify all the parameters that can be set at the class level in their <code>__init__</code> as explicit keyword arguments (no <code>*args</code> or <code>**kwargs</code>).</p> Source code in <code>ruptures/base.py</code> <pre><code>class BaseCost(object, metaclass=abc.ABCMeta):\n    \"\"\"Base class for all segment cost classes.\n\n    Notes:\n        All classes should specify all the parameters that can be set\n        at the class level in their ``__init__`` as explicit keyword\n        arguments (no ``*args`` or ``**kwargs``).\n    \"\"\"\n\n    @abc.abstractmethod\n    def fit(self, *args, **kwargs):\n        \"\"\"Set the parameters of the cost function, for instance the Gram\n        matrix, etc.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def error(self, start, end):\n        \"\"\"Returns the cost on segment [start:end].\"\"\"\n        pass\n\n    def sum_of_costs(self, bkps):\n        \"\"\"Returns the sum of segments cost for the given segmentation.\n\n        Args:\n            bkps (list): list of change points. By convention, bkps[-1]==n_samples.\n\n        Returns:\n            float: sum of costs\n        \"\"\"\n        soc = sum(self.error(start, end) for start, end in pairwise([0] + bkps))\n        return soc\n\n    @property\n    @abc.abstractmethod\n    def model(self):\n        pass\n</code></pre>"},{"location":"code-reference/base-reference/#ruptures.base.BaseCost.error","title":"<code>error(start, end)</code>  <code>abstractmethod</code>","text":"<p>Returns the cost on segment [start:end].</p> Source code in <code>ruptures/base.py</code> <pre><code>@abc.abstractmethod\ndef error(self, start, end):\n    \"\"\"Returns the cost on segment [start:end].\"\"\"\n    pass\n</code></pre>"},{"location":"code-reference/base-reference/#ruptures.base.BaseCost.fit","title":"<code>fit(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Set the parameters of the cost function, for instance the Gram matrix, etc.</p> Source code in <code>ruptures/base.py</code> <pre><code>@abc.abstractmethod\ndef fit(self, *args, **kwargs):\n    \"\"\"Set the parameters of the cost function, for instance the Gram\n    matrix, etc.\"\"\"\n    pass\n</code></pre>"},{"location":"code-reference/base-reference/#ruptures.base.BaseCost.sum_of_costs","title":"<code>sum_of_costs(bkps)</code>","text":"<p>Returns the sum of segments cost for the given segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>bkps</code> <code>list</code> <p>list of change points. By convention, bkps[-1]==n_samples.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>sum of costs</p> Source code in <code>ruptures/base.py</code> <pre><code>def sum_of_costs(self, bkps):\n    \"\"\"Returns the sum of segments cost for the given segmentation.\n\n    Args:\n        bkps (list): list of change points. By convention, bkps[-1]==n_samples.\n\n    Returns:\n        float: sum of costs\n    \"\"\"\n    soc = sum(self.error(start, end) for start, end in pairwise([0] + bkps))\n    return soc\n</code></pre>"},{"location":"code-reference/base-reference/#ruptures.base.BaseEstimator","title":"<code>BaseEstimator</code>","text":"<p>Base class for all change point detection estimators.</p> Notes <p>All estimators should specify all the parameters that can be set at the class level in their <code>__init__</code> as explicit keyword arguments (no <code>*args</code> or <code>**kwargs</code>).</p> Source code in <code>ruptures/base.py</code> <pre><code>class BaseEstimator(metaclass=abc.ABCMeta):\n    \"\"\"Base class for all change point detection estimators.\n\n    Notes:\n        All estimators should specify all the parameters that can be set\n        at the class level in their ``__init__`` as explicit keyword\n        arguments (no ``*args`` or ``**kwargs``).\n    \"\"\"\n\n    @abc.abstractmethod\n    def fit(self, *args, **kwargs):\n        \"\"\"To call the segmentation algorithm.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def predict(self, *args, **kwargs):\n        \"\"\"To call the segmentation algorithm.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def fit_predict(self, *args, **kwargs):\n        \"\"\"To call the segmentation algorithm.\"\"\"\n        pass\n</code></pre>"},{"location":"code-reference/base-reference/#ruptures.base.BaseEstimator.fit","title":"<code>fit(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>To call the segmentation algorithm.</p> Source code in <code>ruptures/base.py</code> <pre><code>@abc.abstractmethod\ndef fit(self, *args, **kwargs):\n    \"\"\"To call the segmentation algorithm.\"\"\"\n    pass\n</code></pre>"},{"location":"code-reference/base-reference/#ruptures.base.BaseEstimator.fit_predict","title":"<code>fit_predict(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>To call the segmentation algorithm.</p> Source code in <code>ruptures/base.py</code> <pre><code>@abc.abstractmethod\ndef fit_predict(self, *args, **kwargs):\n    \"\"\"To call the segmentation algorithm.\"\"\"\n    pass\n</code></pre>"},{"location":"code-reference/base-reference/#ruptures.base.BaseEstimator.predict","title":"<code>predict(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>To call the segmentation algorithm.</p> Source code in <code>ruptures/base.py</code> <pre><code>@abc.abstractmethod\ndef predict(self, *args, **kwargs):\n    \"\"\"To call the segmentation algorithm.\"\"\"\n    pass\n</code></pre>"},{"location":"code-reference/costs/costautoregressive-reference/","title":"Autoregressive model change (CostAutoregressive)","text":""},{"location":"code-reference/costs/costautoregressive-reference/#ruptures.costs.costautoregressive.CostAR","title":"<code>CostAR</code>","text":"<p>             Bases: <code>BaseCost</code></p> <p>Least-squares estimate for changes in autoregressive coefficients.</p> Source code in <code>ruptures/costs/costautoregressive.py</code> <pre><code>class CostAR(BaseCost):\n    r\"\"\"Least-squares estimate for changes in autoregressive coefficients.\"\"\"\n\n    model = \"ar\"\n\n    def __init__(self, order=4):\n        \"\"\"Initialize the object.\n\n        Args:\n            order (int): autoregressive order\n        \"\"\"\n        self.signal = None\n        self.covar = None\n        self.min_size = max(5, order + 1)\n        self.order = order\n\n    def fit(self, signal):\n        \"\"\"Set parameters of the instance. The signal must be 1D.\n\n        Args:\n            signal (array): 1d signal. Shape (n_samples, 1) or (n_samples,).\n\n        Returns:\n            self: the current object\n        \"\"\"\n        self.signal = deepcopy(signal)\n        if signal.ndim == 1:\n            self.signal = self.signal.reshape(-1, 1)\n\n        # lagged covariates\n        n_samples, _ = self.signal.shape\n        strides = (self.signal.itemsize, self.signal.itemsize)\n        shape = (n_samples - self.order, self.order)\n        lagged = as_strided(self.signal, shape=shape, strides=strides)\n        # pad the first columns\n        lagged_after_padding = np.pad(lagged, ((self.order, 0), (0, 0)), mode=\"edge\")\n        # add intercept\n        self.covar = np.c_[lagged_after_padding, np.ones(n_samples)]\n        # pad signal on the edges\n        self.signal[: self.order] = self.signal[self.order]\n        return self\n\n    def error(self, start, end):\n        \"\"\"Return the approximation cost on the segment [start:end].\n\n        Args:\n            start (int): start of the segment\n            end (int): end of the segment\n\n        Returns:\n            float: segment cost\n\n        Raises:\n            NotEnoughPoints: when the segment is too short (less than ``'min_size'`` samples).\n        \"\"\"\n        if end - start &lt; self.min_size:\n            raise NotEnoughPoints\n        y, X = self.signal[start:end], self.covar[start:end]\n        _, residual, _, _ = lstsq(X, y, rcond=None)\n        return residual.sum()\n</code></pre>"},{"location":"code-reference/costs/costautoregressive-reference/#ruptures.costs.costautoregressive.CostAR.__init__","title":"<code>__init__(order=4)</code>","text":"<p>Initialize the object.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>int</code> <p>autoregressive order</p> <code>4</code> Source code in <code>ruptures/costs/costautoregressive.py</code> <pre><code>def __init__(self, order=4):\n    \"\"\"Initialize the object.\n\n    Args:\n        order (int): autoregressive order\n    \"\"\"\n    self.signal = None\n    self.covar = None\n    self.min_size = max(5, order + 1)\n    self.order = order\n</code></pre>"},{"location":"code-reference/costs/costautoregressive-reference/#ruptures.costs.costautoregressive.CostAR.error","title":"<code>error(start, end)</code>","text":"<p>Return the approximation cost on the segment [start:end].</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>start of the segment</p> required <code>end</code> <code>int</code> <p>end of the segment</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>segment cost</p> <p>Raises:</p> Type Description <code>NotEnoughPoints</code> <p>when the segment is too short (less than <code>'min_size'</code> samples).</p> Source code in <code>ruptures/costs/costautoregressive.py</code> <pre><code>def error(self, start, end):\n    \"\"\"Return the approximation cost on the segment [start:end].\n\n    Args:\n        start (int): start of the segment\n        end (int): end of the segment\n\n    Returns:\n        float: segment cost\n\n    Raises:\n        NotEnoughPoints: when the segment is too short (less than ``'min_size'`` samples).\n    \"\"\"\n    if end - start &lt; self.min_size:\n        raise NotEnoughPoints\n    y, X = self.signal[start:end], self.covar[start:end]\n    _, residual, _, _ = lstsq(X, y, rcond=None)\n    return residual.sum()\n</code></pre>"},{"location":"code-reference/costs/costautoregressive-reference/#ruptures.costs.costautoregressive.CostAR.fit","title":"<code>fit(signal)</code>","text":"<p>Set parameters of the instance. The signal must be 1D.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>1d signal. Shape (n_samples, 1) or (n_samples,).</p> required <p>Returns:</p> Name Type Description <code>self</code> <p>the current object</p> Source code in <code>ruptures/costs/costautoregressive.py</code> <pre><code>def fit(self, signal):\n    \"\"\"Set parameters of the instance. The signal must be 1D.\n\n    Args:\n        signal (array): 1d signal. Shape (n_samples, 1) or (n_samples,).\n\n    Returns:\n        self: the current object\n    \"\"\"\n    self.signal = deepcopy(signal)\n    if signal.ndim == 1:\n        self.signal = self.signal.reshape(-1, 1)\n\n    # lagged covariates\n    n_samples, _ = self.signal.shape\n    strides = (self.signal.itemsize, self.signal.itemsize)\n    shape = (n_samples - self.order, self.order)\n    lagged = as_strided(self.signal, shape=shape, strides=strides)\n    # pad the first columns\n    lagged_after_padding = np.pad(lagged, ((self.order, 0), (0, 0)), mode=\"edge\")\n    # add intercept\n    self.covar = np.c_[lagged_after_padding, np.ones(n_samples)]\n    # pad signal on the edges\n    self.signal[: self.order] = self.signal[self.order]\n    return self\n</code></pre>"},{"location":"code-reference/costs/costclinear-reference/","title":"Continuous linear change (CostCLinear)","text":"<p>             Bases: <code>BaseCost</code></p> <p>Piecewise linear approximation with a continuity constraint.</p> Source code in <code>ruptures/costs/costclinear.py</code> <pre><code>class CostCLinear(BaseCost):\n    r\"\"\"Piecewise linear approximation with a continuity constraint.\"\"\"\n\n    model = \"clinear\"\n\n    def __init__(self):\n        \"\"\"Initialize the object.\"\"\"\n        self.signal = None\n        self.min_size = 3\n\n    def fit(self, signal) -&gt; \"CostCLinear\":\n        \"\"\"Set parameters of the instance.\n\n        Args:\n            signal (array): signal of shape (n_samples, n_dims) or (n_samples,)\n\n        Returns:\n            self\n        \"\"\"\n        if signal.ndim == 1:\n            self.signal = signal.reshape(-1, 1)\n        else:\n            self.signal = signal\n\n        return self\n\n    def error(self, start, end) -&gt; float:\n        \"\"\"Return the approximation cost on the segment [start:end].\n\n        Args:\n            start (int): start of the segment\n            end (int): end of the segment\n\n        Returns:\n            segment cost (float)\n\n        Raises:\n            NotEnoughPoints: when the segment is too short (less than `min_size`\n                samples).\n        \"\"\"\n        if end - start &lt; self.min_size:\n            raise NotEnoughPoints\n\n        if start == 0:\n            start = 1\n\n        sub = self.signal[start:end]\n        slope = (self.signal[end - 1] - self.signal[start - 1]) / (end - start)\n        intercept = self.signal[start - 1]\n        approx = slope.reshape(-1, 1) * np.arange(\n            1, end - start + 1\n        ) + intercept.reshape(-1, 1)\n        return np.sum((sub - approx.transpose()) ** 2)\n</code></pre>"},{"location":"code-reference/costs/costclinear-reference/#ruptures.costs.costclinear.CostCLinear.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the object.</p> Source code in <code>ruptures/costs/costclinear.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the object.\"\"\"\n    self.signal = None\n    self.min_size = 3\n</code></pre>"},{"location":"code-reference/costs/costclinear-reference/#ruptures.costs.costclinear.CostCLinear.error","title":"<code>error(start, end)</code>","text":"<p>Return the approximation cost on the segment [start:end].</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>start of the segment</p> required <code>end</code> <code>int</code> <p>end of the segment</p> required <p>Returns:</p> Type Description <code>float</code> <p>segment cost (float)</p> <p>Raises:</p> Type Description <code>NotEnoughPoints</code> <p>when the segment is too short (less than <code>min_size</code> samples).</p> Source code in <code>ruptures/costs/costclinear.py</code> <pre><code>def error(self, start, end) -&gt; float:\n    \"\"\"Return the approximation cost on the segment [start:end].\n\n    Args:\n        start (int): start of the segment\n        end (int): end of the segment\n\n    Returns:\n        segment cost (float)\n\n    Raises:\n        NotEnoughPoints: when the segment is too short (less than `min_size`\n            samples).\n    \"\"\"\n    if end - start &lt; self.min_size:\n        raise NotEnoughPoints\n\n    if start == 0:\n        start = 1\n\n    sub = self.signal[start:end]\n    slope = (self.signal[end - 1] - self.signal[start - 1]) / (end - start)\n    intercept = self.signal[start - 1]\n    approx = slope.reshape(-1, 1) * np.arange(\n        1, end - start + 1\n    ) + intercept.reshape(-1, 1)\n    return np.sum((sub - approx.transpose()) ** 2)\n</code></pre>"},{"location":"code-reference/costs/costclinear-reference/#ruptures.costs.costclinear.CostCLinear.fit","title":"<code>fit(signal)</code>","text":"<p>Set parameters of the instance.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>signal of shape (n_samples, n_dims) or (n_samples,)</p> required <p>Returns:</p> Type Description <code>CostCLinear</code> <p>self</p> Source code in <code>ruptures/costs/costclinear.py</code> <pre><code>def fit(self, signal) -&gt; \"CostCLinear\":\n    \"\"\"Set parameters of the instance.\n\n    Args:\n        signal (array): signal of shape (n_samples, n_dims) or (n_samples,)\n\n    Returns:\n        self\n    \"\"\"\n    if signal.ndim == 1:\n        self.signal = signal.reshape(-1, 1)\n    else:\n        self.signal = signal\n\n    return self\n</code></pre>"},{"location":"code-reference/costs/costcosine-reference/","title":"Kernelized mean change (CostCosine)","text":"<p>             Bases: <code>BaseCost</code></p> <p>Kernel change point detection with the cosine similarity.</p> Source code in <code>ruptures/costs/costcosine.py</code> <pre><code>class CostCosine(BaseCost):\n    r\"\"\"Kernel change point detection with the cosine similarity.\"\"\"\n\n    model = \"cosine\"\n\n    def __init__(self):\n        \"\"\"Initialize the object.\"\"\"\n        self.signal = None\n        self.min_size = 1\n        self._gram = None\n\n    @property\n    def gram(self):\n        \"\"\"Generate the gram matrix (lazy loading).\n\n        Only access this function after a `.fit()` (otherwise\n        `self.signal` is not defined).\n        \"\"\"\n        if self._gram is None:\n            self._gram = squareform(1 - pdist(self.signal, metric=\"cosine\"))\n        return self._gram\n\n    def fit(self, signal) -&gt; \"CostCosine\":\n        \"\"\"Set parameters of the instance.\n\n        Args:\n            signal (array): array of shape (n_samples,) or (n_samples, n_features)\n\n        Returns:\n            self\n        \"\"\"\n        if signal.ndim == 1:\n            self.signal = signal.reshape(-1, 1)\n        else:\n            self.signal = signal\n        return self\n\n    def error(self, start, end) -&gt; float:\n        \"\"\"Return the approximation cost on the segment [start:end].\n\n        Args:\n            start (int): start of the segment\n            end (int): end of the segment\n\n        Returns:\n            segment cost\n\n        Raises:\n            NotEnoughPoints: when the segment is too short (less than `min_size` samples).\n        \"\"\"\n        if end - start &lt; self.min_size:\n            raise NotEnoughPoints\n        sub_gram = self.gram[start:end, start:end]\n        val = np.diagonal(sub_gram).sum()\n        val -= sub_gram.sum() / (end - start)\n        return val\n</code></pre>"},{"location":"code-reference/costs/costcosine-reference/#ruptures.costs.costcosine.CostCosine.gram","title":"<code>gram</code>  <code>property</code>","text":"<p>Generate the gram matrix (lazy loading).</p> <p>Only access this function after a <code>.fit()</code> (otherwise <code>self.signal</code> is not defined).</p>"},{"location":"code-reference/costs/costcosine-reference/#ruptures.costs.costcosine.CostCosine.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the object.</p> Source code in <code>ruptures/costs/costcosine.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the object.\"\"\"\n    self.signal = None\n    self.min_size = 1\n    self._gram = None\n</code></pre>"},{"location":"code-reference/costs/costcosine-reference/#ruptures.costs.costcosine.CostCosine.error","title":"<code>error(start, end)</code>","text":"<p>Return the approximation cost on the segment [start:end].</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>start of the segment</p> required <code>end</code> <code>int</code> <p>end of the segment</p> required <p>Returns:</p> Type Description <code>float</code> <p>segment cost</p> <p>Raises:</p> Type Description <code>NotEnoughPoints</code> <p>when the segment is too short (less than <code>min_size</code> samples).</p> Source code in <code>ruptures/costs/costcosine.py</code> <pre><code>def error(self, start, end) -&gt; float:\n    \"\"\"Return the approximation cost on the segment [start:end].\n\n    Args:\n        start (int): start of the segment\n        end (int): end of the segment\n\n    Returns:\n        segment cost\n\n    Raises:\n        NotEnoughPoints: when the segment is too short (less than `min_size` samples).\n    \"\"\"\n    if end - start &lt; self.min_size:\n        raise NotEnoughPoints\n    sub_gram = self.gram[start:end, start:end]\n    val = np.diagonal(sub_gram).sum()\n    val -= sub_gram.sum() / (end - start)\n    return val\n</code></pre>"},{"location":"code-reference/costs/costcosine-reference/#ruptures.costs.costcosine.CostCosine.fit","title":"<code>fit(signal)</code>","text":"<p>Set parameters of the instance.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>array of shape (n_samples,) or (n_samples, n_features)</p> required <p>Returns:</p> Type Description <code>CostCosine</code> <p>self</p> Source code in <code>ruptures/costs/costcosine.py</code> <pre><code>def fit(self, signal) -&gt; \"CostCosine\":\n    \"\"\"Set parameters of the instance.\n\n    Args:\n        signal (array): array of shape (n_samples,) or (n_samples, n_features)\n\n    Returns:\n        self\n    \"\"\"\n    if signal.ndim == 1:\n        self.signal = signal.reshape(-1, 1)\n    else:\n        self.signal = signal\n    return self\n</code></pre>"},{"location":"code-reference/costs/costl1-reference/","title":"CostL1 (least absolute deviation)","text":"<p>             Bases: <code>BaseCost</code></p> <p>Least absolute deviation.</p> Source code in <code>ruptures/costs/costl1.py</code> <pre><code>class CostL1(BaseCost):\n    r\"\"\"Least absolute deviation.\"\"\"\n\n    model = \"l1\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the object.\"\"\"\n        self.signal = None\n        self.min_size = 2\n\n    def fit(self, signal) -&gt; \"CostL1\":\n        \"\"\"Set parameters of the instance.\n\n        Args:\n            signal (array): signal. Shape (n_samples,) or (n_samples, n_features)\n\n        Returns:\n            self\n        \"\"\"\n        if signal.ndim == 1:\n            self.signal = signal.reshape(-1, 1)\n        else:\n            self.signal = signal\n\n        return self\n\n    def error(self, start, end) -&gt; float:\n        \"\"\"Return the approximation cost on the segment [start:end].\n\n        Args:\n            start (int): start of the segment\n            end (int): end of the segment\n\n        Returns:\n            segment cost\n\n        Raises:\n            NotEnoughPoints: when the segment is too short (less than `min_size` samples).\n        \"\"\"\n        if end - start &lt; self.min_size:\n            raise NotEnoughPoints\n        sub = self.signal[start:end]\n        med = np.median(sub, axis=0)\n\n        return abs(sub - med).sum()\n</code></pre>"},{"location":"code-reference/costs/costl1-reference/#ruptures.costs.costl1.CostL1.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the object.</p> Source code in <code>ruptures/costs/costl1.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the object.\"\"\"\n    self.signal = None\n    self.min_size = 2\n</code></pre>"},{"location":"code-reference/costs/costl1-reference/#ruptures.costs.costl1.CostL1.error","title":"<code>error(start, end)</code>","text":"<p>Return the approximation cost on the segment [start:end].</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>start of the segment</p> required <code>end</code> <code>int</code> <p>end of the segment</p> required <p>Returns:</p> Type Description <code>float</code> <p>segment cost</p> <p>Raises:</p> Type Description <code>NotEnoughPoints</code> <p>when the segment is too short (less than <code>min_size</code> samples).</p> Source code in <code>ruptures/costs/costl1.py</code> <pre><code>def error(self, start, end) -&gt; float:\n    \"\"\"Return the approximation cost on the segment [start:end].\n\n    Args:\n        start (int): start of the segment\n        end (int): end of the segment\n\n    Returns:\n        segment cost\n\n    Raises:\n        NotEnoughPoints: when the segment is too short (less than `min_size` samples).\n    \"\"\"\n    if end - start &lt; self.min_size:\n        raise NotEnoughPoints\n    sub = self.signal[start:end]\n    med = np.median(sub, axis=0)\n\n    return abs(sub - med).sum()\n</code></pre>"},{"location":"code-reference/costs/costl1-reference/#ruptures.costs.costl1.CostL1.fit","title":"<code>fit(signal)</code>","text":"<p>Set parameters of the instance.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>signal. Shape (n_samples,) or (n_samples, n_features)</p> required <p>Returns:</p> Type Description <code>CostL1</code> <p>self</p> Source code in <code>ruptures/costs/costl1.py</code> <pre><code>def fit(self, signal) -&gt; \"CostL1\":\n    \"\"\"Set parameters of the instance.\n\n    Args:\n        signal (array): signal. Shape (n_samples,) or (n_samples, n_features)\n\n    Returns:\n        self\n    \"\"\"\n    if signal.ndim == 1:\n        self.signal = signal.reshape(-1, 1)\n    else:\n        self.signal = signal\n\n    return self\n</code></pre>"},{"location":"code-reference/costs/costl2-reference/","title":"CostL2 (least squared deviation)","text":"<p>             Bases: <code>BaseCost</code></p> <p>Least squared deviation.</p> Source code in <code>ruptures/costs/costl2.py</code> <pre><code>class CostL2(BaseCost):\n    r\"\"\"Least squared deviation.\"\"\"\n\n    model = \"l2\"\n\n    def __init__(self):\n        \"\"\"Initialize the object.\"\"\"\n        self.signal = None\n        self.min_size = 1\n\n    def fit(self, signal) -&gt; \"CostL2\":\n        \"\"\"Set parameters of the instance.\n\n        Args:\n            signal (array): array of shape (n_samples,) or (n_samples, n_features)\n\n        Returns:\n            self\n        \"\"\"\n        if signal.ndim == 1:\n            self.signal = signal.reshape(-1, 1)\n        else:\n            self.signal = signal\n\n        return self\n\n    def error(self, start, end) -&gt; float:\n        \"\"\"Return the approximation cost on the segment [start:end].\n\n        Args:\n            start (int): start of the segment\n            end (int): end of the segment\n\n        Returns:\n            segment cost\n\n        Raises:\n            NotEnoughPoints: when the segment is too short (less than `min_size` samples).\n        \"\"\"\n        if end - start &lt; self.min_size:\n            raise NotEnoughPoints\n\n        return self.signal[start:end].var(axis=0).sum() * (end - start)\n</code></pre>"},{"location":"code-reference/costs/costl2-reference/#ruptures.costs.costl2.CostL2.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the object.</p> Source code in <code>ruptures/costs/costl2.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the object.\"\"\"\n    self.signal = None\n    self.min_size = 1\n</code></pre>"},{"location":"code-reference/costs/costl2-reference/#ruptures.costs.costl2.CostL2.error","title":"<code>error(start, end)</code>","text":"<p>Return the approximation cost on the segment [start:end].</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>start of the segment</p> required <code>end</code> <code>int</code> <p>end of the segment</p> required <p>Returns:</p> Type Description <code>float</code> <p>segment cost</p> <p>Raises:</p> Type Description <code>NotEnoughPoints</code> <p>when the segment is too short (less than <code>min_size</code> samples).</p> Source code in <code>ruptures/costs/costl2.py</code> <pre><code>def error(self, start, end) -&gt; float:\n    \"\"\"Return the approximation cost on the segment [start:end].\n\n    Args:\n        start (int): start of the segment\n        end (int): end of the segment\n\n    Returns:\n        segment cost\n\n    Raises:\n        NotEnoughPoints: when the segment is too short (less than `min_size` samples).\n    \"\"\"\n    if end - start &lt; self.min_size:\n        raise NotEnoughPoints\n\n    return self.signal[start:end].var(axis=0).sum() * (end - start)\n</code></pre>"},{"location":"code-reference/costs/costl2-reference/#ruptures.costs.costl2.CostL2.fit","title":"<code>fit(signal)</code>","text":"<p>Set parameters of the instance.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>array of shape (n_samples,) or (n_samples, n_features)</p> required <p>Returns:</p> Type Description <code>CostL2</code> <p>self</p> Source code in <code>ruptures/costs/costl2.py</code> <pre><code>def fit(self, signal) -&gt; \"CostL2\":\n    \"\"\"Set parameters of the instance.\n\n    Args:\n        signal (array): array of shape (n_samples,) or (n_samples, n_features)\n\n    Returns:\n        self\n    \"\"\"\n    if signal.ndim == 1:\n        self.signal = signal.reshape(-1, 1)\n    else:\n        self.signal = signal\n\n    return self\n</code></pre>"},{"location":"code-reference/costs/costlinear-reference/","title":"Linear model change (CostLinear)","text":"<p>             Bases: <code>BaseCost</code></p> <p>Least-square estimate for linear changes.</p> Source code in <code>ruptures/costs/costlinear.py</code> <pre><code>class CostLinear(BaseCost):\n    r\"\"\"Least-square estimate for linear changes.\"\"\"\n\n    model = \"linear\"\n\n    def __init__(self):\n        \"\"\"Initialize the object.\"\"\"\n        self.signal = None\n        self.covar = None\n        self.min_size = 2\n\n    def fit(self, signal) -&gt; \"CostLinear\":\n        \"\"\"Set parameters of the instance. The first column contains the\n        observed variable. The other columns contains the covariates.\n\n        Args:\n            signal (array): signal of shape (n_samples, n_regressors+1)\n\n        Returns:\n            self\n        \"\"\"\n        assert signal.ndim &gt; 1, \"Not enough dimensions\"\n\n        self.signal = signal[:, 0].reshape(-1, 1)\n        self.covar = signal[:, 1:]\n        return self\n\n    def error(self, start, end) -&gt; float:\n        \"\"\"Return the approximation cost on the segment [start:end].\n\n        Args:\n            start (int): start of the segment\n            end (int): end of the segment\n\n        Returns:\n            segment cost\n\n        Raises:\n            NotEnoughPoints: when the segment is too short (less than `min_size` samples).\n        \"\"\"\n        if end - start &lt; self.min_size:\n            raise NotEnoughPoints\n        y, X = self.signal[start:end], self.covar[start:end]\n        _, residual, _, _ = lstsq(X, y, rcond=None)\n        return residual.sum()\n</code></pre>"},{"location":"code-reference/costs/costlinear-reference/#ruptures.costs.costlinear.CostLinear.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the object.</p> Source code in <code>ruptures/costs/costlinear.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the object.\"\"\"\n    self.signal = None\n    self.covar = None\n    self.min_size = 2\n</code></pre>"},{"location":"code-reference/costs/costlinear-reference/#ruptures.costs.costlinear.CostLinear.error","title":"<code>error(start, end)</code>","text":"<p>Return the approximation cost on the segment [start:end].</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>start of the segment</p> required <code>end</code> <code>int</code> <p>end of the segment</p> required <p>Returns:</p> Type Description <code>float</code> <p>segment cost</p> <p>Raises:</p> Type Description <code>NotEnoughPoints</code> <p>when the segment is too short (less than <code>min_size</code> samples).</p> Source code in <code>ruptures/costs/costlinear.py</code> <pre><code>def error(self, start, end) -&gt; float:\n    \"\"\"Return the approximation cost on the segment [start:end].\n\n    Args:\n        start (int): start of the segment\n        end (int): end of the segment\n\n    Returns:\n        segment cost\n\n    Raises:\n        NotEnoughPoints: when the segment is too short (less than `min_size` samples).\n    \"\"\"\n    if end - start &lt; self.min_size:\n        raise NotEnoughPoints\n    y, X = self.signal[start:end], self.covar[start:end]\n    _, residual, _, _ = lstsq(X, y, rcond=None)\n    return residual.sum()\n</code></pre>"},{"location":"code-reference/costs/costlinear-reference/#ruptures.costs.costlinear.CostLinear.fit","title":"<code>fit(signal)</code>","text":"<p>Set parameters of the instance. The first column contains the observed variable. The other columns contains the covariates.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>signal of shape (n_samples, n_regressors+1)</p> required <p>Returns:</p> Type Description <code>CostLinear</code> <p>self</p> Source code in <code>ruptures/costs/costlinear.py</code> <pre><code>def fit(self, signal) -&gt; \"CostLinear\":\n    \"\"\"Set parameters of the instance. The first column contains the\n    observed variable. The other columns contains the covariates.\n\n    Args:\n        signal (array): signal of shape (n_samples, n_regressors+1)\n\n    Returns:\n        self\n    \"\"\"\n    assert signal.ndim &gt; 1, \"Not enough dimensions\"\n\n    self.signal = signal[:, 0].reshape(-1, 1)\n    self.covar = signal[:, 1:]\n    return self\n</code></pre>"},{"location":"code-reference/costs/costml-reference/","title":"Mahalanobis-type change (CostMl)","text":"<p>             Bases: <code>BaseCost</code></p> <p>Mahalanobis-type cost function.</p> Source code in <code>ruptures/costs/costml.py</code> <pre><code>class CostMl(BaseCost):\n    r\"\"\"Mahalanobis-type cost function.\"\"\"\n\n    model = \"mahalanobis\"\n\n    def __init__(self, metric=None):\n        \"\"\"Create a new instance.\n\n        Args:\n            metric (ndarray, optional): PSD matrix that defines a\n                Mahalanobis-type pseudo distance. If None, defaults to the\n                Mahalanobis matrix. Shape (n_features, n_features).\n        \"\"\"\n        self.metric = metric  # metric matrix\n        self.has_custom_metric = False if self.metric is None else True\n        self.gram = None\n        self.min_size = 2\n\n    def fit(self, signal) -&gt; \"CostMl\":\n        \"\"\"Set parameters of the instance.\n\n        Args:\n            signal (array): signal. Shape (n_samples,) or\n                (n_samples, n_features)\n\n        Returns:\n            self\n        \"\"\"\n        s_ = signal.reshape(-1, 1) if signal.ndim == 1 else signal\n\n        # fit a Mahalanobis metric if self.has_custom_metric is False\n        if self.has_custom_metric is False:\n            covar = np.cov(s_.T)\n            self.metric = inv(covar.reshape(1, 1) if covar.size == 1 else covar)\n\n        self.gram = s_.dot(self.metric).dot(s_.T)\n        self.signal = s_\n\n        return self\n\n    def error(self, start, end):\n        \"\"\"Return the approximation cost on the segment [start:end].\n\n        Args:\n            start (int): start of the segment\n            end (int): end of the segment\n\n        Returns:\n            float: segment cost\n\n        Raises:\n            NotEnoughPoints: when the segment is too short (less than\n                ``'min_size'`` samples).\n        \"\"\"\n        if end - start &lt; self.min_size:\n            raise NotEnoughPoints\n        sub_gram = self.gram[start:end, start:end]\n        val = np.diagonal(sub_gram).sum()\n        val -= sub_gram.sum() / (end - start)\n        return val\n</code></pre>"},{"location":"code-reference/costs/costml-reference/#ruptures.costs.costml.CostMl.__init__","title":"<code>__init__(metric=None)</code>","text":"<p>Create a new instance.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>ndarray</code> <p>PSD matrix that defines a Mahalanobis-type pseudo distance. If None, defaults to the Mahalanobis matrix. Shape (n_features, n_features).</p> <code>None</code> Source code in <code>ruptures/costs/costml.py</code> <pre><code>def __init__(self, metric=None):\n    \"\"\"Create a new instance.\n\n    Args:\n        metric (ndarray, optional): PSD matrix that defines a\n            Mahalanobis-type pseudo distance. If None, defaults to the\n            Mahalanobis matrix. Shape (n_features, n_features).\n    \"\"\"\n    self.metric = metric  # metric matrix\n    self.has_custom_metric = False if self.metric is None else True\n    self.gram = None\n    self.min_size = 2\n</code></pre>"},{"location":"code-reference/costs/costml-reference/#ruptures.costs.costml.CostMl.error","title":"<code>error(start, end)</code>","text":"<p>Return the approximation cost on the segment [start:end].</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>start of the segment</p> required <code>end</code> <code>int</code> <p>end of the segment</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>segment cost</p> <p>Raises:</p> Type Description <code>NotEnoughPoints</code> <p>when the segment is too short (less than <code>'min_size'</code> samples).</p> Source code in <code>ruptures/costs/costml.py</code> <pre><code>def error(self, start, end):\n    \"\"\"Return the approximation cost on the segment [start:end].\n\n    Args:\n        start (int): start of the segment\n        end (int): end of the segment\n\n    Returns:\n        float: segment cost\n\n    Raises:\n        NotEnoughPoints: when the segment is too short (less than\n            ``'min_size'`` samples).\n    \"\"\"\n    if end - start &lt; self.min_size:\n        raise NotEnoughPoints\n    sub_gram = self.gram[start:end, start:end]\n    val = np.diagonal(sub_gram).sum()\n    val -= sub_gram.sum() / (end - start)\n    return val\n</code></pre>"},{"location":"code-reference/costs/costml-reference/#ruptures.costs.costml.CostMl.fit","title":"<code>fit(signal)</code>","text":"<p>Set parameters of the instance.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>signal. Shape (n_samples,) or (n_samples, n_features)</p> required <p>Returns:</p> Type Description <code>CostMl</code> <p>self</p> Source code in <code>ruptures/costs/costml.py</code> <pre><code>def fit(self, signal) -&gt; \"CostMl\":\n    \"\"\"Set parameters of the instance.\n\n    Args:\n        signal (array): signal. Shape (n_samples,) or\n            (n_samples, n_features)\n\n    Returns:\n        self\n    \"\"\"\n    s_ = signal.reshape(-1, 1) if signal.ndim == 1 else signal\n\n    # fit a Mahalanobis metric if self.has_custom_metric is False\n    if self.has_custom_metric is False:\n        covar = np.cov(s_.T)\n        self.metric = inv(covar.reshape(1, 1) if covar.size == 1 else covar)\n\n    self.gram = s_.dot(self.metric).dot(s_.T)\n    self.signal = s_\n\n    return self\n</code></pre>"},{"location":"code-reference/costs/costnormal-reference/","title":"Gaussian process change (CostNormal)","text":"<p>             Bases: <code>BaseCost</code></p> <p>Gaussian process change.</p> Source code in <code>ruptures/costs/costnormal.py</code> <pre><code>class CostNormal(BaseCost):\n    \"\"\"Gaussian process change.\"\"\"\n\n    model = \"normal\"\n\n    def __init__(self, add_small_diag=True):\n        \"\"\"Initialize the object.\n\n        Args:\n            add_small_diag (bool, optional): For signals with truly constant\n                segments, the covariance matrix is badly conditioned, so we add\n                a small diagonal matrix. Defaults to True.\n        \"\"\"\n        self.signal = None\n        self.min_size = 2\n        self.add_small_diag = add_small_diag\n        if add_small_diag:\n            warnings.warn(\n                \"New behaviour in v1.1.5: \"\n                \"a small bias is added to the covariance matrix to cope with truly \"\n                \"constant segments (see PR#198).\",\n                UserWarning,\n            )\n\n    def fit(self, signal) -&gt; \"CostNormal\":\n        \"\"\"Set parameters of the instance.\n\n        Args:\n            signal (array): signal of shape (n_samples,) or\n                (n_samples, n_features)\n\n        Returns:\n            self\n        \"\"\"\n        if signal.ndim == 1:\n            self.signal = signal.reshape(-1, 1)\n        else:\n            self.signal = signal\n        self.n_samples, self.n_dims = self.signal.shape\n        return self\n\n    def error(self, start, end) -&gt; float:\n        \"\"\"Return the approximation cost on the segment [start:end].\n\n        Args:\n            start (int): start of the segment\n            end (int): end of the segment\n\n        Returns:\n            segment cost\n\n        Raises:\n            NotEnoughPoints: when the segment is too short (less than `min_size`\n                samples).\n        \"\"\"\n        if end - start &lt; self.min_size:\n            raise NotEnoughPoints\n        sub = self.signal[start:end]\n\n        if self.signal.shape[1] &gt; 1:\n            cov = np.cov(sub.T)\n        else:\n            cov = np.array([[sub.var()]])\n        if self.add_small_diag:  # adding small bias\n            cov += 1e-6 * np.eye(self.n_dims)\n        _, val = slogdet(cov)\n        return val * (end - start)\n</code></pre>"},{"location":"code-reference/costs/costnormal-reference/#ruptures.costs.costnormal.CostNormal.__init__","title":"<code>__init__(add_small_diag=True)</code>","text":"<p>Initialize the object.</p> <p>Parameters:</p> Name Type Description Default <code>add_small_diag</code> <code>bool</code> <p>For signals with truly constant segments, the covariance matrix is badly conditioned, so we add a small diagonal matrix. Defaults to True.</p> <code>True</code> Source code in <code>ruptures/costs/costnormal.py</code> <pre><code>def __init__(self, add_small_diag=True):\n    \"\"\"Initialize the object.\n\n    Args:\n        add_small_diag (bool, optional): For signals with truly constant\n            segments, the covariance matrix is badly conditioned, so we add\n            a small diagonal matrix. Defaults to True.\n    \"\"\"\n    self.signal = None\n    self.min_size = 2\n    self.add_small_diag = add_small_diag\n    if add_small_diag:\n        warnings.warn(\n            \"New behaviour in v1.1.5: \"\n            \"a small bias is added to the covariance matrix to cope with truly \"\n            \"constant segments (see PR#198).\",\n            UserWarning,\n        )\n</code></pre>"},{"location":"code-reference/costs/costnormal-reference/#ruptures.costs.costnormal.CostNormal.error","title":"<code>error(start, end)</code>","text":"<p>Return the approximation cost on the segment [start:end].</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>start of the segment</p> required <code>end</code> <code>int</code> <p>end of the segment</p> required <p>Returns:</p> Type Description <code>float</code> <p>segment cost</p> <p>Raises:</p> Type Description <code>NotEnoughPoints</code> <p>when the segment is too short (less than <code>min_size</code> samples).</p> Source code in <code>ruptures/costs/costnormal.py</code> <pre><code>def error(self, start, end) -&gt; float:\n    \"\"\"Return the approximation cost on the segment [start:end].\n\n    Args:\n        start (int): start of the segment\n        end (int): end of the segment\n\n    Returns:\n        segment cost\n\n    Raises:\n        NotEnoughPoints: when the segment is too short (less than `min_size`\n            samples).\n    \"\"\"\n    if end - start &lt; self.min_size:\n        raise NotEnoughPoints\n    sub = self.signal[start:end]\n\n    if self.signal.shape[1] &gt; 1:\n        cov = np.cov(sub.T)\n    else:\n        cov = np.array([[sub.var()]])\n    if self.add_small_diag:  # adding small bias\n        cov += 1e-6 * np.eye(self.n_dims)\n    _, val = slogdet(cov)\n    return val * (end - start)\n</code></pre>"},{"location":"code-reference/costs/costnormal-reference/#ruptures.costs.costnormal.CostNormal.fit","title":"<code>fit(signal)</code>","text":"<p>Set parameters of the instance.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>signal of shape (n_samples,) or (n_samples, n_features)</p> required <p>Returns:</p> Type Description <code>CostNormal</code> <p>self</p> Source code in <code>ruptures/costs/costnormal.py</code> <pre><code>def fit(self, signal) -&gt; \"CostNormal\":\n    \"\"\"Set parameters of the instance.\n\n    Args:\n        signal (array): signal of shape (n_samples,) or\n            (n_samples, n_features)\n\n    Returns:\n        self\n    \"\"\"\n    if signal.ndim == 1:\n        self.signal = signal.reshape(-1, 1)\n    else:\n        self.signal = signal\n    self.n_samples, self.n_dims = self.signal.shape\n    return self\n</code></pre>"},{"location":"code-reference/costs/costrank-reference/","title":"Rank-based change (CostRank)","text":"<p>             Bases: <code>BaseCost</code></p> <p>Rank-based cost function.</p> Source code in <code>ruptures/costs/costrank.py</code> <pre><code>class CostRank(BaseCost):\n    r\"\"\"Rank-based cost function.\"\"\"\n\n    model = \"rank\"\n\n    def __init__(self):\n        \"\"\"Initialize the object.\"\"\"\n        self.inv_cov = None\n        self.ranks = None\n        self.min_size = 2\n\n    def fit(self, signal) -&gt; \"CostRank\":\n        \"\"\"Set parameters of the instance.\n\n        Args:\n            signal (array): signal. Shape (n_samples,) or (n_samples, n_features)\n\n        Returns:\n            self\n        \"\"\"\n        if signal.ndim == 1:\n            signal = signal.reshape(-1, 1)\n\n        obs, vars = signal.shape\n\n        # Convert signal data into ranks in the range [1, n]\n        ranks = rankdata(signal, axis=0)\n        # Center the ranks into the range [-(n+1)/2, (n+1)/2]\n        centered_ranks = ranks - ((obs + 1) / 2)\n        # Sigma is the covariance of these ranks.\n        # If it's a scalar, reshape it into a 1x1 matrix\n        cov = np.cov(centered_ranks, rowvar=False, bias=True).reshape(vars, vars)\n\n        # Use the pseudoinverse to handle linear dependencies\n        # see Lung-Yut-Fong, A., L\u00e9vy-Leduc, C., &amp; Capp\u00e9, O. (2015)\n        try:\n            self.inv_cov = pinv(cov)\n        except LinAlgError as e:\n            raise LinAlgError(\n                \"The covariance matrix of the rank signal is not invertible and the \"\n                \"pseudo-inverse computation did not converge.\"\n            ) from e\n        self.ranks = centered_ranks\n        self.signal = signal\n\n        return self\n\n    def error(self, start, end):\n        \"\"\"Return the approximation cost on the segment [start:end].\n\n        Args:\n            start (int): start of the segment\n            end (int): end of the segment\n\n        Returns:\n            float: segment cost\n\n        Raises:\n            NotEnoughPoints: when the segment is too short (less than `min_size` samples).\n        \"\"\"\n        if end - start &lt; self.min_size:\n            raise NotEnoughPoints\n\n        mean = np.reshape(np.mean(self.ranks[start:end], axis=0), (-1, 1))\n\n        return -(end - start) * mean.T @ self.inv_cov @ mean\n</code></pre>"},{"location":"code-reference/costs/costrank-reference/#ruptures.costs.costrank.CostRank.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the object.</p> Source code in <code>ruptures/costs/costrank.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the object.\"\"\"\n    self.inv_cov = None\n    self.ranks = None\n    self.min_size = 2\n</code></pre>"},{"location":"code-reference/costs/costrank-reference/#ruptures.costs.costrank.CostRank.error","title":"<code>error(start, end)</code>","text":"<p>Return the approximation cost on the segment [start:end].</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>start of the segment</p> required <code>end</code> <code>int</code> <p>end of the segment</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>segment cost</p> <p>Raises:</p> Type Description <code>NotEnoughPoints</code> <p>when the segment is too short (less than <code>min_size</code> samples).</p> Source code in <code>ruptures/costs/costrank.py</code> <pre><code>def error(self, start, end):\n    \"\"\"Return the approximation cost on the segment [start:end].\n\n    Args:\n        start (int): start of the segment\n        end (int): end of the segment\n\n    Returns:\n        float: segment cost\n\n    Raises:\n        NotEnoughPoints: when the segment is too short (less than `min_size` samples).\n    \"\"\"\n    if end - start &lt; self.min_size:\n        raise NotEnoughPoints\n\n    mean = np.reshape(np.mean(self.ranks[start:end], axis=0), (-1, 1))\n\n    return -(end - start) * mean.T @ self.inv_cov @ mean\n</code></pre>"},{"location":"code-reference/costs/costrank-reference/#ruptures.costs.costrank.CostRank.fit","title":"<code>fit(signal)</code>","text":"<p>Set parameters of the instance.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>signal. Shape (n_samples,) or (n_samples, n_features)</p> required <p>Returns:</p> Type Description <code>CostRank</code> <p>self</p> Source code in <code>ruptures/costs/costrank.py</code> <pre><code>def fit(self, signal) -&gt; \"CostRank\":\n    \"\"\"Set parameters of the instance.\n\n    Args:\n        signal (array): signal. Shape (n_samples,) or (n_samples, n_features)\n\n    Returns:\n        self\n    \"\"\"\n    if signal.ndim == 1:\n        signal = signal.reshape(-1, 1)\n\n    obs, vars = signal.shape\n\n    # Convert signal data into ranks in the range [1, n]\n    ranks = rankdata(signal, axis=0)\n    # Center the ranks into the range [-(n+1)/2, (n+1)/2]\n    centered_ranks = ranks - ((obs + 1) / 2)\n    # Sigma is the covariance of these ranks.\n    # If it's a scalar, reshape it into a 1x1 matrix\n    cov = np.cov(centered_ranks, rowvar=False, bias=True).reshape(vars, vars)\n\n    # Use the pseudoinverse to handle linear dependencies\n    # see Lung-Yut-Fong, A., L\u00e9vy-Leduc, C., &amp; Capp\u00e9, O. (2015)\n    try:\n        self.inv_cov = pinv(cov)\n    except LinAlgError as e:\n        raise LinAlgError(\n            \"The covariance matrix of the rank signal is not invertible and the \"\n            \"pseudo-inverse computation did not converge.\"\n        ) from e\n    self.ranks = centered_ranks\n    self.signal = signal\n\n    return self\n</code></pre>"},{"location":"code-reference/costs/costrbf-reference/","title":"Kernelized mean change (CostRbf)","text":"<p>             Bases: <code>BaseCost</code></p> <p>Kernel cost function (rbf kernel).</p> Source code in <code>ruptures/costs/costrbf.py</code> <pre><code>class CostRbf(BaseCost):\n    r\"\"\"Kernel cost function (rbf kernel).\"\"\"\n\n    model = \"rbf\"\n\n    def __init__(self, gamma=None):\n        \"\"\"Initialize the object.\"\"\"\n        self.min_size = 1\n        self.gamma = gamma\n        self._gram = None\n\n    @property\n    def gram(self):\n        \"\"\"Generate the gram matrix (lazy loading).\n\n        Only access this function after a `.fit()` (otherwise\n        `self.signal` is not defined).\n        \"\"\"\n        if self._gram is None:\n            K = pdist(self.signal, metric=\"sqeuclidean\")\n            if self.gamma is None:\n                self.gamma = 1.0\n                # median heuristics\n                K_median = np.median(K)\n                if K_median != 0:\n                    # K /= K_median\n                    self.gamma = 1 / K_median\n            K *= self.gamma\n            np.clip(K, 1e-2, 1e2, K)  # clipping to avoid exponential under/overflow\n            self._gram = np.exp(squareform(-K))\n        return self._gram\n\n    def fit(self, signal) -&gt; \"CostRbf\":\n        \"\"\"Sets parameters of the instance.\n\n        Args:\n            signal (array): signal. Shape (n_samples,) or (n_samples, n_features)\n\n        Returns:\n            self\n        \"\"\"\n        if signal.ndim == 1:\n            self.signal = signal.reshape(-1, 1)\n        else:\n            self.signal = signal\n\n        # If gamma is none, set it using the median heuristic.\n        # This heuristic involves computing the gram matrix which is lazy loaded\n        # so we simply access the `.gram` property\n        if self.gamma is None:\n            self.gram\n\n        return self\n\n    def error(self, start, end) -&gt; float:\n        \"\"\"Return the approximation cost on the segment [start:end].\n\n        Args:\n            start (int): start of the segment\n            end (int): end of the segment\n\n        Returns:\n            segment cost\n\n        Raises:\n            NotEnoughPoints: when the segment is too short (less than `min_size` samples).\n        \"\"\"\n        if end - start &lt; self.min_size:\n            raise NotEnoughPoints\n        sub_gram = self.gram[start:end, start:end]\n        val = np.diagonal(sub_gram).sum()\n        val -= sub_gram.sum() / (end - start)\n        return val\n</code></pre>"},{"location":"code-reference/costs/costrbf-reference/#ruptures.costs.costrbf.CostRbf.gram","title":"<code>gram</code>  <code>property</code>","text":"<p>Generate the gram matrix (lazy loading).</p> <p>Only access this function after a <code>.fit()</code> (otherwise <code>self.signal</code> is not defined).</p>"},{"location":"code-reference/costs/costrbf-reference/#ruptures.costs.costrbf.CostRbf.__init__","title":"<code>__init__(gamma=None)</code>","text":"<p>Initialize the object.</p> Source code in <code>ruptures/costs/costrbf.py</code> <pre><code>def __init__(self, gamma=None):\n    \"\"\"Initialize the object.\"\"\"\n    self.min_size = 1\n    self.gamma = gamma\n    self._gram = None\n</code></pre>"},{"location":"code-reference/costs/costrbf-reference/#ruptures.costs.costrbf.CostRbf.error","title":"<code>error(start, end)</code>","text":"<p>Return the approximation cost on the segment [start:end].</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>start of the segment</p> required <code>end</code> <code>int</code> <p>end of the segment</p> required <p>Returns:</p> Type Description <code>float</code> <p>segment cost</p> <p>Raises:</p> Type Description <code>NotEnoughPoints</code> <p>when the segment is too short (less than <code>min_size</code> samples).</p> Source code in <code>ruptures/costs/costrbf.py</code> <pre><code>def error(self, start, end) -&gt; float:\n    \"\"\"Return the approximation cost on the segment [start:end].\n\n    Args:\n        start (int): start of the segment\n        end (int): end of the segment\n\n    Returns:\n        segment cost\n\n    Raises:\n        NotEnoughPoints: when the segment is too short (less than `min_size` samples).\n    \"\"\"\n    if end - start &lt; self.min_size:\n        raise NotEnoughPoints\n    sub_gram = self.gram[start:end, start:end]\n    val = np.diagonal(sub_gram).sum()\n    val -= sub_gram.sum() / (end - start)\n    return val\n</code></pre>"},{"location":"code-reference/costs/costrbf-reference/#ruptures.costs.costrbf.CostRbf.fit","title":"<code>fit(signal)</code>","text":"<p>Sets parameters of the instance.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>signal. Shape (n_samples,) or (n_samples, n_features)</p> required <p>Returns:</p> Type Description <code>CostRbf</code> <p>self</p> Source code in <code>ruptures/costs/costrbf.py</code> <pre><code>def fit(self, signal) -&gt; \"CostRbf\":\n    \"\"\"Sets parameters of the instance.\n\n    Args:\n        signal (array): signal. Shape (n_samples,) or (n_samples, n_features)\n\n    Returns:\n        self\n    \"\"\"\n    if signal.ndim == 1:\n        self.signal = signal.reshape(-1, 1)\n    else:\n        self.signal = signal\n\n    # If gamma is none, set it using the median heuristic.\n    # This heuristic involves computing the gram matrix which is lazy loaded\n    # so we simply access the `.gram` property\n    if self.gamma is None:\n        self.gram\n\n    return self\n</code></pre>"},{"location":"code-reference/datasets/pw_constant-reference/","title":"Piecewise constant (pw_constant)","text":"<p>Return a piecewise constant signal and the associated changepoints.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>signal length</p> <code>200</code> <code>n_features</code> <code>int</code> <p>number of dimensions</p> <code>1</code> <code>n_bkps</code> <code>int</code> <p>number of changepoints</p> <code>3</code> <code>noise_std</code> <code>float</code> <p>noise std. If None, no noise is added</p> <code>None</code> <code>delta</code> <code>tuple</code> <p>(delta_min, delta_max) max and min jump values</p> <code>(1, 10)</code> <code>seed</code> <code>int</code> <p>random seed</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>signal of shape (n_samples, n_features), list of breakpoints</p> Source code in <code>ruptures/datasets/pw_constant.py</code> <pre><code>def pw_constant(\n    n_samples=200, n_features=1, n_bkps=3, noise_std=None, delta=(1, 10), seed=None\n):\n    \"\"\"Return a piecewise constant signal and the associated changepoints.\n\n    Args:\n        n_samples (int): signal length\n        n_features (int, optional): number of dimensions\n        n_bkps (int, optional): number of changepoints\n        noise_std (float, optional): noise std. If None, no noise is added\n        delta (tuple, optional): (delta_min, delta_max) max and min jump values\n        seed (int): random seed\n\n    Returns:\n        tuple: signal of shape (n_samples, n_features), list of breakpoints\n    \"\"\"\n    # breakpoints\n    bkps = draw_bkps(n_samples, n_bkps, seed=seed)\n    # we create the signal\n    signal = np.empty((n_samples, n_features), dtype=float)\n    tt_ = np.arange(n_samples)\n    delta_min, delta_max = delta\n    # mean value\n    center = np.zeros(n_features)\n    rng = np.random.default_rng(seed=seed)\n    for ind in np.split(tt_, bkps):\n        if ind.size &gt; 0:\n            # jump value\n            jump = rng.uniform(delta_min, delta_max, size=n_features)\n            spin = rng.choice([-1, 1], n_features)\n            center += jump * spin\n            signal[ind] = center\n\n    if noise_std is not None:\n        noise = rng.normal(size=signal.shape) * noise_std\n        signal = signal + noise\n\n    return signal, bkps\n</code></pre>"},{"location":"code-reference/datasets/pw_linear-reference/","title":"Piecewise linear (pw_linear)","text":"<p>Return piecewise linear signal and the associated changepoints.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>signal length</p> <code>200</code> <code>n_features</code> <code>int</code> <p>number of covariates</p> <code>1</code> <code>n_bkps</code> <code>int</code> <p>number of change points</p> <code>3</code> <code>noise_std</code> <code>float</code> <p>noise std. If None, no noise is added</p> <code>None</code> <code>seed</code> <code>int</code> <p>random seed</p> <code>None</code> <p>Returns:     tuple: signal of shape (n_samples, n_features+1), list of breakpoints</p> Source code in <code>ruptures/datasets/pw_linear.py</code> <pre><code>def pw_linear(n_samples=200, n_features=1, n_bkps=3, noise_std=None, seed=None):\n    \"\"\"Return piecewise linear signal and the associated changepoints.\n\n    Args:\n        n_samples (int, optional): signal length\n        n_features (int, optional): number of covariates\n        n_bkps (int, optional): number of change points\n        noise_std (float, optional): noise std. If None, no noise is added\n        seed (int): random seed\n    Returns:\n        tuple: signal of shape (n_samples, n_features+1), list of breakpoints\n    \"\"\"\n    rng = np.random.default_rng(seed=seed)\n    covar = rng.normal(size=(n_samples, n_features))\n    linear_coeff, bkps = pw_constant(\n        n_samples=n_samples,\n        n_bkps=n_bkps,\n        n_features=n_features,\n        noise_std=None,\n        seed=seed,\n    )\n    var = np.sum(linear_coeff * covar, axis=1)\n    if noise_std is not None:\n        var += rng.normal(scale=noise_std, size=var.shape)\n    signal = np.c_[var, covar]\n    return signal, bkps\n</code></pre>"},{"location":"code-reference/datasets/pw_normal-reference/","title":"Piecewise Gaussian (pw_normal)","text":"<p>Return a 2D piecewise Gaussian signal and the associated changepoints.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>signal length</p> <code>200</code> <code>n_bkps</code> <code>int</code> <p>number of change points</p> <code>3</code> <code>seed</code> <code>int</code> <p>random seed</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>signal of shape (n_samples, 2), list of breakpoints</p> Source code in <code>ruptures/datasets/pw_normal.py</code> <pre><code>def pw_normal(n_samples=200, n_bkps=3, seed=None):\n    \"\"\"Return a 2D piecewise Gaussian signal and the associated changepoints.\n\n    Args:\n        n_samples (int, optional): signal length\n        n_bkps (int, optional): number of change points\n        seed (int): random seed\n\n    Returns:\n        tuple: signal of shape (n_samples, 2), list of breakpoints\n    \"\"\"\n    # breakpoints\n    bkps = draw_bkps(n_samples, n_bkps, seed=seed)\n    # we create the signal\n    signal = np.zeros((n_samples, 2), dtype=float)\n    cov1 = np.array([[1, 0.9], [0.9, 1]])\n    cov2 = np.array([[1, -0.9], [-0.9, 1]])\n    rng = np.random.default_rng(seed=seed)\n    for sub, cov in zip(np.split(signal, bkps), cycle((cov1, cov2))):\n        n_sub, _ = sub.shape\n        sub += rng.multivariate_normal([0, 0], cov, size=n_sub)\n\n    return signal, bkps\n</code></pre>"},{"location":"code-reference/datasets/pw_wavy-reference/","title":"Piecewise wavy (pw_wavy)","text":"<p>Return a 1D piecewise wavy signal and the associated changepoints.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>signal length</p> <code>200</code> <code>n_bkps</code> <code>int</code> <p>number of changepoints</p> <code>3</code> <code>noise_std</code> <code>float</code> <p>noise std. If None, no noise is added</p> <code>None</code> <code>seed</code> <code>int</code> <p>random seed</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>signal of shape (n_samples, 1), list of breakpoints</p> Source code in <code>ruptures/datasets/pw_wavy.py</code> <pre><code>def pw_wavy(n_samples=200, n_bkps=3, noise_std=None, seed=None):\n    \"\"\"Return a 1D piecewise wavy signal and the associated changepoints.\n\n    Args:\n        n_samples (int, optional): signal length\n        n_bkps (int, optional): number of changepoints\n        noise_std (float, optional): noise std. If None, no noise is added\n        seed (int): random seed\n\n    Returns:\n        tuple: signal of shape (n_samples, 1), list of breakpoints\n    \"\"\"\n    # breakpoints\n    bkps = draw_bkps(n_samples, n_bkps, seed=seed)\n    # we create the signal\n    f1 = np.array([0.075, 0.1])\n    f2 = np.array([0.1, 0.125])\n    freqs = np.zeros((n_samples, 2))\n    for sub, val in zip(np.split(freqs, bkps[:-1]), cycle([f1, f2])):\n        sub += val\n    tt = np.arange(n_samples)\n\n    # DeprecationWarning: Calling np.sum(generator) is deprecated\n    # Use np.sum(np.from_iter(generator)) or the python sum builtin instead.\n    signal = np.sum([np.sin(2 * np.pi * tt * f) for f in freqs.T], axis=0)\n\n    if noise_std is not None:\n        rng = np.random.default_rng(seed=seed)\n        noise = rng.normal(scale=noise_std, size=signal.shape)\n        signal += noise\n\n    return signal, bkps\n</code></pre>"},{"location":"code-reference/detection/binseg-reference/","title":"Binary segmentation","text":"<p>             Bases: <code>BaseEstimator</code></p> <p>Binary segmentation.</p> Source code in <code>ruptures/detection/binseg.py</code> <pre><code>class Binseg(BaseEstimator):\n    \"\"\"Binary segmentation.\"\"\"\n\n    def __init__(self, model=\"l2\", custom_cost=None, min_size=2, jump=5, params=None):\n        \"\"\"Initialize a Binseg instance.\n\n        Args:\n            model (str, optional): segment model, [\"l1\", \"l2\", \"rbf\",...]. Not used if ``'custom_cost'`` is not None.\n            custom_cost (BaseCost, optional): custom cost function. Defaults to None.\n            min_size (int, optional): minimum segment length. Defaults to 2 samples.\n            jump (int, optional): subsample (one every *jump* points). Defaults to 5 samples.\n            params (dict, optional): a dictionary of parameters for the cost instance.\n        \"\"\"\n        if custom_cost is not None and isinstance(custom_cost, BaseCost):\n            self.cost = custom_cost\n        else:\n            if params is None:\n                self.cost = cost_factory(model=model)\n            else:\n                self.cost = cost_factory(model=model, **params)\n        self.min_size = max(min_size, self.cost.min_size)\n        self.jump = jump\n        self.n_samples = None\n        self.signal = None\n\n    def _seg(self, n_bkps=None, pen=None, epsilon=None):\n        \"\"\"Computes the binary segmentation.\n\n        The stopping rule depends on the parameter passed to the function.\n\n        Args:\n            n_bkps (int): number of breakpoints to find before stopping.\n            penalty (float): penalty value (&gt;0)\n            epsilon (float): reconstruction budget (&gt;0)\n\n        Returns:\n            dict: partition dict {(start, end): cost value,...}\n        \"\"\"\n        # initialization\n        bkps = [self.n_samples]\n        stop = False\n        while not stop:\n            stop = True\n            new_bkps = [\n                self.single_bkp(start, end) for start, end in pairwise([0] + bkps)\n            ]\n            bkp, gain = max(new_bkps, key=lambda x: x[1])\n\n            if bkp is None:  # all possible configuration have been explored.\n                break\n\n            if n_bkps is not None:\n                if len(bkps) - 1 &lt; n_bkps:\n                    stop = False\n            elif pen is not None:\n                if gain &gt; pen:\n                    stop = False\n            elif epsilon is not None:\n                error = self.cost.sum_of_costs(bkps)\n                if error &gt; epsilon:\n                    stop = False\n\n            if not stop:\n                bkps.append(bkp)\n                bkps.sort()\n        partition = {\n            (start, end): self.cost.error(start, end)\n            for start, end in pairwise([0] + bkps)\n        }\n        return partition\n\n    @lru_cache(maxsize=None)\n    def single_bkp(self, start, end):\n        \"\"\"Return the optimal breakpoint of [start:end] (if it exists).\"\"\"\n        segment_cost = self.cost.error(start, end)\n        if np.isinf(segment_cost) and segment_cost &lt; 0:  # if cost is -inf\n            return None, 0\n        gain_list = list()\n        for bkp in range(start, end, self.jump):\n            if bkp - start &gt;= self.min_size and end - bkp &gt;= self.min_size:\n                gain = (\n                    segment_cost\n                    - self.cost.error(start, bkp)\n                    - self.cost.error(bkp, end)\n                )\n                gain_list.append((gain, bkp))\n        try:\n            gain, bkp = max(gain_list)\n        except ValueError:  # if empty sub_sampling\n            return None, 0\n        return bkp, gain\n\n    def fit(self, signal) -&gt; \"Binseg\":\n        \"\"\"Compute params to segment signal.\n\n        Args:\n            signal (array): signal to segment. Shape (n_samples, n_features) or (n_samples,).\n\n        Returns:\n            self\n        \"\"\"\n        # update some params\n        if signal.ndim == 1:\n            self.signal = signal.reshape(-1, 1)\n        else:\n            self.signal = signal\n        self.n_samples, _ = self.signal.shape\n        self.cost.fit(signal)\n        self.single_bkp.cache_clear()\n\n        return self\n\n    def predict(self, n_bkps=None, pen=None, epsilon=None):\n        \"\"\"Return the optimal breakpoints.\n\n        Must be called after the fit method. The breakpoints are associated with the\n        signal passed to [`fit()`][ruptures.detection.binseg.Binseg.fit].\n        The stopping rule depends on the parameter passed to the function.\n\n        Args:\n            n_bkps (int): number of breakpoints to find before stopping.\n            pen (float): penalty value (&gt;0)\n            epsilon (float): reconstruction budget (&gt;0)\n\n        Raises:\n            AssertionError: if none of `n_bkps`, `pen`, `epsilon` is set.\n            BadSegmentationParameters: in case of impossible segmentation\n                configuration\n\n        Returns:\n            list: sorted list of breakpoints\n        \"\"\"\n        msg = \"Give a parameter.\"\n        assert any(param is not None for param in (n_bkps, pen, epsilon)), msg\n\n        # raise an exception in case of impossible segmentation configuration\n        if not sanity_check(\n            n_samples=self.cost.signal.shape[0],\n            n_bkps=0 if n_bkps is None else n_bkps,\n            jump=self.jump,\n            min_size=self.min_size,\n        ):\n            raise BadSegmentationParameters\n\n        partition = self._seg(n_bkps=n_bkps, pen=pen, epsilon=epsilon)\n        bkps = sorted(e for s, e in partition.keys())\n        return bkps\n\n    def fit_predict(self, signal, n_bkps=None, pen=None, epsilon=None):\n        \"\"\"Fit to the signal and return the optimal breakpoints.\n\n        Helper method to call fit and predict once\n\n        Args:\n            signal (array): signal. Shape (n_samples, n_features) or (n_samples,).\n            n_bkps (int): number of breakpoints.\n            pen (float): penalty value (&gt;0)\n            epsilon (float): reconstruction budget (&gt;0)\n\n        Returns:\n            list: sorted list of breakpoints\n        \"\"\"\n        self.fit(signal)\n        return self.predict(n_bkps=n_bkps, pen=pen, epsilon=epsilon)\n</code></pre>"},{"location":"code-reference/detection/binseg-reference/#ruptures.detection.binseg.Binseg.__init__","title":"<code>__init__(model='l2', custom_cost=None, min_size=2, jump=5, params=None)</code>","text":"<p>Initialize a Binseg instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>segment model, [\"l1\", \"l2\", \"rbf\",...]. Not used if <code>'custom_cost'</code> is not None.</p> <code>'l2'</code> <code>custom_cost</code> <code>BaseCost</code> <p>custom cost function. Defaults to None.</p> <code>None</code> <code>min_size</code> <code>int</code> <p>minimum segment length. Defaults to 2 samples.</p> <code>2</code> <code>jump</code> <code>int</code> <p>subsample (one every jump points). Defaults to 5 samples.</p> <code>5</code> <code>params</code> <code>dict</code> <p>a dictionary of parameters for the cost instance.</p> <code>None</code> Source code in <code>ruptures/detection/binseg.py</code> <pre><code>def __init__(self, model=\"l2\", custom_cost=None, min_size=2, jump=5, params=None):\n    \"\"\"Initialize a Binseg instance.\n\n    Args:\n        model (str, optional): segment model, [\"l1\", \"l2\", \"rbf\",...]. Not used if ``'custom_cost'`` is not None.\n        custom_cost (BaseCost, optional): custom cost function. Defaults to None.\n        min_size (int, optional): minimum segment length. Defaults to 2 samples.\n        jump (int, optional): subsample (one every *jump* points). Defaults to 5 samples.\n        params (dict, optional): a dictionary of parameters for the cost instance.\n    \"\"\"\n    if custom_cost is not None and isinstance(custom_cost, BaseCost):\n        self.cost = custom_cost\n    else:\n        if params is None:\n            self.cost = cost_factory(model=model)\n        else:\n            self.cost = cost_factory(model=model, **params)\n    self.min_size = max(min_size, self.cost.min_size)\n    self.jump = jump\n    self.n_samples = None\n    self.signal = None\n</code></pre>"},{"location":"code-reference/detection/binseg-reference/#ruptures.detection.binseg.Binseg.fit","title":"<code>fit(signal)</code>","text":"<p>Compute params to segment signal.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>signal to segment. Shape (n_samples, n_features) or (n_samples,).</p> required <p>Returns:</p> Type Description <code>Binseg</code> <p>self</p> Source code in <code>ruptures/detection/binseg.py</code> <pre><code>def fit(self, signal) -&gt; \"Binseg\":\n    \"\"\"Compute params to segment signal.\n\n    Args:\n        signal (array): signal to segment. Shape (n_samples, n_features) or (n_samples,).\n\n    Returns:\n        self\n    \"\"\"\n    # update some params\n    if signal.ndim == 1:\n        self.signal = signal.reshape(-1, 1)\n    else:\n        self.signal = signal\n    self.n_samples, _ = self.signal.shape\n    self.cost.fit(signal)\n    self.single_bkp.cache_clear()\n\n    return self\n</code></pre>"},{"location":"code-reference/detection/binseg-reference/#ruptures.detection.binseg.Binseg.fit_predict","title":"<code>fit_predict(signal, n_bkps=None, pen=None, epsilon=None)</code>","text":"<p>Fit to the signal and return the optimal breakpoints.</p> <p>Helper method to call fit and predict once</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>signal. Shape (n_samples, n_features) or (n_samples,).</p> required <code>n_bkps</code> <code>int</code> <p>number of breakpoints.</p> <code>None</code> <code>pen</code> <code>float</code> <p>penalty value (&gt;0)</p> <code>None</code> <code>epsilon</code> <code>float</code> <p>reconstruction budget (&gt;0)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <p>sorted list of breakpoints</p> Source code in <code>ruptures/detection/binseg.py</code> <pre><code>def fit_predict(self, signal, n_bkps=None, pen=None, epsilon=None):\n    \"\"\"Fit to the signal and return the optimal breakpoints.\n\n    Helper method to call fit and predict once\n\n    Args:\n        signal (array): signal. Shape (n_samples, n_features) or (n_samples,).\n        n_bkps (int): number of breakpoints.\n        pen (float): penalty value (&gt;0)\n        epsilon (float): reconstruction budget (&gt;0)\n\n    Returns:\n        list: sorted list of breakpoints\n    \"\"\"\n    self.fit(signal)\n    return self.predict(n_bkps=n_bkps, pen=pen, epsilon=epsilon)\n</code></pre>"},{"location":"code-reference/detection/binseg-reference/#ruptures.detection.binseg.Binseg.predict","title":"<code>predict(n_bkps=None, pen=None, epsilon=None)</code>","text":"<p>Return the optimal breakpoints.</p> <p>Must be called after the fit method. The breakpoints are associated with the signal passed to <code>fit()</code>. The stopping rule depends on the parameter passed to the function.</p> <p>Parameters:</p> Name Type Description Default <code>n_bkps</code> <code>int</code> <p>number of breakpoints to find before stopping.</p> <code>None</code> <code>pen</code> <code>float</code> <p>penalty value (&gt;0)</p> <code>None</code> <code>epsilon</code> <code>float</code> <p>reconstruction budget (&gt;0)</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if none of <code>n_bkps</code>, <code>pen</code>, <code>epsilon</code> is set.</p> <code>BadSegmentationParameters</code> <p>in case of impossible segmentation configuration</p> <p>Returns:</p> Name Type Description <code>list</code> <p>sorted list of breakpoints</p> Source code in <code>ruptures/detection/binseg.py</code> <pre><code>def predict(self, n_bkps=None, pen=None, epsilon=None):\n    \"\"\"Return the optimal breakpoints.\n\n    Must be called after the fit method. The breakpoints are associated with the\n    signal passed to [`fit()`][ruptures.detection.binseg.Binseg.fit].\n    The stopping rule depends on the parameter passed to the function.\n\n    Args:\n        n_bkps (int): number of breakpoints to find before stopping.\n        pen (float): penalty value (&gt;0)\n        epsilon (float): reconstruction budget (&gt;0)\n\n    Raises:\n        AssertionError: if none of `n_bkps`, `pen`, `epsilon` is set.\n        BadSegmentationParameters: in case of impossible segmentation\n            configuration\n\n    Returns:\n        list: sorted list of breakpoints\n    \"\"\"\n    msg = \"Give a parameter.\"\n    assert any(param is not None for param in (n_bkps, pen, epsilon)), msg\n\n    # raise an exception in case of impossible segmentation configuration\n    if not sanity_check(\n        n_samples=self.cost.signal.shape[0],\n        n_bkps=0 if n_bkps is None else n_bkps,\n        jump=self.jump,\n        min_size=self.min_size,\n    ):\n        raise BadSegmentationParameters\n\n    partition = self._seg(n_bkps=n_bkps, pen=pen, epsilon=epsilon)\n    bkps = sorted(e for s, e in partition.keys())\n    return bkps\n</code></pre>"},{"location":"code-reference/detection/binseg-reference/#ruptures.detection.binseg.Binseg.single_bkp","title":"<code>single_bkp(start, end)</code>  <code>cached</code>","text":"<p>Return the optimal breakpoint of [start:end] (if it exists).</p> Source code in <code>ruptures/detection/binseg.py</code> <pre><code>@lru_cache(maxsize=None)\ndef single_bkp(self, start, end):\n    \"\"\"Return the optimal breakpoint of [start:end] (if it exists).\"\"\"\n    segment_cost = self.cost.error(start, end)\n    if np.isinf(segment_cost) and segment_cost &lt; 0:  # if cost is -inf\n        return None, 0\n    gain_list = list()\n    for bkp in range(start, end, self.jump):\n        if bkp - start &gt;= self.min_size and end - bkp &gt;= self.min_size:\n            gain = (\n                segment_cost\n                - self.cost.error(start, bkp)\n                - self.cost.error(bkp, end)\n            )\n            gain_list.append((gain, bkp))\n    try:\n        gain, bkp = max(gain_list)\n    except ValueError:  # if empty sub_sampling\n        return None, 0\n    return bkp, gain\n</code></pre>"},{"location":"code-reference/detection/bottomup-reference/","title":"Bottom-up segmentation","text":"<p>             Bases: <code>BaseEstimator</code></p> <p>Bottom-up segmentation.</p> Source code in <code>ruptures/detection/bottomup.py</code> <pre><code>class BottomUp(BaseEstimator):\n    \"\"\"Bottom-up segmentation.\"\"\"\n\n    def __init__(self, model=\"l2\", custom_cost=None, min_size=2, jump=5, params=None):\n        \"\"\"Initialize a BottomUp instance.\n\n        Args:\n            model (str, optional): segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if ``'custom_cost'`` is not None.\n            custom_cost (BaseCost, optional): custom cost function. Defaults to None.\n            min_size (int, optional): minimum segment length. Defaults to 2 samples.\n            jump (int, optional): subsample (one every *jump* points). Defaults to 5 samples.\n            params (dict, optional): a dictionary of parameters for the cost instance.\n        \"\"\"\n        if custom_cost is not None and isinstance(custom_cost, BaseCost):\n            self.cost = custom_cost\n        else:\n            if params is None:\n                self.cost = cost_factory(model=model)\n            else:\n                self.cost = cost_factory(model=model, **params)\n        self.min_size = max(min_size, self.cost.min_size)\n        self.jump = jump\n        self.n_samples = None\n        self.signal = None\n        self.leaves = None\n\n    def _grow_tree(self):\n        \"\"\"Grow the entire binary tree.\"\"\"\n        partition = [(-self.n_samples, (0, self.n_samples))]\n        stop = False\n        while not stop:  # recursively divide the signal\n            stop = True\n            _, (start, end) = partition[0]\n            mid = (start + end) * 0.5\n            bkps = list()\n            for bkp in range(start, end):\n                if bkp % self.jump == 0:\n                    if bkp - start &gt;= self.min_size and end - bkp &gt;= self.min_size:\n                        bkps.append(bkp)\n            if len(bkps) &gt; 0:  # at least one admissible breakpoint was found\n                bkp = min(bkps, key=lambda x: abs(x - mid))\n                heapq.heappop(partition)\n                heapq.heappush(partition, (-bkp + start, (start, bkp)))\n                heapq.heappush(partition, (-end + bkp, (bkp, end)))\n                stop = False\n\n        partition.sort(key=lambda x: x[1])\n        # compute segment costs\n        leaves = list()\n        for _, (start, end) in partition:\n            val = self.cost.error(start, end)\n            leaf = Bnode(start, end, val)\n            leaves.append(leaf)\n        return leaves\n\n    @lru_cache(maxsize=None)\n    def merge(self, left, right):\n        \"\"\"Merge two contiguous segments.\"\"\"\n        assert left.end == right.start, \"Segments are not contiguous.\"\n        start, end = left.start, right.end\n        val = self.cost.error(start, end)\n        node = Bnode(start, end, val, left=left, right=right)\n        return node\n\n    def _seg(self, n_bkps=None, pen=None, epsilon=None):\n        \"\"\"Compute the bottom-up segmentation.\n\n        The stopping rule depends on the parameter passed to the function.\n\n        Args:\n            n_bkps (int): number of breakpoints to find before stopping.\n            penalty (float): penalty value (&gt;0)\n            epsilon (float): reconstruction budget (&gt;0)\n\n        Returns:\n            dict: partition dict {(start, end): cost value,...}\n        \"\"\"\n        leaves = sorted(self.leaves)\n        keys = [leaf.start for leaf in leaves]\n        removed = set()\n        merged = []\n        for left, right in pairwise(leaves):\n            candidate = self.merge(left, right)\n            heapq.heappush(merged, (candidate.gain, candidate))\n        # bottom up fusion\n        stop = False\n        while not stop:\n            stop = True\n\n            try:\n                gain, leaf = heapq.heappop(merged)\n                # Ignore any merge candidates whose left or right children\n                # no longer exist (because they were merged with another node).\n                # It's cheaper to do this here than during the initial merge.\n                while leaf.left in removed or leaf.right in removed:\n                    gain, leaf = heapq.heappop(merged)\n            # if merged is empty (all nodes have been merged).\n            except IndexError:\n                break\n\n            if n_bkps is not None:\n                if len(leaves) &gt; n_bkps + 1:\n                    stop = False\n            elif pen is not None:\n                if gain &lt; pen:\n                    stop = False\n            elif epsilon is not None:\n                if sum(leaf_tmp.val for leaf_tmp in leaves) &lt; epsilon:\n                    stop = False\n\n            if not stop:\n                # updates the list of leaves (i.e. segments of the partitions)\n                # find the merged segments indexes\n                left_idx = bisect_left(keys, leaf.left.start)\n                # replace leaf.left\n                leaves[left_idx] = leaf\n                keys[left_idx] = leaf.start\n                # remove leaf.right\n                del leaves[left_idx + 1]\n                del keys[left_idx + 1]\n                # add to the set of removed segments.\n                removed.add(leaf.left)\n                removed.add(leaf.right)\n                # add new merge candidates\n                if left_idx &gt; 0:\n                    left_candidate = self.merge(leaves[left_idx - 1], leaf)\n                    heapq.heappush(merged, (left_candidate.gain, left_candidate))\n                if left_idx &lt; len(leaves) - 1:\n                    right_candidate = self.merge(leaf, leaves[left_idx + 1])\n                    heapq.heappush(merged, (right_candidate.gain, right_candidate))\n\n        partition = {(leaf.start, leaf.end): leaf.val for leaf in leaves}\n        return partition\n\n    def fit(self, signal) -&gt; \"BottomUp\":\n        \"\"\"Compute params to segment signal.\n\n        Args:\n            signal (array): signal to segment. Shape (n_samples, n_features) or (n_samples,).\n\n        Returns:\n            self\n        \"\"\"\n        # update some params\n        self.cost.fit(signal)\n        self.merge.cache_clear()\n        if signal.ndim == 1:\n            (n_samples,) = signal.shape\n        else:\n            n_samples, _ = signal.shape\n        self.n_samples = n_samples\n        self.leaves = self._grow_tree()\n        return self\n\n    def predict(self, n_bkps=None, pen=None, epsilon=None):\n        \"\"\"Return the optimal breakpoints.\n\n        Must be called after the fit method. The breakpoints are associated with the signal passed\n        to [`fit()`][ruptures.detection.bottomup.BottomUp.fit].\n        The stopping rule depends on the parameter passed to the function.\n\n        Args:\n            n_bkps (int): number of breakpoints to find before stopping.\n            pen (float): penalty value (&gt;0)\n            epsilon (float): reconstruction budget (&gt;0)\n\n        Raises:\n            AssertionError: if none of `n_bkps`, `pen`, `epsilon` is set.\n            BadSegmentationParameters: in case of impossible segmentation\n                configuration\n\n        Returns:\n            list: sorted list of breakpoints\n        \"\"\"\n        msg = \"Give a parameter.\"\n        assert any(param is not None for param in (n_bkps, pen, epsilon)), msg\n\n        # raise an exception in case of impossible segmentation configuration\n        if not sanity_check(\n            n_samples=self.cost.signal.shape[0],\n            n_bkps=0 if n_bkps is None else n_bkps,\n            jump=self.jump,\n            min_size=self.min_size,\n        ):\n            raise BadSegmentationParameters\n\n        partition = self._seg(n_bkps=n_bkps, pen=pen, epsilon=epsilon)\n        bkps = sorted(e for s, e in partition.keys())\n        return bkps\n\n    def fit_predict(self, signal, n_bkps=None, pen=None, epsilon=None):\n        \"\"\"Fit to the signal and return the optimal breakpoints.\n\n        Helper method to call fit and predict once\n\n        Args:\n            signal (array): signal. Shape (n_samples, n_features) or (n_samples,).\n            n_bkps (int): number of breakpoints.\n            pen (float): penalty value (&gt;0)\n            epsilon (float): reconstruction budget (&gt;0)\n\n        Returns:\n            list: sorted list of breakpoints\n        \"\"\"\n        self.fit(signal)\n        return self.predict(n_bkps=n_bkps, pen=pen, epsilon=epsilon)\n</code></pre>"},{"location":"code-reference/detection/bottomup-reference/#ruptures.detection.bottomup.BottomUp.__init__","title":"<code>__init__(model='l2', custom_cost=None, min_size=2, jump=5, params=None)</code>","text":"<p>Initialize a BottomUp instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if <code>'custom_cost'</code> is not None.</p> <code>'l2'</code> <code>custom_cost</code> <code>BaseCost</code> <p>custom cost function. Defaults to None.</p> <code>None</code> <code>min_size</code> <code>int</code> <p>minimum segment length. Defaults to 2 samples.</p> <code>2</code> <code>jump</code> <code>int</code> <p>subsample (one every jump points). Defaults to 5 samples.</p> <code>5</code> <code>params</code> <code>dict</code> <p>a dictionary of parameters for the cost instance.</p> <code>None</code> Source code in <code>ruptures/detection/bottomup.py</code> <pre><code>def __init__(self, model=\"l2\", custom_cost=None, min_size=2, jump=5, params=None):\n    \"\"\"Initialize a BottomUp instance.\n\n    Args:\n        model (str, optional): segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if ``'custom_cost'`` is not None.\n        custom_cost (BaseCost, optional): custom cost function. Defaults to None.\n        min_size (int, optional): minimum segment length. Defaults to 2 samples.\n        jump (int, optional): subsample (one every *jump* points). Defaults to 5 samples.\n        params (dict, optional): a dictionary of parameters for the cost instance.\n    \"\"\"\n    if custom_cost is not None and isinstance(custom_cost, BaseCost):\n        self.cost = custom_cost\n    else:\n        if params is None:\n            self.cost = cost_factory(model=model)\n        else:\n            self.cost = cost_factory(model=model, **params)\n    self.min_size = max(min_size, self.cost.min_size)\n    self.jump = jump\n    self.n_samples = None\n    self.signal = None\n    self.leaves = None\n</code></pre>"},{"location":"code-reference/detection/bottomup-reference/#ruptures.detection.bottomup.BottomUp.fit","title":"<code>fit(signal)</code>","text":"<p>Compute params to segment signal.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>signal to segment. Shape (n_samples, n_features) or (n_samples,).</p> required <p>Returns:</p> Type Description <code>BottomUp</code> <p>self</p> Source code in <code>ruptures/detection/bottomup.py</code> <pre><code>def fit(self, signal) -&gt; \"BottomUp\":\n    \"\"\"Compute params to segment signal.\n\n    Args:\n        signal (array): signal to segment. Shape (n_samples, n_features) or (n_samples,).\n\n    Returns:\n        self\n    \"\"\"\n    # update some params\n    self.cost.fit(signal)\n    self.merge.cache_clear()\n    if signal.ndim == 1:\n        (n_samples,) = signal.shape\n    else:\n        n_samples, _ = signal.shape\n    self.n_samples = n_samples\n    self.leaves = self._grow_tree()\n    return self\n</code></pre>"},{"location":"code-reference/detection/bottomup-reference/#ruptures.detection.bottomup.BottomUp.fit_predict","title":"<code>fit_predict(signal, n_bkps=None, pen=None, epsilon=None)</code>","text":"<p>Fit to the signal and return the optimal breakpoints.</p> <p>Helper method to call fit and predict once</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>signal. Shape (n_samples, n_features) or (n_samples,).</p> required <code>n_bkps</code> <code>int</code> <p>number of breakpoints.</p> <code>None</code> <code>pen</code> <code>float</code> <p>penalty value (&gt;0)</p> <code>None</code> <code>epsilon</code> <code>float</code> <p>reconstruction budget (&gt;0)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <p>sorted list of breakpoints</p> Source code in <code>ruptures/detection/bottomup.py</code> <pre><code>def fit_predict(self, signal, n_bkps=None, pen=None, epsilon=None):\n    \"\"\"Fit to the signal and return the optimal breakpoints.\n\n    Helper method to call fit and predict once\n\n    Args:\n        signal (array): signal. Shape (n_samples, n_features) or (n_samples,).\n        n_bkps (int): number of breakpoints.\n        pen (float): penalty value (&gt;0)\n        epsilon (float): reconstruction budget (&gt;0)\n\n    Returns:\n        list: sorted list of breakpoints\n    \"\"\"\n    self.fit(signal)\n    return self.predict(n_bkps=n_bkps, pen=pen, epsilon=epsilon)\n</code></pre>"},{"location":"code-reference/detection/bottomup-reference/#ruptures.detection.bottomup.BottomUp.merge","title":"<code>merge(left, right)</code>  <code>cached</code>","text":"<p>Merge two contiguous segments.</p> Source code in <code>ruptures/detection/bottomup.py</code> <pre><code>@lru_cache(maxsize=None)\ndef merge(self, left, right):\n    \"\"\"Merge two contiguous segments.\"\"\"\n    assert left.end == right.start, \"Segments are not contiguous.\"\n    start, end = left.start, right.end\n    val = self.cost.error(start, end)\n    node = Bnode(start, end, val, left=left, right=right)\n    return node\n</code></pre>"},{"location":"code-reference/detection/bottomup-reference/#ruptures.detection.bottomup.BottomUp.predict","title":"<code>predict(n_bkps=None, pen=None, epsilon=None)</code>","text":"<p>Return the optimal breakpoints.</p> <p>Must be called after the fit method. The breakpoints are associated with the signal passed to <code>fit()</code>. The stopping rule depends on the parameter passed to the function.</p> <p>Parameters:</p> Name Type Description Default <code>n_bkps</code> <code>int</code> <p>number of breakpoints to find before stopping.</p> <code>None</code> <code>pen</code> <code>float</code> <p>penalty value (&gt;0)</p> <code>None</code> <code>epsilon</code> <code>float</code> <p>reconstruction budget (&gt;0)</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if none of <code>n_bkps</code>, <code>pen</code>, <code>epsilon</code> is set.</p> <code>BadSegmentationParameters</code> <p>in case of impossible segmentation configuration</p> <p>Returns:</p> Name Type Description <code>list</code> <p>sorted list of breakpoints</p> Source code in <code>ruptures/detection/bottomup.py</code> <pre><code>def predict(self, n_bkps=None, pen=None, epsilon=None):\n    \"\"\"Return the optimal breakpoints.\n\n    Must be called after the fit method. The breakpoints are associated with the signal passed\n    to [`fit()`][ruptures.detection.bottomup.BottomUp.fit].\n    The stopping rule depends on the parameter passed to the function.\n\n    Args:\n        n_bkps (int): number of breakpoints to find before stopping.\n        pen (float): penalty value (&gt;0)\n        epsilon (float): reconstruction budget (&gt;0)\n\n    Raises:\n        AssertionError: if none of `n_bkps`, `pen`, `epsilon` is set.\n        BadSegmentationParameters: in case of impossible segmentation\n            configuration\n\n    Returns:\n        list: sorted list of breakpoints\n    \"\"\"\n    msg = \"Give a parameter.\"\n    assert any(param is not None for param in (n_bkps, pen, epsilon)), msg\n\n    # raise an exception in case of impossible segmentation configuration\n    if not sanity_check(\n        n_samples=self.cost.signal.shape[0],\n        n_bkps=0 if n_bkps is None else n_bkps,\n        jump=self.jump,\n        min_size=self.min_size,\n    ):\n        raise BadSegmentationParameters\n\n    partition = self._seg(n_bkps=n_bkps, pen=pen, epsilon=epsilon)\n    bkps = sorted(e for s, e in partition.keys())\n    return bkps\n</code></pre>"},{"location":"code-reference/detection/dynp-reference/","title":"Dynamic programming","text":"<p>             Bases: <code>BaseEstimator</code></p> <p>Find optimal change points using dynamic programming.</p> <p>Given a segment model, it computes the best partition for which the sum of errors is minimum.</p> Source code in <code>ruptures/detection/dynp.py</code> <pre><code>class Dynp(BaseEstimator):\n    \"\"\"Find optimal change points using dynamic programming.\n\n    Given a segment model, it computes the best partition for which the\n    sum of errors is minimum.\n    \"\"\"\n\n    def __init__(self, model=\"l2\", custom_cost=None, min_size=2, jump=5, params=None):\n        \"\"\"Creates a Dynp instance.\n\n        Args:\n            model (str, optional): segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if ``'custom_cost'`` is not None.\n            custom_cost (BaseCost, optional): custom cost function. Defaults to None.\n            min_size (int, optional): minimum segment length.\n            jump (int, optional): subsample (one every *jump* points).\n            params (dict, optional): a dictionary of parameters for the cost instance.\n        \"\"\"\n        if custom_cost is not None and isinstance(custom_cost, BaseCost):\n            self.cost = custom_cost\n        else:\n            self.model_name = model\n            if params is None:\n                self.cost = cost_factory(model=model)\n            else:\n                self.cost = cost_factory(model=model, **params)\n        self.min_size = max(min_size, self.cost.min_size)\n        self.jump = jump\n        self.n_samples = None\n\n    @lru_cache(maxsize=None)\n    def seg(self, start, end, n_bkps):\n        \"\"\"Recurrence to find the optimal partition of signal[start:end].\n\n        This method is to be memoized and then used.\n\n        Args:\n            start (int):\u00a0start of the segment (inclusive)\n            end (int): end of the segment (exclusive)\n            n_bkps (int): number of breakpoints\n\n        Returns:\n            dict: {(start, end): cost value, ...}\n        \"\"\"\n        jump, min_size = self.jump, self.min_size\n\n        if n_bkps == 0:\n            cost = self.cost.error(start, end)\n            return {(start, end): cost}\n        elif n_bkps &gt; 0:\n            # Let's fill the list of admissible last breakpoints\n            multiple_of_jump = (k for k in range(start, end) if k % jump == 0)\n            admissible_bkps = list()\n            for bkp in multiple_of_jump:\n                n_samples = bkp - start\n                # first check if left subproblem is possible\n                if sanity_check(\n                    n_samples=n_samples,\n                    n_bkps=n_bkps - 1,\n                    jump=jump,\n                    min_size=min_size,\n                ):\n                    # second check if the right subproblem has enough points\n                    if end - bkp &gt;= min_size:\n                        admissible_bkps.append(bkp)\n\n            assert (\n                len(admissible_bkps) &gt; 0\n            ), \"No admissible last breakpoints found.\\\n             start, end: ({},{}), n_bkps: {}.\".format(\n                start, end, n_bkps\n            )\n\n            # Compute the subproblems\n            sub_problems = list()\n            for bkp in admissible_bkps:\n                left_partition = self.seg(start, bkp, n_bkps - 1)\n                right_partition = self.seg(bkp, end, 0)\n                tmp_partition = dict(left_partition)\n                tmp_partition[(bkp, end)] = right_partition[(bkp, end)]\n                sub_problems.append(tmp_partition)\n\n            # Find the optimal partition\n            return min(sub_problems, key=lambda d: sum(d.values()))\n\n    def fit(self, signal) -&gt; \"Dynp\":\n        \"\"\"Create the cache associated with the signal.\n\n        Dynamic programming is a recurrence; intermediate results are cached to speed up\n        computations. This method sets up the cache.\n\n        Args:\n            signal (array): signal. Shape (n_samples, n_features) or (n_samples,).\n\n        Returns:\n            self\n        \"\"\"\n        # clear cache\n        self.seg.cache_clear()\n        # update some params\n        self.cost.fit(signal)\n        self.n_samples = signal.shape[0]\n        return self\n\n    def predict(self, n_bkps):\n        \"\"\"Return the optimal breakpoints.\n\n        Must be called after the fit method. The breakpoints are associated with the signal passed\n        to [`fit()`][ruptures.detection.dynp.Dynp.fit].\n\n        Args:\n            n_bkps (int): number of breakpoints.\n\n        Raises:\n            BadSegmentationParameters: in case of impossible segmentation\n                configuration\n\n        Returns:\n            list: sorted list of breakpoints\n        \"\"\"\n        # raise an exception in case of impossible segmentation configuration\n        if not sanity_check(\n            n_samples=self.cost.signal.shape[0],\n            n_bkps=n_bkps,\n            jump=self.jump,\n            min_size=self.min_size,\n        ):\n            raise BadSegmentationParameters\n        partition = self.seg(0, self.n_samples, n_bkps)\n        bkps = sorted(e for s, e in partition.keys())\n        return bkps\n\n    def fit_predict(self, signal, n_bkps):\n        \"\"\"Fit to the signal and return the optimal breakpoints.\n\n        Helper method to call fit and predict once\n\n        Args:\n            signal (array): signal. Shape (n_samples, n_features) or (n_samples,).\n            n_bkps (int): number of breakpoints.\n\n        Returns:\n            list: sorted list of breakpoints\n        \"\"\"\n        self.fit(signal)\n        return self.predict(n_bkps)\n</code></pre>"},{"location":"code-reference/detection/dynp-reference/#ruptures.detection.dynp.Dynp.__init__","title":"<code>__init__(model='l2', custom_cost=None, min_size=2, jump=5, params=None)</code>","text":"<p>Creates a Dynp instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if <code>'custom_cost'</code> is not None.</p> <code>'l2'</code> <code>custom_cost</code> <code>BaseCost</code> <p>custom cost function. Defaults to None.</p> <code>None</code> <code>min_size</code> <code>int</code> <p>minimum segment length.</p> <code>2</code> <code>jump</code> <code>int</code> <p>subsample (one every jump points).</p> <code>5</code> <code>params</code> <code>dict</code> <p>a dictionary of parameters for the cost instance.</p> <code>None</code> Source code in <code>ruptures/detection/dynp.py</code> <pre><code>def __init__(self, model=\"l2\", custom_cost=None, min_size=2, jump=5, params=None):\n    \"\"\"Creates a Dynp instance.\n\n    Args:\n        model (str, optional): segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if ``'custom_cost'`` is not None.\n        custom_cost (BaseCost, optional): custom cost function. Defaults to None.\n        min_size (int, optional): minimum segment length.\n        jump (int, optional): subsample (one every *jump* points).\n        params (dict, optional): a dictionary of parameters for the cost instance.\n    \"\"\"\n    if custom_cost is not None and isinstance(custom_cost, BaseCost):\n        self.cost = custom_cost\n    else:\n        self.model_name = model\n        if params is None:\n            self.cost = cost_factory(model=model)\n        else:\n            self.cost = cost_factory(model=model, **params)\n    self.min_size = max(min_size, self.cost.min_size)\n    self.jump = jump\n    self.n_samples = None\n</code></pre>"},{"location":"code-reference/detection/dynp-reference/#ruptures.detection.dynp.Dynp.fit","title":"<code>fit(signal)</code>","text":"<p>Create the cache associated with the signal.</p> <p>Dynamic programming is a recurrence; intermediate results are cached to speed up computations. This method sets up the cache.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>signal. Shape (n_samples, n_features) or (n_samples,).</p> required <p>Returns:</p> Type Description <code>Dynp</code> <p>self</p> Source code in <code>ruptures/detection/dynp.py</code> <pre><code>def fit(self, signal) -&gt; \"Dynp\":\n    \"\"\"Create the cache associated with the signal.\n\n    Dynamic programming is a recurrence; intermediate results are cached to speed up\n    computations. This method sets up the cache.\n\n    Args:\n        signal (array): signal. Shape (n_samples, n_features) or (n_samples,).\n\n    Returns:\n        self\n    \"\"\"\n    # clear cache\n    self.seg.cache_clear()\n    # update some params\n    self.cost.fit(signal)\n    self.n_samples = signal.shape[0]\n    return self\n</code></pre>"},{"location":"code-reference/detection/dynp-reference/#ruptures.detection.dynp.Dynp.fit_predict","title":"<code>fit_predict(signal, n_bkps)</code>","text":"<p>Fit to the signal and return the optimal breakpoints.</p> <p>Helper method to call fit and predict once</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>signal. Shape (n_samples, n_features) or (n_samples,).</p> required <code>n_bkps</code> <code>int</code> <p>number of breakpoints.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>sorted list of breakpoints</p> Source code in <code>ruptures/detection/dynp.py</code> <pre><code>def fit_predict(self, signal, n_bkps):\n    \"\"\"Fit to the signal and return the optimal breakpoints.\n\n    Helper method to call fit and predict once\n\n    Args:\n        signal (array): signal. Shape (n_samples, n_features) or (n_samples,).\n        n_bkps (int): number of breakpoints.\n\n    Returns:\n        list: sorted list of breakpoints\n    \"\"\"\n    self.fit(signal)\n    return self.predict(n_bkps)\n</code></pre>"},{"location":"code-reference/detection/dynp-reference/#ruptures.detection.dynp.Dynp.predict","title":"<code>predict(n_bkps)</code>","text":"<p>Return the optimal breakpoints.</p> <p>Must be called after the fit method. The breakpoints are associated with the signal passed to <code>fit()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>n_bkps</code> <code>int</code> <p>number of breakpoints.</p> required <p>Raises:</p> Type Description <code>BadSegmentationParameters</code> <p>in case of impossible segmentation configuration</p> <p>Returns:</p> Name Type Description <code>list</code> <p>sorted list of breakpoints</p> Source code in <code>ruptures/detection/dynp.py</code> <pre><code>def predict(self, n_bkps):\n    \"\"\"Return the optimal breakpoints.\n\n    Must be called after the fit method. The breakpoints are associated with the signal passed\n    to [`fit()`][ruptures.detection.dynp.Dynp.fit].\n\n    Args:\n        n_bkps (int): number of breakpoints.\n\n    Raises:\n        BadSegmentationParameters: in case of impossible segmentation\n            configuration\n\n    Returns:\n        list: sorted list of breakpoints\n    \"\"\"\n    # raise an exception in case of impossible segmentation configuration\n    if not sanity_check(\n        n_samples=self.cost.signal.shape[0],\n        n_bkps=n_bkps,\n        jump=self.jump,\n        min_size=self.min_size,\n    ):\n        raise BadSegmentationParameters\n    partition = self.seg(0, self.n_samples, n_bkps)\n    bkps = sorted(e for s, e in partition.keys())\n    return bkps\n</code></pre>"},{"location":"code-reference/detection/dynp-reference/#ruptures.detection.dynp.Dynp.seg","title":"<code>seg(start, end, n_bkps)</code>  <code>cached</code>","text":"<p>Recurrence to find the optimal partition of signal[start:end].</p> <p>This method is to be memoized and then used.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>start of the segment (inclusive)</p> required <code>end</code> <code>int</code> <p>end of the segment (exclusive)</p> required <code>n_bkps</code> <code>int</code> <p>number of breakpoints</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>{(start, end): cost value, ...}</p> Source code in <code>ruptures/detection/dynp.py</code> <pre><code>@lru_cache(maxsize=None)\ndef seg(self, start, end, n_bkps):\n    \"\"\"Recurrence to find the optimal partition of signal[start:end].\n\n    This method is to be memoized and then used.\n\n    Args:\n        start (int):\u00a0start of the segment (inclusive)\n        end (int): end of the segment (exclusive)\n        n_bkps (int): number of breakpoints\n\n    Returns:\n        dict: {(start, end): cost value, ...}\n    \"\"\"\n    jump, min_size = self.jump, self.min_size\n\n    if n_bkps == 0:\n        cost = self.cost.error(start, end)\n        return {(start, end): cost}\n    elif n_bkps &gt; 0:\n        # Let's fill the list of admissible last breakpoints\n        multiple_of_jump = (k for k in range(start, end) if k % jump == 0)\n        admissible_bkps = list()\n        for bkp in multiple_of_jump:\n            n_samples = bkp - start\n            # first check if left subproblem is possible\n            if sanity_check(\n                n_samples=n_samples,\n                n_bkps=n_bkps - 1,\n                jump=jump,\n                min_size=min_size,\n            ):\n                # second check if the right subproblem has enough points\n                if end - bkp &gt;= min_size:\n                    admissible_bkps.append(bkp)\n\n        assert (\n            len(admissible_bkps) &gt; 0\n        ), \"No admissible last breakpoints found.\\\n         start, end: ({},{}), n_bkps: {}.\".format(\n            start, end, n_bkps\n        )\n\n        # Compute the subproblems\n        sub_problems = list()\n        for bkp in admissible_bkps:\n            left_partition = self.seg(start, bkp, n_bkps - 1)\n            right_partition = self.seg(bkp, end, 0)\n            tmp_partition = dict(left_partition)\n            tmp_partition[(bkp, end)] = right_partition[(bkp, end)]\n            sub_problems.append(tmp_partition)\n\n        # Find the optimal partition\n        return min(sub_problems, key=lambda d: sum(d.values()))\n</code></pre>"},{"location":"code-reference/detection/kernelcpd-reference/","title":"Efficient kernel change point detection","text":"<p>             Bases: <code>BaseEstimator</code></p> <p>Find optimal change points (using dynamic programming or pelt) for the special case where the cost function derives from a kernel function.</p> <p>Given a segment model, it computes the best partition for which the sum of errors is minimum.</p> <p>See the user guide for more information.</p> Source code in <code>ruptures/detection/kernelcpd.py</code> <pre><code>class KernelCPD(BaseEstimator):\n    \"\"\"Find optimal change points (using dynamic programming or pelt) for the\n    special case where the cost function derives from a kernel function.\n\n    Given a segment model, it computes the best partition for which the\n    sum of errors is minimum.\n\n    See the [user guide](../../../user-guide/detection/kernelcpd) for\n    more information.\n    \"\"\"\n\n    def __init__(self, kernel=\"linear\", min_size=2, jump=1, params=None):\n        r\"\"\"Creates a KernelCPD instance.\n\n        Available kernels:\n\n        - `linear`: $k(x,y) = x^T y$.\n        - `rbf`: $k(x, y) = exp(\\gamma \\|x-y\\|^2)$ where $\\gamma&gt;0$\n        (`gamma`) is a user-defined parameter.\n        - `cosine`: $k(x,y)= (x^T y)/(\\|x\\|\\|y\\|)$.\n\n        Args:\n            kernel (str, optional): name of the kernel, [\"linear\", \"rbf\", \"cosine\"]\n            min_size (int, optional): minimum segment length.\n            jump (int, optional): not considered, set to 1.\n            params (dict, optional): a dictionary of parameters for the kernel instance\n\n        Raises:\n            AssertionError: if the kernel is not implemented.\n        \"\"\"\n        self.kernel_name = kernel\n        err_msg = \"Kernel not found: {}.\".format(self.kernel_name)\n        assert self.kernel_name in [\"linear\", \"rbf\", \"cosine\"], err_msg\n        self.model_name = \"l2\" if self.kernel_name == \"linear\" else self.kernel_name\n        self.params = params\n        # load the associated cost function\n        if self.params is None:\n            self.cost = cost_factory(model=self.model_name)\n        else:\n            self.cost = cost_factory(model=self.model_name, **self.params)\n        self.min_size = max(min_size, self.cost.min_size)\n\n        self.jump = 1  # set to 1\n        self.n_samples = None\n        self.segmentations_dict = dict()  # {n_bkps: bkps_list}\n\n    def fit(self, signal) -&gt; \"KernelCPD\":\n        \"\"\"Update some parameters (no computation in this function).\n\n        Args:\n            signal (array): signal. Shape (n_samples, n_features) or (n_samples,).\n\n        Returns:\n            self\n        \"\"\"\n        # update some params\n        self.segmentations_dict = dict()\n        self.cost.fit(signal.astype(np.double))\n        self.n_samples = signal.shape[0]\n        return self\n\n    def predict(self, n_bkps=None, pen=None):\n        \"\"\"Return the optimal breakpoints. Must be called after the fit method.\n\n        The breakpoints are associated with the signal passed to\n        [`fit()`][ruptures.detection.kernelcpd.KernelCPD.fit].\n\n        Args:\n            n_bkps (int, optional): Number of change points. Defaults to None.\n            pen (float, optional): penalty value (&gt;0). Defaults to None. Not considered\n                if n_bkps is not None.\n\n        Raises:\n            AssertionError: if `pen` or `n_bkps` is not strictly positive.\n            BadSegmentationParameters: in case of impossible segmentation\n                configuration\n\n        Returns:\n            list[int]: sorted list of breakpoints\n        \"\"\"\n        # Our KernelCPD implementation with Pelt implies that we have at least one change point\n        # raise an exception in case of impossible segmentation configuration\n        if not sanity_check(\n            n_samples=self.cost.signal.shape[0],\n            n_bkps=1 if n_bkps is None else n_bkps,\n            jump=self.jump,\n            min_size=self.min_size,\n        ):\n            raise BadSegmentationParameters\n\n        # dynamic programming if the user passed a number change points\n        if n_bkps is not None:\n            n_bkps = int(n_bkps)\n            err_msg = \"The number of changes must be positive: {}\".format(n_bkps)\n            assert n_bkps &gt; 0, err_msg\n            # if we have already computed it, return it without computations.\n            if n_bkps in self.segmentations_dict:\n                return self.segmentations_dict[n_bkps]\n            # otherwise, call the C function\n            if self.kernel_name == \"linear\":\n                path_matrix_flat = ekcpd_L2(self.cost.signal, n_bkps, self.min_size)\n            elif self.kernel_name == \"rbf\":\n                path_matrix_flat = ekcpd_Gaussian(\n                    self.cost.signal, n_bkps, self.min_size, self.cost.gamma\n                )\n            elif self.kernel_name == \"cosine\":\n                path_matrix_flat = ekcpd_cosine(self.cost.signal, n_bkps, self.min_size)\n            # from the path matrix, get all segmentation for k=1,...,n_bkps changes\n            for k in range(1, n_bkps + 1):\n                self.segmentations_dict[k] = from_path_matrix_to_bkps_list(\n                    path_matrix_flat, k, self.n_samples, n_bkps, self.jump\n                )\n            return self.segmentations_dict[n_bkps]\n\n        # Call pelt if the user passed a penalty\n        if pen is not None:\n            assert pen &gt; 0, \"The penalty must be positive: {}\".format(pen)\n            if self.kernel_name == \"linear\":\n                path_matrix = ekcpd_pelt_L2(self.cost.signal, pen, self.min_size)\n            elif self.kernel_name == \"rbf\":\n                path_matrix = ekcpd_pelt_Gaussian(\n                    self.cost.signal, pen, self.min_size, self.cost.gamma\n                )\n            elif self.kernel_name == \"cosine\":\n                path_matrix = ekcpd_pelt_cosine(self.cost.signal, pen, self.min_size)\n\n            my_bkps = list()\n            ind = self.n_samples\n            while ind &gt; 0:\n                my_bkps.append(ind)\n                ind = path_matrix[ind]\n            return my_bkps[::-1]\n\n    def fit_predict(self, signal, n_bkps=None, pen=None):\n        \"\"\"Fit to the signal and return the optimal breakpoints.\n\n        Helper method to call fit and predict once\n\n        Args:\n            signal (array): signal. Shape (n_samples, n_features) or (n_samples,).\n            n_bkps (int, optional): Number of change points. Defaults to None.\n            pen (float, optional): penalty value (&gt;0). Defaults to None. Not considered if n_bkps is not None.\n\n        Returns:\n            list: sorted list of breakpoints\n        \"\"\"\n        self.fit(signal)\n        return self.predict(n_bkps=n_bkps, pen=pen)\n</code></pre>"},{"location":"code-reference/detection/kernelcpd-reference/#ruptures.detection.kernelcpd.KernelCPD.__init__","title":"<code>__init__(kernel='linear', min_size=2, jump=1, params=None)</code>","text":"<p>Creates a KernelCPD instance.</p> <p>Available kernels:</p> <ul> <li><code>linear</code>: \\(k(x,y) = x^T y\\).</li> <li><code>rbf</code>: \\(k(x, y) = exp(\\gamma \\|x-y\\|^2)\\) where \\(\\gamma&gt;0\\) (<code>gamma</code>) is a user-defined parameter.</li> <li><code>cosine</code>: \\(k(x,y)= (x^T y)/(\\|x\\|\\|y\\|)\\).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>str</code> <p>name of the kernel, [\"linear\", \"rbf\", \"cosine\"]</p> <code>'linear'</code> <code>min_size</code> <code>int</code> <p>minimum segment length.</p> <code>2</code> <code>jump</code> <code>int</code> <p>not considered, set to 1.</p> <code>1</code> <code>params</code> <code>dict</code> <p>a dictionary of parameters for the kernel instance</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if the kernel is not implemented.</p> Source code in <code>ruptures/detection/kernelcpd.py</code> <pre><code>def __init__(self, kernel=\"linear\", min_size=2, jump=1, params=None):\n    r\"\"\"Creates a KernelCPD instance.\n\n    Available kernels:\n\n    - `linear`: $k(x,y) = x^T y$.\n    - `rbf`: $k(x, y) = exp(\\gamma \\|x-y\\|^2)$ where $\\gamma&gt;0$\n    (`gamma`) is a user-defined parameter.\n    - `cosine`: $k(x,y)= (x^T y)/(\\|x\\|\\|y\\|)$.\n\n    Args:\n        kernel (str, optional): name of the kernel, [\"linear\", \"rbf\", \"cosine\"]\n        min_size (int, optional): minimum segment length.\n        jump (int, optional): not considered, set to 1.\n        params (dict, optional): a dictionary of parameters for the kernel instance\n\n    Raises:\n        AssertionError: if the kernel is not implemented.\n    \"\"\"\n    self.kernel_name = kernel\n    err_msg = \"Kernel not found: {}.\".format(self.kernel_name)\n    assert self.kernel_name in [\"linear\", \"rbf\", \"cosine\"], err_msg\n    self.model_name = \"l2\" if self.kernel_name == \"linear\" else self.kernel_name\n    self.params = params\n    # load the associated cost function\n    if self.params is None:\n        self.cost = cost_factory(model=self.model_name)\n    else:\n        self.cost = cost_factory(model=self.model_name, **self.params)\n    self.min_size = max(min_size, self.cost.min_size)\n\n    self.jump = 1  # set to 1\n    self.n_samples = None\n    self.segmentations_dict = dict()  # {n_bkps: bkps_list}\n</code></pre>"},{"location":"code-reference/detection/kernelcpd-reference/#ruptures.detection.kernelcpd.KernelCPD.fit","title":"<code>fit(signal)</code>","text":"<p>Update some parameters (no computation in this function).</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>signal. Shape (n_samples, n_features) or (n_samples,).</p> required <p>Returns:</p> Type Description <code>KernelCPD</code> <p>self</p> Source code in <code>ruptures/detection/kernelcpd.py</code> <pre><code>def fit(self, signal) -&gt; \"KernelCPD\":\n    \"\"\"Update some parameters (no computation in this function).\n\n    Args:\n        signal (array): signal. Shape (n_samples, n_features) or (n_samples,).\n\n    Returns:\n        self\n    \"\"\"\n    # update some params\n    self.segmentations_dict = dict()\n    self.cost.fit(signal.astype(np.double))\n    self.n_samples = signal.shape[0]\n    return self\n</code></pre>"},{"location":"code-reference/detection/kernelcpd-reference/#ruptures.detection.kernelcpd.KernelCPD.fit_predict","title":"<code>fit_predict(signal, n_bkps=None, pen=None)</code>","text":"<p>Fit to the signal and return the optimal breakpoints.</p> <p>Helper method to call fit and predict once</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>signal. Shape (n_samples, n_features) or (n_samples,).</p> required <code>n_bkps</code> <code>int</code> <p>Number of change points. Defaults to None.</p> <code>None</code> <code>pen</code> <code>float</code> <p>penalty value (&gt;0). Defaults to None. Not considered if n_bkps is not None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <p>sorted list of breakpoints</p> Source code in <code>ruptures/detection/kernelcpd.py</code> <pre><code>def fit_predict(self, signal, n_bkps=None, pen=None):\n    \"\"\"Fit to the signal and return the optimal breakpoints.\n\n    Helper method to call fit and predict once\n\n    Args:\n        signal (array): signal. Shape (n_samples, n_features) or (n_samples,).\n        n_bkps (int, optional): Number of change points. Defaults to None.\n        pen (float, optional): penalty value (&gt;0). Defaults to None. Not considered if n_bkps is not None.\n\n    Returns:\n        list: sorted list of breakpoints\n    \"\"\"\n    self.fit(signal)\n    return self.predict(n_bkps=n_bkps, pen=pen)\n</code></pre>"},{"location":"code-reference/detection/kernelcpd-reference/#ruptures.detection.kernelcpd.KernelCPD.predict","title":"<code>predict(n_bkps=None, pen=None)</code>","text":"<p>Return the optimal breakpoints. Must be called after the fit method.</p> <p>The breakpoints are associated with the signal passed to <code>fit()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>n_bkps</code> <code>int</code> <p>Number of change points. Defaults to None.</p> <code>None</code> <code>pen</code> <code>float</code> <p>penalty value (&gt;0). Defaults to None. Not considered if n_bkps is not None.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if <code>pen</code> or <code>n_bkps</code> is not strictly positive.</p> <code>BadSegmentationParameters</code> <p>in case of impossible segmentation configuration</p> <p>Returns:</p> Type Description <p>list[int]: sorted list of breakpoints</p> Source code in <code>ruptures/detection/kernelcpd.py</code> <pre><code>def predict(self, n_bkps=None, pen=None):\n    \"\"\"Return the optimal breakpoints. Must be called after the fit method.\n\n    The breakpoints are associated with the signal passed to\n    [`fit()`][ruptures.detection.kernelcpd.KernelCPD.fit].\n\n    Args:\n        n_bkps (int, optional): Number of change points. Defaults to None.\n        pen (float, optional): penalty value (&gt;0). Defaults to None. Not considered\n            if n_bkps is not None.\n\n    Raises:\n        AssertionError: if `pen` or `n_bkps` is not strictly positive.\n        BadSegmentationParameters: in case of impossible segmentation\n            configuration\n\n    Returns:\n        list[int]: sorted list of breakpoints\n    \"\"\"\n    # Our KernelCPD implementation with Pelt implies that we have at least one change point\n    # raise an exception in case of impossible segmentation configuration\n    if not sanity_check(\n        n_samples=self.cost.signal.shape[0],\n        n_bkps=1 if n_bkps is None else n_bkps,\n        jump=self.jump,\n        min_size=self.min_size,\n    ):\n        raise BadSegmentationParameters\n\n    # dynamic programming if the user passed a number change points\n    if n_bkps is not None:\n        n_bkps = int(n_bkps)\n        err_msg = \"The number of changes must be positive: {}\".format(n_bkps)\n        assert n_bkps &gt; 0, err_msg\n        # if we have already computed it, return it without computations.\n        if n_bkps in self.segmentations_dict:\n            return self.segmentations_dict[n_bkps]\n        # otherwise, call the C function\n        if self.kernel_name == \"linear\":\n            path_matrix_flat = ekcpd_L2(self.cost.signal, n_bkps, self.min_size)\n        elif self.kernel_name == \"rbf\":\n            path_matrix_flat = ekcpd_Gaussian(\n                self.cost.signal, n_bkps, self.min_size, self.cost.gamma\n            )\n        elif self.kernel_name == \"cosine\":\n            path_matrix_flat = ekcpd_cosine(self.cost.signal, n_bkps, self.min_size)\n        # from the path matrix, get all segmentation for k=1,...,n_bkps changes\n        for k in range(1, n_bkps + 1):\n            self.segmentations_dict[k] = from_path_matrix_to_bkps_list(\n                path_matrix_flat, k, self.n_samples, n_bkps, self.jump\n            )\n        return self.segmentations_dict[n_bkps]\n\n    # Call pelt if the user passed a penalty\n    if pen is not None:\n        assert pen &gt; 0, \"The penalty must be positive: {}\".format(pen)\n        if self.kernel_name == \"linear\":\n            path_matrix = ekcpd_pelt_L2(self.cost.signal, pen, self.min_size)\n        elif self.kernel_name == \"rbf\":\n            path_matrix = ekcpd_pelt_Gaussian(\n                self.cost.signal, pen, self.min_size, self.cost.gamma\n            )\n        elif self.kernel_name == \"cosine\":\n            path_matrix = ekcpd_pelt_cosine(self.cost.signal, pen, self.min_size)\n\n        my_bkps = list()\n        ind = self.n_samples\n        while ind &gt; 0:\n            my_bkps.append(ind)\n            ind = path_matrix[ind]\n        return my_bkps[::-1]\n</code></pre>"},{"location":"code-reference/detection/pelt-reference/","title":"Pelt","text":"<p>             Bases: <code>BaseEstimator</code></p> <p>Penalized change point detection.</p> <p>For a given model and penalty level, computes the segmentation which minimizes the constrained sum of approximation errors.</p> Source code in <code>ruptures/detection/pelt.py</code> <pre><code>class Pelt(BaseEstimator):\n    \"\"\"Penalized change point detection.\n\n    For a given model and penalty level, computes the segmentation which\n    minimizes the constrained sum of approximation errors.\n    \"\"\"\n\n    def __init__(self, model=\"l2\", custom_cost=None, min_size=2, jump=5, params=None):\n        \"\"\"Initialize a Pelt instance.\n\n        Args:\n            model (str, optional): segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if ``'custom_cost'`` is not None.\n            custom_cost (BaseCost, optional): custom cost function. Defaults to None.\n            min_size (int, optional): minimum segment length.\n            jump (int, optional): subsample (one every *jump* points).\n            params (dict, optional): a dictionary of parameters for the cost instance.\n        \"\"\"\n        if custom_cost is not None and isinstance(custom_cost, BaseCost):\n            self.cost = custom_cost\n        else:\n            if params is None:\n                self.cost = cost_factory(model=model)\n            else:\n                self.cost = cost_factory(model=model, **params)\n        self.min_size = max(min_size, self.cost.min_size)\n        self.jump = jump\n        self.n_samples = None\n\n    def _seg(self, pen):\n        \"\"\"Computes the segmentation for a given penalty using PELT (or a list\n        of penalties).\n\n        Args:\n            penalty (float): penalty value\n\n        Returns:\n            dict: partition dict {(start, end): cost value,...}\n        \"\"\"\n        # initialization\n        # partitions[t] contains the optimal partition of signal[0:t]\n        partitions = dict()  # this dict will be recursively filled\n        partitions[0] = {(0, 0): 0}\n        admissible = []\n\n        # Recursion\n        ind = [k for k in range(0, self.n_samples, self.jump) if k &gt;= self.min_size]\n        ind += [self.n_samples]\n        for bkp in ind:\n            # adding a point to the admissible set from the previous loop.\n            new_adm_pt = floor((bkp - self.min_size) / self.jump)\n            new_adm_pt *= self.jump\n            admissible.append(new_adm_pt)\n\n            subproblems = list()\n            for t in admissible:\n                # left partition\n                try:\n                    tmp_partition = partitions[t].copy()\n                except KeyError:  # no partition of 0:t exists\n                    continue\n                # we update with the right partition\n                tmp_partition.update({(t, bkp): self.cost.error(t, bkp) + pen})\n                subproblems.append(tmp_partition)\n\n            # finding the optimal partition\n            partitions[bkp] = min(subproblems, key=lambda d: sum(d.values()))\n            # trimming the admissible set\n            admissible = [\n                t\n                for t, partition in zip(admissible, subproblems)\n                if sum(partition.values()) &lt;= sum(partitions[bkp].values()) + pen\n            ]\n\n        best_partition = partitions[self.n_samples]\n        del best_partition[(0, 0)]\n        return best_partition\n\n    def fit(self, signal) -&gt; \"Pelt\":\n        \"\"\"Set params.\n\n        Args:\n            signal (array): signal to segment. Shape (n_samples, n_features) or (n_samples,).\n\n        Returns:\n            self\n        \"\"\"\n        # update params\n        self.cost.fit(signal)\n        if signal.ndim == 1:\n            (n_samples,) = signal.shape\n        else:\n            n_samples, _ = signal.shape\n        self.n_samples = n_samples\n        return self\n\n    def predict(self, pen):\n        \"\"\"Return the optimal breakpoints.\n\n        Must be called after the fit method. The breakpoints are associated with the signal passed\n        to [`fit()`][ruptures.detection.pelt.Pelt.fit].\n\n        Args:\n            pen (float): penalty value (&gt;0)\n\n        Raises:\n            BadSegmentationParameters: in case of impossible segmentation\n                configuration\n\n        Returns:\n            list: sorted list of breakpoints\n        \"\"\"\n        # raise an exception in case of impossible segmentation configuration\n        if not sanity_check(\n            n_samples=self.cost.signal.shape[0],\n            n_bkps=0,\n            jump=self.jump,\n            min_size=self.min_size,\n        ):\n            raise BadSegmentationParameters\n\n        partition = self._seg(pen)\n        bkps = sorted(e for s, e in partition.keys())\n        return bkps\n\n    def fit_predict(self, signal, pen):\n        \"\"\"Fit to the signal and return the optimal breakpoints.\n\n        Helper method to call fit and predict once\n\n        Args:\n            signal (array): signal. Shape (n_samples, n_features) or (n_samples,).\n            pen (float): penalty value (&gt;0)\n\n        Returns:\n            list: sorted list of breakpoints\n        \"\"\"\n        self.fit(signal)\n        return self.predict(pen)\n</code></pre>"},{"location":"code-reference/detection/pelt-reference/#ruptures.detection.pelt.Pelt.__init__","title":"<code>__init__(model='l2', custom_cost=None, min_size=2, jump=5, params=None)</code>","text":"<p>Initialize a Pelt instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if <code>'custom_cost'</code> is not None.</p> <code>'l2'</code> <code>custom_cost</code> <code>BaseCost</code> <p>custom cost function. Defaults to None.</p> <code>None</code> <code>min_size</code> <code>int</code> <p>minimum segment length.</p> <code>2</code> <code>jump</code> <code>int</code> <p>subsample (one every jump points).</p> <code>5</code> <code>params</code> <code>dict</code> <p>a dictionary of parameters for the cost instance.</p> <code>None</code> Source code in <code>ruptures/detection/pelt.py</code> <pre><code>def __init__(self, model=\"l2\", custom_cost=None, min_size=2, jump=5, params=None):\n    \"\"\"Initialize a Pelt instance.\n\n    Args:\n        model (str, optional): segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if ``'custom_cost'`` is not None.\n        custom_cost (BaseCost, optional): custom cost function. Defaults to None.\n        min_size (int, optional): minimum segment length.\n        jump (int, optional): subsample (one every *jump* points).\n        params (dict, optional): a dictionary of parameters for the cost instance.\n    \"\"\"\n    if custom_cost is not None and isinstance(custom_cost, BaseCost):\n        self.cost = custom_cost\n    else:\n        if params is None:\n            self.cost = cost_factory(model=model)\n        else:\n            self.cost = cost_factory(model=model, **params)\n    self.min_size = max(min_size, self.cost.min_size)\n    self.jump = jump\n    self.n_samples = None\n</code></pre>"},{"location":"code-reference/detection/pelt-reference/#ruptures.detection.pelt.Pelt.fit","title":"<code>fit(signal)</code>","text":"<p>Set params.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>signal to segment. Shape (n_samples, n_features) or (n_samples,).</p> required <p>Returns:</p> Type Description <code>Pelt</code> <p>self</p> Source code in <code>ruptures/detection/pelt.py</code> <pre><code>def fit(self, signal) -&gt; \"Pelt\":\n    \"\"\"Set params.\n\n    Args:\n        signal (array): signal to segment. Shape (n_samples, n_features) or (n_samples,).\n\n    Returns:\n        self\n    \"\"\"\n    # update params\n    self.cost.fit(signal)\n    if signal.ndim == 1:\n        (n_samples,) = signal.shape\n    else:\n        n_samples, _ = signal.shape\n    self.n_samples = n_samples\n    return self\n</code></pre>"},{"location":"code-reference/detection/pelt-reference/#ruptures.detection.pelt.Pelt.fit_predict","title":"<code>fit_predict(signal, pen)</code>","text":"<p>Fit to the signal and return the optimal breakpoints.</p> <p>Helper method to call fit and predict once</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>signal. Shape (n_samples, n_features) or (n_samples,).</p> required <code>pen</code> <code>float</code> <p>penalty value (&gt;0)</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>sorted list of breakpoints</p> Source code in <code>ruptures/detection/pelt.py</code> <pre><code>def fit_predict(self, signal, pen):\n    \"\"\"Fit to the signal and return the optimal breakpoints.\n\n    Helper method to call fit and predict once\n\n    Args:\n        signal (array): signal. Shape (n_samples, n_features) or (n_samples,).\n        pen (float): penalty value (&gt;0)\n\n    Returns:\n        list: sorted list of breakpoints\n    \"\"\"\n    self.fit(signal)\n    return self.predict(pen)\n</code></pre>"},{"location":"code-reference/detection/pelt-reference/#ruptures.detection.pelt.Pelt.predict","title":"<code>predict(pen)</code>","text":"<p>Return the optimal breakpoints.</p> <p>Must be called after the fit method. The breakpoints are associated with the signal passed to <code>fit()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pen</code> <code>float</code> <p>penalty value (&gt;0)</p> required <p>Raises:</p> Type Description <code>BadSegmentationParameters</code> <p>in case of impossible segmentation configuration</p> <p>Returns:</p> Name Type Description <code>list</code> <p>sorted list of breakpoints</p> Source code in <code>ruptures/detection/pelt.py</code> <pre><code>def predict(self, pen):\n    \"\"\"Return the optimal breakpoints.\n\n    Must be called after the fit method. The breakpoints are associated with the signal passed\n    to [`fit()`][ruptures.detection.pelt.Pelt.fit].\n\n    Args:\n        pen (float): penalty value (&gt;0)\n\n    Raises:\n        BadSegmentationParameters: in case of impossible segmentation\n            configuration\n\n    Returns:\n        list: sorted list of breakpoints\n    \"\"\"\n    # raise an exception in case of impossible segmentation configuration\n    if not sanity_check(\n        n_samples=self.cost.signal.shape[0],\n        n_bkps=0,\n        jump=self.jump,\n        min_size=self.min_size,\n    ):\n        raise BadSegmentationParameters\n\n    partition = self._seg(pen)\n    bkps = sorted(e for s, e in partition.keys())\n    return bkps\n</code></pre>"},{"location":"code-reference/detection/window-reference/","title":"Window-based change point detection","text":"<p>             Bases: <code>BaseEstimator</code></p> <p>Window sliding method.</p> Source code in <code>ruptures/detection/window.py</code> <pre><code>class Window(BaseEstimator):\n    \"\"\"Window sliding method.\"\"\"\n\n    def __init__(\n        self, width=100, model=\"l2\", custom_cost=None, min_size=2, jump=5, params=None\n    ):\n        \"\"\"Instanciate with window length.\n\n        Args:\n            width (int, optional): window length. Defaults to 100 samples.\n            model (str, optional): segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if `custom_cost` is not None.\n            custom_cost (BaseCost, optional): custom cost function. Defaults to None.\n            min_size (int, optional): minimum segment length.\n            jump (int, optional): subsample (one every *jump* points).\n            params (dict, optional): a dictionary of parameters for the cost instance.`\n        \"\"\"\n        self.min_size = min_size\n        self.jump = jump\n        self.width = 2 * (width // 2)\n        self.n_samples = None\n        self.signal = None\n        self.inds = None\n        if custom_cost is not None and isinstance(custom_cost, BaseCost):\n            self.cost = custom_cost\n        else:\n            if params is None:\n                self.cost = cost_factory(model=model)\n            else:\n                self.cost = cost_factory(model=model, **params)\n        self.score = list()\n\n    def _seg(self, n_bkps=None, pen=None, epsilon=None):\n        \"\"\"Sequential peak search.\n\n        The stopping rule depends on the parameter passed to the function.\n\n        Args:\n            n_bkps (int): number of breakpoints to find before stopping.\n            penalty (float): penalty value (&gt;0)\n            epsilon (float): reconstruction budget (&gt;0)\n\n        Returns:\n            list: breakpoint index list\n        \"\"\"\n        # initialization\n        bkps = [self.n_samples]\n        stop = False\n        error = self.cost.sum_of_costs(bkps)\n        # peak search\n        # forcing order to be above one in case jump is too large (issue #16)\n        order = max(max(self.width, 2 * self.min_size) // (2 * self.jump), 1)\n        peak_inds_shifted = argrelmax(self.score, order=order, mode=\"wrap\")[0]\n\n        if peak_inds_shifted.size == 0:  # no peaks if the score is constant\n            return bkps\n        gains = np.take(self.score, peak_inds_shifted)\n        peak_inds_arr = np.take(self.inds, peak_inds_shifted)\n        # sort according to score value\n        _, peak_inds = unzip(sorted(zip(gains, peak_inds_arr)))\n        peak_inds = list(peak_inds)\n\n        while not stop:\n            stop = True\n            # _, bkp = max((v, k) for k, v in enumerate(self.score, start=1)\n            # if not any(abs(k - b) &lt; self.width // 2 for b in bkps[:-1]))\n\n            try:\n                # index with maximum score\n                bkp = peak_inds.pop()\n            except IndexError:  # peak_inds is empty\n                break\n\n            if n_bkps is not None:\n                if len(bkps) - 1 &lt; n_bkps:\n                    stop = False\n            elif pen is not None:\n                gain = error - self.cost.sum_of_costs(sorted([bkp] + bkps))\n                if gain &gt; pen:\n                    stop = False\n            elif epsilon is not None:\n                if error &gt; epsilon:\n                    stop = False\n\n            if not stop:\n                bkps.append(bkp)\n                bkps.sort()\n                error = self.cost.sum_of_costs(bkps)\n\n        return bkps\n\n    def fit(self, signal) -&gt; \"Window\":\n        \"\"\"Compute params to segment signal.\n\n        Args:\n            signal (array): signal to segment. Shape (n_samples, n_features) or (n_samples,).\n\n        Returns:\n            self\n        \"\"\"\n        # update some params\n        if signal.ndim == 1:\n            self.signal = signal.reshape(-1, 1)\n        else:\n            self.signal = signal\n        self.n_samples, _ = self.signal.shape\n        # indexes\n        self.inds = np.arange(self.n_samples, step=self.jump)\n        # delete borders\n        keep = (self.inds &gt;= self.width // 2) &amp; (\n            self.inds &lt; self.n_samples - self.width // 2\n        )\n        self.inds = self.inds[keep]\n        self.cost.fit(signal)\n        # compute score\n        score = list()\n        for k in self.inds:\n            start, end = k - self.width // 2, k + self.width // 2\n            gain = self.cost.error(start, end)\n            if np.isinf(gain) and gain &lt; 0:\n                # segment is constant and no improvment possible on start .. end\n                score.append(0)\n                continue\n            gain -= self.cost.error(start, k) + self.cost.error(k, end)\n            score.append(gain)\n        self.score = np.array(score)\n        return self\n\n    def predict(self, n_bkps=None, pen=None, epsilon=None):\n        \"\"\"Return the optimal breakpoints.\n\n        Must be called after the fit method. The breakpoints are associated with the signal passed\n        to [`fit()`][ruptures.detection.window.Window.fit].\n        The stopping rule depends on the parameter passed to the function.\n\n        Args:\n            n_bkps (int): number of breakpoints to find before stopping.\n            pen (float): penalty value (&gt;0)\n            epsilon (float): reconstruction budget (&gt;0)\n\n        Raises:\n            AssertionError: if none of `n_bkps`, `pen`, `epsilon` is set.\n            BadSegmentationParameters: in case of impossible segmentation\n                configuration\n\n        Returns:\n            list: sorted list of breakpoints\n        \"\"\"\n        # raise an exception in case of impossible segmentation configuration\n        if not sanity_check(\n            n_samples=self.cost.signal.shape[0],\n            n_bkps=0 if n_bkps is None else n_bkps,\n            jump=self.jump,\n            min_size=self.min_size,\n        ):\n            raise BadSegmentationParameters\n\n        msg = \"Give a parameter.\"\n        assert any(param is not None for param in (n_bkps, pen, epsilon)), msg\n\n        bkps = self._seg(n_bkps=n_bkps, pen=pen, epsilon=epsilon)\n        return bkps\n\n    def fit_predict(self, signal, n_bkps=None, pen=None, epsilon=None):\n        \"\"\"Helper method to call fit and predict once.\"\"\"\n        self.fit(signal)\n        return self.predict(n_bkps=n_bkps, pen=pen, epsilon=epsilon)\n</code></pre>"},{"location":"code-reference/detection/window-reference/#ruptures.detection.window.Window.__init__","title":"<code>__init__(width=100, model='l2', custom_cost=None, min_size=2, jump=5, params=None)</code>","text":"<p>Instanciate with window length.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>int</code> <p>window length. Defaults to 100 samples.</p> <code>100</code> <code>model</code> <code>str</code> <p>segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if <code>custom_cost</code> is not None.</p> <code>'l2'</code> <code>custom_cost</code> <code>BaseCost</code> <p>custom cost function. Defaults to None.</p> <code>None</code> <code>min_size</code> <code>int</code> <p>minimum segment length.</p> <code>2</code> <code>jump</code> <code>int</code> <p>subsample (one every jump points).</p> <code>5</code> <code>params</code> <code>dict</code> <p>a dictionary of parameters for the cost instance.`</p> <code>None</code> Source code in <code>ruptures/detection/window.py</code> <pre><code>def __init__(\n    self, width=100, model=\"l2\", custom_cost=None, min_size=2, jump=5, params=None\n):\n    \"\"\"Instanciate with window length.\n\n    Args:\n        width (int, optional): window length. Defaults to 100 samples.\n        model (str, optional): segment model, [\"l1\", \"l2\", \"rbf\"]. Not used if `custom_cost` is not None.\n        custom_cost (BaseCost, optional): custom cost function. Defaults to None.\n        min_size (int, optional): minimum segment length.\n        jump (int, optional): subsample (one every *jump* points).\n        params (dict, optional): a dictionary of parameters for the cost instance.`\n    \"\"\"\n    self.min_size = min_size\n    self.jump = jump\n    self.width = 2 * (width // 2)\n    self.n_samples = None\n    self.signal = None\n    self.inds = None\n    if custom_cost is not None and isinstance(custom_cost, BaseCost):\n        self.cost = custom_cost\n    else:\n        if params is None:\n            self.cost = cost_factory(model=model)\n        else:\n            self.cost = cost_factory(model=model, **params)\n    self.score = list()\n</code></pre>"},{"location":"code-reference/detection/window-reference/#ruptures.detection.window.Window.fit","title":"<code>fit(signal)</code>","text":"<p>Compute params to segment signal.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>signal to segment. Shape (n_samples, n_features) or (n_samples,).</p> required <p>Returns:</p> Type Description <code>Window</code> <p>self</p> Source code in <code>ruptures/detection/window.py</code> <pre><code>def fit(self, signal) -&gt; \"Window\":\n    \"\"\"Compute params to segment signal.\n\n    Args:\n        signal (array): signal to segment. Shape (n_samples, n_features) or (n_samples,).\n\n    Returns:\n        self\n    \"\"\"\n    # update some params\n    if signal.ndim == 1:\n        self.signal = signal.reshape(-1, 1)\n    else:\n        self.signal = signal\n    self.n_samples, _ = self.signal.shape\n    # indexes\n    self.inds = np.arange(self.n_samples, step=self.jump)\n    # delete borders\n    keep = (self.inds &gt;= self.width // 2) &amp; (\n        self.inds &lt; self.n_samples - self.width // 2\n    )\n    self.inds = self.inds[keep]\n    self.cost.fit(signal)\n    # compute score\n    score = list()\n    for k in self.inds:\n        start, end = k - self.width // 2, k + self.width // 2\n        gain = self.cost.error(start, end)\n        if np.isinf(gain) and gain &lt; 0:\n            # segment is constant and no improvment possible on start .. end\n            score.append(0)\n            continue\n        gain -= self.cost.error(start, k) + self.cost.error(k, end)\n        score.append(gain)\n    self.score = np.array(score)\n    return self\n</code></pre>"},{"location":"code-reference/detection/window-reference/#ruptures.detection.window.Window.fit_predict","title":"<code>fit_predict(signal, n_bkps=None, pen=None, epsilon=None)</code>","text":"<p>Helper method to call fit and predict once.</p> Source code in <code>ruptures/detection/window.py</code> <pre><code>def fit_predict(self, signal, n_bkps=None, pen=None, epsilon=None):\n    \"\"\"Helper method to call fit and predict once.\"\"\"\n    self.fit(signal)\n    return self.predict(n_bkps=n_bkps, pen=pen, epsilon=epsilon)\n</code></pre>"},{"location":"code-reference/detection/window-reference/#ruptures.detection.window.Window.predict","title":"<code>predict(n_bkps=None, pen=None, epsilon=None)</code>","text":"<p>Return the optimal breakpoints.</p> <p>Must be called after the fit method. The breakpoints are associated with the signal passed to <code>fit()</code>. The stopping rule depends on the parameter passed to the function.</p> <p>Parameters:</p> Name Type Description Default <code>n_bkps</code> <code>int</code> <p>number of breakpoints to find before stopping.</p> <code>None</code> <code>pen</code> <code>float</code> <p>penalty value (&gt;0)</p> <code>None</code> <code>epsilon</code> <code>float</code> <p>reconstruction budget (&gt;0)</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if none of <code>n_bkps</code>, <code>pen</code>, <code>epsilon</code> is set.</p> <code>BadSegmentationParameters</code> <p>in case of impossible segmentation configuration</p> <p>Returns:</p> Name Type Description <code>list</code> <p>sorted list of breakpoints</p> Source code in <code>ruptures/detection/window.py</code> <pre><code>def predict(self, n_bkps=None, pen=None, epsilon=None):\n    \"\"\"Return the optimal breakpoints.\n\n    Must be called after the fit method. The breakpoints are associated with the signal passed\n    to [`fit()`][ruptures.detection.window.Window.fit].\n    The stopping rule depends on the parameter passed to the function.\n\n    Args:\n        n_bkps (int): number of breakpoints to find before stopping.\n        pen (float): penalty value (&gt;0)\n        epsilon (float): reconstruction budget (&gt;0)\n\n    Raises:\n        AssertionError: if none of `n_bkps`, `pen`, `epsilon` is set.\n        BadSegmentationParameters: in case of impossible segmentation\n            configuration\n\n    Returns:\n        list: sorted list of breakpoints\n    \"\"\"\n    # raise an exception in case of impossible segmentation configuration\n    if not sanity_check(\n        n_samples=self.cost.signal.shape[0],\n        n_bkps=0 if n_bkps is None else n_bkps,\n        jump=self.jump,\n        min_size=self.min_size,\n    ):\n        raise BadSegmentationParameters\n\n    msg = \"Give a parameter.\"\n    assert any(param is not None for param in (n_bkps, pen, epsilon)), msg\n\n    bkps = self._seg(n_bkps=n_bkps, pen=pen, epsilon=epsilon)\n    return bkps\n</code></pre>"},{"location":"code-reference/metrics/hausdorff/","title":"Hausdorff metric (<code>hausdorff</code>)","text":"<p>Compute the Hausdorff distance between changepoints.</p> <p>Parameters:</p> Name Type Description Default <code>bkps1</code> <code>list</code> <p>list of the last index of each regime.</p> required <code>bkps2</code> <code>list</code> <p>list of the last index of each regime.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Hausdorff distance.</p> Source code in <code>ruptures/metrics/hausdorff.py</code> <pre><code>def hausdorff(bkps1, bkps2):\n    \"\"\"Compute the Hausdorff distance between changepoints.\n\n    Args:\n        bkps1 (list): list of the last index of each regime.\n        bkps2 (list): list of the last index of each regime.\n\n    Returns:\n        float: Hausdorff distance.\n    \"\"\"\n    sanity_check(bkps1, bkps2)\n    bkps1_arr = np.array(bkps1[:-1]).reshape(-1, 1)\n    bkps2_arr = np.array(bkps2[:-1]).reshape(-1, 1)\n    pw_dist = cdist(bkps1_arr, bkps2_arr)\n    res = max(pw_dist.min(axis=0).max(), pw_dist.min(axis=1).max())\n    return res\n</code></pre>"},{"location":"code-reference/metrics/precisionrecall/","title":"Precision and recall (<code>precision_recall</code>)","text":"<p>Calculate the precision/recall of an estimated segmentation compared with the true segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>true_bkps</code> <code>list</code> <p>list of the last index of each regime (true partition).</p> required <code>my_bkps</code> <code>list</code> <p>list of the last index of each regime (computed partition).</p> required <code>margin</code> <code>int</code> <p>allowed error (in points).</p> <code>10</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>(precision, recall)</p> Source code in <code>ruptures/metrics/precisionrecall.py</code> <pre><code>def precision_recall(true_bkps, my_bkps, margin=10):\n    \"\"\"Calculate the precision/recall of an estimated segmentation compared\n    with the true segmentation.\n\n    Args:\n        true_bkps (list): list of the last index of each regime (true\n            partition).\n        my_bkps (list): list of the last index of each regime (computed\n            partition).\n        margin (int, optional): allowed error (in points).\n\n    Returns:\n        tuple: (precision, recall)\n    \"\"\"\n    sanity_check(true_bkps, my_bkps)\n    assert margin &gt; 0, \"Margin of error must be positive (margin = {})\".format(margin)\n\n    if len(my_bkps) == 1:\n        return 0, 0\n\n    used = set()\n    true_pos = set(\n        true_b\n        for true_b, my_b in product(true_bkps[:-1], my_bkps[:-1])\n        if my_b - margin &lt; true_b &lt; my_b + margin\n        and not (my_b in used or used.add(my_b))\n    )\n\n    tp_ = len(true_pos)\n    precision = tp_ / (len(my_bkps) - 1)\n    recall = tp_ / (len(true_bkps) - 1)\n    return precision, recall\n</code></pre>"},{"location":"code-reference/metrics/randindex/","title":"Rand index (<code>randindex</code>)","text":"<p>Computes the Rand index (between 0 and 1) between two segmentations.</p> <p>The Rand index (RI) measures the similarity between two segmentations and is equal to the proportion of aggreement between two partitions.</p> <p>RI is between 0 (total disagreement) and 1 (total agreement). This function uses the efficient implementation of [1].</p> <p>[1] Prates, L. (2021). A more efficient algorithm to compute the Rand Index for change-point problems. ArXiv:2112.03738.</p> <p>Parameters:</p> Name Type Description Default <code>bkps1</code> <code>list</code> <p>sorted list of the last index of each regime.</p> required <code>bkps2</code> <code>list</code> <p>sorted list of the last index of each regime.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Rand index</p> Source code in <code>ruptures/metrics/randindex.py</code> <pre><code>def randindex(bkps1, bkps2):\n    \"\"\"Computes the Rand index (between 0 and 1) between two segmentations.\n\n    The Rand index (RI) measures the similarity between two segmentations and\n    is equal to the proportion of aggreement between two partitions.\n\n    RI is between 0 (total disagreement) and 1 (total agreement).\n    This function uses the efficient implementation of [1].\n\n    [1] Prates, L. (2021). A more efficient algorithm to compute the Rand Index for\n    change-point problems. ArXiv:2112.03738.\n\n    Args:\n        bkps1 (list): sorted list of the last index of each regime.\n        bkps2 (list): sorted list of the last index of each regime.\n\n    Returns:\n        float: Rand index\n    \"\"\"\n    sanity_check(bkps1, bkps2)\n    n_samples = bkps1[-1]\n    bkps1_with_0 = [0] + bkps1\n    bkps2_with_0 = [0] + bkps2\n    n_bkps1 = len(bkps1)\n    n_bkps2 = len(bkps2)\n\n    disagreement = 0\n    beginj: int = 0  # avoids unnecessary computations\n    for index_bkps1 in range(n_bkps1):\n        start1: int = bkps1_with_0[index_bkps1]\n        end1: int = bkps1_with_0[index_bkps1 + 1]\n        for index_bkps2 in range(beginj, n_bkps2):\n            start2: int = bkps2_with_0[index_bkps2]\n            end2: int = bkps2_with_0[index_bkps2 + 1]\n            nij = max(min(end1, end2) - max(start1, start2), 0)\n            disagreement += nij * abs(end1 - end2)\n\n            # we can skip the rest of the iteration, nij will be 0\n            if end1 &lt; end2:\n                break\n            else:\n                beginj = index_bkps2 + 1\n\n    disagreement /= n_samples * (n_samples - 1) / 2\n    return 1.0 - disagreement\n</code></pre>"},{"location":"code-reference/show/display/","title":"Display (<code>display</code>)","text":"<p>Display a signal and the change points provided in alternating colors. If another set of change point is provided, they are displayed with dashed vertical dashed lines. The following matplotlib subplots options is set by default, but can be changed when calling <code>display</code>):</p> <ul> <li>figure size <code>figsize</code>, defaults to <code>(10, 2 * n_features)</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>array</code> <p>signal array, shape (n_samples,) or (n_samples, n_features).</p> required <code>true_chg_pts</code> <code>list</code> <p>list of change point indexes.</p> required <code>computed_chg_pts</code> <code>list</code> <p>list of change point indexes.</p> <code>None</code> <code>computed_chg_pts_color</code> <code>str</code> <p>color of the lines indicating the computed_chg_pts. Defaults to \"k\".</p> <code>'k'</code> <code>computed_chg_pts_linewidth</code> <code>int</code> <p>linewidth of the lines indicating the computed_chg_pts. Defaults to 3.</p> <code>3</code> <code>computed_chg_pts_linestyle</code> <code>str</code> <p>linestyle of the lines indicating the computed_chg_pts. Defaults to \"--\".</p> <code>'--'</code> <code>computed_chg_pts_alpha</code> <code>float</code> <p>alpha of the lines indicating the computed_chg_pts. Defaults to \"1.0\".</p> <code>1.0</code> <code>**kwargs</code> <p>all additional keyword arguments are passed to the plt.subplots call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>(figure, axarr) with a :class:<code>matplotlib.figure.Figure</code> object and an array of Axes objects.</p> Source code in <code>ruptures/show/display.py</code> <pre><code>def display(\n    signal,\n    true_chg_pts,\n    computed_chg_pts=None,\n    computed_chg_pts_color=\"k\",\n    computed_chg_pts_linewidth=3,\n    computed_chg_pts_linestyle=\"--\",\n    computed_chg_pts_alpha=1.0,\n    **kwargs\n):\n    \"\"\"Display a signal and the change points provided in alternating colors.\n    If another set of change point is provided, they are displayed with dashed\n    vertical dashed lines. The following matplotlib subplots options is set by\n    default, but can be changed when calling `display`):\n\n    - figure size `figsize`, defaults to `(10, 2 * n_features)`.\n\n    Args:\n        signal (array): signal array, shape (n_samples,) or (n_samples, n_features).\n        true_chg_pts (list): list of change point indexes.\n        computed_chg_pts (list, optional): list of change point indexes.\n        computed_chg_pts_color (str, optional): color of the lines indicating\n            the computed_chg_pts. Defaults to \"k\".\n        computed_chg_pts_linewidth (int, optional): linewidth of the lines\n            indicating the computed_chg_pts. Defaults to 3.\n        computed_chg_pts_linestyle (str, optional): linestyle of the lines\n            indicating the computed_chg_pts. Defaults to \"--\".\n        computed_chg_pts_alpha (float, optional): alpha of the lines indicating\n            the computed_chg_pts. Defaults to \"1.0\".\n        **kwargs : all additional keyword arguments are passed to the plt.subplots call.\n\n    Returns:\n        tuple: (figure, axarr) with a :class:`matplotlib.figure.Figure` object and an array of Axes objects.\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n    except ImportError:\n        raise MatplotlibMissingError(\n            \"This feature requires the optional dependency matpotlib, you can install it using `pip install matplotlib`.\"\n        )\n\n    if type(signal) != np.ndarray:\n        # Try to get array from Pandas dataframe\n        signal = signal.values\n\n    if signal.ndim == 1:\n        signal = signal.reshape(-1, 1)\n    n_samples, n_features = signal.shape\n\n    # let's set a sensible defaut size for the subplots\n    matplotlib_options = {\n        \"figsize\": (10, 2 * n_features),  # figure size\n    }\n    # add/update the options given by the user\n    matplotlib_options.update(kwargs)\n\n    # create plots\n    fig, axarr = plt.subplots(n_features, sharex=True, **matplotlib_options)\n    if n_features == 1:\n        axarr = [axarr]\n\n    for axe, sig in zip(axarr, signal.T):\n        color_cycle = cycle(COLOR_CYCLE)\n        # plot s\n        axe.plot(range(n_samples), sig)\n\n        # color each (true) regime\n        bkps = [0] + sorted(true_chg_pts)\n        alpha = 0.2  # transparency of the colored background\n\n        for (start, end), col in zip(pairwise(bkps), color_cycle):\n            axe.axvspan(max(0, start - 0.5), end - 0.5, facecolor=col, alpha=alpha)\n        # vertical lines to mark the computed_chg_pts\n        if computed_chg_pts is not None:\n            for bkp in computed_chg_pts:\n                if bkp != 0 and bkp &lt; n_samples:\n                    axe.axvline(\n                        x=bkp - 0.5,\n                        color=computed_chg_pts_color,\n                        linewidth=computed_chg_pts_linewidth,\n                        linestyle=computed_chg_pts_linestyle,\n                        alpha=computed_chg_pts_alpha,\n                    )\n\n    fig.tight_layout()\n\n    return fig, axarr\n</code></pre>"},{"location":"examples/basic-usage/","title":"Basic usage","text":"<pre><code>import matplotlib.pyplot as plt  # for display purposes\n\nimport ruptures as rpt  # our package\n</code></pre> <pre><code>n_samples, n_dims, sigma = 1000, 3, 2\nn_bkps = 4  # number of breakpoints\nsignal, bkps = rpt.pw_constant(n_samples, n_dims, n_bkps, noise_std=sigma)\n</code></pre> <p>The true change points of this synthetic signal are available in the <code>bkps</code> variable.</p> <pre><code>print(bkps)\n</code></pre> <pre>\n<code>[213, 398, 600, 794, 1000]\n</code>\n</pre> <p>Note that the first four element are change point indexes while the last is simply the number of samples. (This is a technical convention so that functions in <code>ruptures</code> always know the length of the signal at hand.)</p> <p>It is also possible to plot our \\(\\mathbb{R}^3\\)-valued signal along with the true change points with the <code>rpt.display</code> function. In the following image, the color changes whenever the mean of the signal shifts.</p> <pre><code>fig, ax_array = rpt.display(signal, bkps)\n</code></pre> <pre><code># detection\nalgo = rpt.Dynp(model=\"l2\").fit(signal)\nresult = algo.predict(n_bkps=4)\n\nprint(result)\n</code></pre> <pre>\n<code>[210, 400, 600, 795, 1000]\n</code>\n</pre> <p>Again the first elements are change point indexes and the last is the number of samples.</p> <p>To visualy compare the true segmentation (<code>bkps</code>) and the estimated one (<code>result</code>), we can resort to <code>rpt.display</code> a second time. In the following image, the alternating colors indicate the true breakpoints and the dashed vertical lines, the estimated breakpoints.</p> <pre><code># display\nrpt.display(signal, bkps, result)\nplt.show()\n</code></pre> <p>In this simple example, both are quite similar and almost undistinguishable.</p>"},{"location":"examples/basic-usage/#basic-usage","title":"Basic usage","text":"<p>Info</p> <ul> <li>Try this notebook in an executable environment with Binder.</li> <li>Download this notebook here.</li> </ul> <p>Let us start with a simple example to illustrate the use of <code>ruptures</code>: generate a 3-dimensional piecewise constant signal with noise and estimate the change points.</p>"},{"location":"examples/basic-usage/#setup","title":"Setup","text":"<p>First, we make the necessary imports.</p>"},{"location":"examples/basic-usage/#generate-and-display-the-signal","title":"Generate and display the signal","text":"<p>Let us generate a 3-dimensional piecewise constant signal with Gaussian noise.</p>"},{"location":"examples/basic-usage/#change-point-detection","title":"Change point detection","text":"<p>We can now perform change point detection, meaning that we find the indexes where the signal mean changes. To that end, we minimize the sum of squared errors when approximating the signal by a piecewise constant signal. Formally, for a signal \\(y_0,y_1,\\dots,y_{T-1}\\) (\\(T\\) samples), we solve the following optimization problem, over all possible change positions \\(t_1&lt;t_2&lt;\\dots&lt;t_k\\) $\\(=\"\" \\(\\bar{y}_{t_k..t_{k+1}}\\)=\"\" \\(k\\)=\"\" \\(t_0=\"0\\)\" \\(t_{k+1}=\"T\\).)\" (y_{t_k},=\"\" (by=\"\" (more=\"\" (where=\"\" :=\"\\sum_{k=0}^K\\sum_{t=t_k}^{t_{k+1}-1}\" &lt;=\"\" [<code>dynp</code>](..=\"\" [user=\"\" [what=\"\" \\hat{t}_1,=\"\" \\hat{t}_2,\\dots,\\hat{t}_k=\"\\arg\\min_{t_1,\\dots,t_K}\" \\|y_t-\\bar{y}_{t_k..t_{k+1}}\\|^2=\"\" and=\"\" by=\"\" change=\"\" changes=\"\" class.=\"\" convention=\"\" defined=\"\" detection=\"\" detection?](=\"\" div=\"\" dynamic=\"\" dynp.md)=\"\" empirical=\"\" guide](=\"\" in=\"\" information=\"\" is=\"\" mean=\"\" number=\"\" of=\"\" optimization=\"\" point=\"\" programming,=\"\" section=\"\" solved=\"\" sub-signal=\"\" the=\"\" this=\"\" user):=\"\" user-guide=\"\" user-guide).)=\"\" using=\"\" v(t_1,t_2,\\dots,t_k)=\"\" what-is-cpd)=\"\" where=\"\" with=\"\" y_{t_k+1},\\dots,y_{t_{k+1}-1}\\).=\"\"&gt; &lt;/t_2&lt;\\dots&lt;t_k\\)&gt;</p>"},{"location":"examples/basic-usage/#display-the-results","title":"Display the results","text":""},{"location":"examples/introduction/","title":"Gallery of examples","text":"<p>These examples illustrate the main features of the <code>ruptures</code> package. Simple examples are direct applications of the library's functions on simulated data. Advanced examples deal with more complex tasks, such as calibration and real-world data.</p>"},{"location":"examples/kernel-cpd-performance-comparison/","title":"Kernel change point detection: a performance comparison","text":"<p>First, we make the necessary imports and generate a toy signal</p> <pre><code>import time  # for execution time comparison\n\nimport matplotlib.pyplot as plt  # for display purposes\n\nimport ruptures as rpt  # our package\nfrom ruptures.metrics import hausdorff\n\n# generate signal\nn_samples, dim, sigma = 500, 3, 3\nn_bkps = 6  # number of breakpoints\nsignal, bkps = rpt.pw_constant(n_samples, dim, n_bkps, noise_std=sigma)\nfig, ax_array = rpt.display(signal, bkps)\n</code></pre> <pre><code>algo_python = rpt.Dynp(model=\"l2\", jump=1, min_size=2).fit(\n    signal\n)  # written in pure python\nalgo_c = rpt.KernelCPD(kernel=\"linear\", min_size=2).fit(signal)  # written in C\n\nfor label, algo in zip(\n    (\"Python implementation\", \"C implementation\"), (algo_python, algo_c)\n):\n    start_time = time.time()\n    result = algo.predict(n_bkps=n_bkps)\n    print(f\"{label}:\\t{time.time() - start_time:.3f} s\")\n</code></pre> <pre>\n<code>Python implementation:    6.397 s\nC implementation:   0.002 s\n</code>\n</pre> <p>The speed-up is quite significant and depends on the signal size (number \\(T\\) of samples and dimension \\(d\\)) and the number \\(K\\) of change points to detect. The C implementation has a time complexity of the order \\(\\mathcal{O}(KdT^2)\\) and space complexity of the order \\(\\mathcal{O}(T)\\). As to the Python implementation, the complexities in time and space are of the order \\(\\mathcal{O}(KdT^3)\\) and \\(\\mathcal{O}(T^2)\\) respectively.</p> <p>We can also check that both methods return the same set of change points.</p> <pre><code>bkps_python = algo_python.predict(n_bkps=n_bkps)\nbkps_c = algo_c.predict(n_bkps=n_bkps)\nprint(f\"Python implementation:\\t{bkps_python}\")\nprint(f\"C implementation:\\t{bkps_c}\")\nprint(f\"(Hausdorff distance: {hausdorff(bkps_python, bkps_c):.0f} samples)\")\n</code></pre> <pre>\n<code>Python implementation:    [73, 147, 221, 288, 354, 426, 500]\nC implementation:   [73, 147, 221, 288, 354, 426, 500]\n(Hausdorff distance: 0 samples)\n</code>\n</pre> <pre><code>algo_python = rpt.Pelt(model=\"l2\", jump=1, min_size=2).fit(\n    signal\n)  # written in pure python\nalgo_c = rpt.KernelCPD(kernel=\"linear\", min_size=2).fit(\n    signal\n)  # written in C, same class as before\n\n\npenalty_value = 100  # beta\n\nfor label, algo in zip(\n    (\"Python implementation\", \"C implementation\"), (algo_python, algo_c)\n):\n    start_time = time.time()\n    result = algo.predict(pen=penalty_value)\n    print(f\"{label}:\\t{time.time() - start_time:.3f} s\")\n</code></pre> <pre>\n<code>Python implementation:    0.424 s\nC implementation:   0.000 s\n</code>\n</pre> <p>Again, the speed-up is quite significant and depends on the signal size (number \\(T\\) of samples and dimension \\(d\\)) and the penalty value \\(\\beta\\). We remark that, for both Python and C implementations, PELT is more efficient then dynamic programming.</p> <p>We can also check that both methods return the same set of change points.</p> <pre><code>bkps_python = algo_python.predict(pen=penalty_value)\nbkps_c = algo_c.predict(pen=penalty_value)\nprint(f\"Python implementation:\\t{bkps_python}\")\nprint(f\"C implementation:\\t{bkps_c}\")\nprint(f\"(Hausdorff distance: {hausdorff(bkps_python, bkps_c):.0f} samples)\")\n</code></pre> <pre>\n<code>Python implementation:    [5, 73, 147, 221, 288, 354, 426, 500]\nC implementation:   [5, 73, 147, 221, 288, 354, 426, 500]\n(Hausdorff distance: 0 samples)\n</code>\n</pre> <p>Note</p> <p>By default, <code>Dynp</code> and <code>Pelt</code> has <code>jump=5</code>. In <code>KernelCPD</code>, <code>jump=1</code> and cannot be changed. This is because, in the C implementation, changing the <code>jump</code> does not improve the running time significatively, while it does in the Python implementation.</p> <pre><code>params = {\"gamma\": 1e-2}\nalgo_python = rpt.Dynp(model=\"rbf\", params=params, jump=1, min_size=2).fit(\n    signal\n)  # written in pure python\nalgo_c = rpt.KernelCPD(kernel=\"rbf\", params=params, min_size=2).fit(\n    signal\n)  # written in C\n\nfor label, algo in zip(\n    (\"Python implementation\", \"C implementation\"), (algo_python, algo_c)\n):\n    start_time = time.time()\n    result = algo.predict(n_bkps=n_bkps)\n    print(f\"{label}:\\t{time.time() - start_time:.3f} s\")\n</code></pre> <pre>\n<code>Python implementation:    5.750 s\nC implementation:   0.003 s\n</code>\n</pre> <p>Again, the speed-up is quite significant. The C implementation has a time complexity of the order \\(\\mathcal{O}(CKT^2)\\) and space complexity of the order \\(\\mathcal{O}(T)\\), where \\(C\\) is the complexity of computing \\(k(y_s, y_t)\\) once. As to the Python implementation, the complexities in time and space are of the order \\(\\mathcal{O}(CKT^4)\\) and \\(\\mathcal{O}(T^2)\\) respectively.</p> <p>We can also check that both methods return the same set of change points.</p> <pre><code>bkps_python = algo_python.predict(n_bkps=n_bkps)\nbkps_c = algo_c.predict(n_bkps=n_bkps)\nprint(f\"Python implementation:\\t{bkps_python}\")\nprint(f\"C implementation:\\t{bkps_c}\")\nprint(f\"(Hausdorff distance: {hausdorff(bkps_python, bkps_c):.0f} samples)\")\n</code></pre> <pre>\n<code>Python implementation:    [73, 147, 221, 288, 354, 426, 500]\nC implementation:   [73, 147, 221, 288, 354, 426, 500]\n(Hausdorff distance: 0 samples)\n</code>\n</pre> <p>Note</p> <p>If not provided by the user, the <code>gamma</code> parameter is chosen using the median heuristics, meaning that it is set to inverse of the median of all pairwise products \\(k(y_s, y_t)\\).</p> <pre><code>algo_python = rpt.Pelt(model=\"rbf\", jump=1, min_size=2).fit(\n    signal\n)  # written in pure python\nalgo_c = rpt.KernelCPD(kernel=\"rbf\", min_size=2).fit(\n    signal\n)  # written in C, same class as before\n\n\npenalty_value = 1  # beta\n\nfor label, algo in zip(\n    (\"Python implementation\", \"C implementation\"), (algo_python, algo_c)\n):\n    start_time = time.time()\n    result = algo.predict(pen=penalty_value)\n    print(f\"{label}:\\t{time.time() - start_time:.3f} s\")\n</code></pre> <pre>\n<code>Python implementation:    0.214 s\nC implementation:   0.000 s\n</code>\n</pre> <p>Again, the speed-up is quite significant and depends on the signal size (number \\(T\\) of samples and dimension \\(d\\)) and the penalty value \\(\\beta\\). We remark that, for both Python and C implementations, PELT is more efficient then dynamic programming.</p> <p>We can also check that both methods return the same set of change points.</p> <pre><code>bkps_python = algo_python.predict(pen=penalty_value)\nbkps_c = algo_c.predict(pen=penalty_value)\nprint(f\"Python implementation:\\t{bkps_python}\")\nprint(f\"C implementation:\\t{bkps_c}\")\nprint(f\"(Hausdorff distance: {hausdorff(bkps_python, bkps_c):.0f} samples)\")\n</code></pre> <pre>\n<code>Python implementation:    [73, 147, 221, 288, 354, 426, 500]\nC implementation:   [73, 147, 221, 288, 354, 426, 500]\n(Hausdorff distance: 0 samples)\n</code>\n</pre>"},{"location":"examples/kernel-cpd-performance-comparison/#kernel-change-point-detection-a-performance-comparison","title":"Kernel change point detection: a performance comparison","text":"<p>Info</p> <ul> <li>Try this notebook in an executable environment with Binder.</li> <li>Download this notebook here.</li> </ul>"},{"location":"examples/kernel-cpd-performance-comparison/#introduction","title":"Introduction","text":"<p>In <code>ruptures</code>, there are two ways to perform kernel change point detection:</p> <ul> <li> <p>by using the pure Python classes Dynp (known number of change points) and Pelt (unknown number of change points),</p> </li> <li> <p>by using the faster class (implemented in C) KernelCPD  which contains both the dynamic programming approach and the penalized approach (PELT).</p> </li> </ul> <p>This example illustrates the performance of the fast C implementation compared to the pure Python one.</p> <p>The kernel change point detection setting is briefly described in the user guide. The interested reader can refer to [Celisse2018, Arlot2019] for a more complete introduction.</p> <p>The list of available kernels is available here, but in this example we only consider two:</p> <ul> <li>the linear kernel, \\(k_{\\text{linear}}(x, y) = x^T y\\) (Euclidean scalar product) and the induced norm is the Euclidean norm;</li> <li>the Gaussian kernel (also known as radial basis function, rbf), \\(k_{\\text{Gaussian}}(x,y)=\\exp(-\\gamma \\|x-y\\|^2)\\) where \\(\\|\\cdot\\|\\) is the Euclidean norm and \\(\\gamma&gt;0\\) is a user-defined parameter.</li> </ul>"},{"location":"examples/kernel-cpd-performance-comparison/#setup","title":"Setup","text":""},{"location":"examples/kernel-cpd-performance-comparison/#linear-kernel","title":"Linear kernel","text":"<p>The linear kernel (see above) \\(k_{\\text{linear}}\\) can detect changes in the mean of a signal. It also corresponds to the cost function <code>CostL2</code>.</p>"},{"location":"examples/kernel-cpd-performance-comparison/#dynamic-programming","title":"Dynamic programming","text":"<p>When the number of changes to detect is known beforehand, we use dynamic programming.</p>"},{"location":"examples/kernel-cpd-performance-comparison/#pelt","title":"PELT","text":"<p>When the number of changes to detect is unknown, we resort to PELT [Killick2012] to solve the penalized detection problem.</p>"},{"location":"examples/kernel-cpd-performance-comparison/#gaussian-kernel","title":"Gaussian kernel","text":"<p>The Gaussian kernel (see above) \\(k_{\\text{Gaussian}}\\) can detect changes in the distribution of an i.i.d. process. This is a feature of several kernel functions (in particular characteristics kernels; see [Gretton2012] for more information). It also corresponds to the cost function <code>CostRbf</code>.</p>"},{"location":"examples/kernel-cpd-performance-comparison/#dynamic-programming_1","title":"Dynamic programming","text":"<p>When the number of changes to detect is known beforehand, we use dynamic programming.</p>"},{"location":"examples/kernel-cpd-performance-comparison/#pelt_1","title":"PELT","text":"<p>When the number of changes to detect is unknown, we resort to PELT to solve the penalized detection problem.</p>"},{"location":"examples/kernel-cpd-performance-comparison/#references","title":"References","text":"<p>[Gretton2012] Gretton, A., Borgwardt, K. M., Rasch, M. J., Sch\u00f6lkopf, B., &amp; Smola, A. (2012). A kernel two-sample test. The Journal of Machine Learning Research, 13, 723\u2013773.</p> <p>[Killick2012] Killick, R., Fearnhead, P., &amp; Eckley, I. (2012). Optimal detection of changepoints with a linear computational cost. Journal of the American Statistical Association, 107(500), 1590\u20131598.</p> <p>[Celisse2018] Celisse, A., Marot, G., Pierre-Jean, M., &amp; Rigaill, G. (2018). New efficient algorithms for multiple change-point detection with reproducing kernels. Computational Statistics and Data Analysis, 128, 200\u2013220.</p> <p>[Arlot2019] Arlot, S., Celisse, A., &amp; Harchaoui, Z. (2019). A kernel multiple change-point algorithm via model selection. Journal of Machine Learning Research, 20(162), 1\u201356.</p>"},{"location":"examples/merging-cost-functions/","title":"Combining cost functions","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport ruptures as rpt\nfrom ruptures.base import BaseCost\n\nWINDOW_SIZE = 200\n\n\ndef minmax_scale(array: np.ndarray) -&amp;gt; np.ndarray:\n    \"\"\"Scale each dimension to the [0, 1] range.\"\"\"\n    return (array - np.min(array, axis=0)) / (\n        np.max(array, axis=0) - np.min(array, axis=0) + 1e-8\n    )\n\n\ndef fig_ax(figsize=(10, 3)):\n    return plt.subplots(figsize=figsize)\n</code></pre> <p>The merging procedure of cost functions is applied on the following 2D toy signal, where each dimension contains three mean-shifts. Note that only one mean-shift is shared by both dimensions and all other changes occur at different indexes.</p> <pre><code># fmt: off\nsignal_no_noise = np.array([[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0]])\n# fmt: on\nbkps_on_dim_0 = [329, 656, 1642, 2000]\nbkps_on_dim_1 = [656, 972, 1291, 2000]\nbkps_all_dims = [329, 656, 972, 1291, 1642, 2000]\n\nsignal_with_noise = signal_no_noise + np.random.normal(size=signal_no_noise.shape)\n\nfig, axes = rpt.display(signal_no_noise, bkps_all_dims)\n_ = axes[0].set_title(\"Toy signal (no noise)\")\n\nfig, axes = rpt.display(signal_with_noise, bkps_all_dims)\n_ = axes[0].set_title(\"Toy signal (with noise)\")\n</code></pre> <pre><code>class CostL2OnSingleDim(BaseCost):\n    \"\"\"This cost function detects mean-shift on a single dimension of a multivariate signal.\"\"\"\n\n    # The 2 following attributes must be specified for compatibility.\n    model = \"CostL2OnSingleDim\"\n    min_size = 1\n\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def fit(self, signal):\n        \"\"\"Set the internal parameter.\"\"\"\n        self.signal = signal[:, self.dim].reshape(-1, 1)\n        return self\n\n    def error(self, start, end) -&amp;gt; float:\n        \"\"\"Return the approximation cost on the segment [start:end].\n\n        Args:\n            start (int): start of the segment\n            end (int): end of the segment\n\n        Returns:\n            segment cost\n\n        Raises:\n            NotEnoughPoints: when the segment is too short (less than `min_size` samples).\n        \"\"\"\n        if end - start &amp;lt; self.min_size:\n            raise rpt.exceptions.NotEnoughPoints\n        if end - start == 1:\n            return 0.0\n        return self.signal[start:end].var(axis=0).sum() * (end - start)\n</code></pre> <p>Combining this cost function with the window search method yields the following change-points. Note that the true changes are shown with the alternating colors and the estimated changes are shown with the vertical dashed lines.</p> <pre><code># Detect mean-shift on the first dimension\ncost_function = CostL2OnSingleDim(dim=0)\nalgo_on_dim_0 = rpt.Window(width=WINDOW_SIZE, custom_cost=cost_function, jump=1).fit(\n    signal_with_noise\n)\nbkps_pred = algo_on_dim_0.predict(\n    n_bkps=len(bkps_on_dim_0) - 1\n)  # the number of changes is known\n# display signal and changes\nfig, axes = rpt.display(signal_with_noise, bkps_on_dim_0, bkps_pred)\n_ = axes[0].set_title(\n    (\n        f\"\"\"Detection of mean-shifts using only Dimension 0:\\n\"\"\"\n        f\"\"\"true changes: {bkps_on_dim_0[:-1]}, detected changes: {bkps_pred[:-1]}.\"\"\"\n    )\n)\n</code></pre> <pre><code># Detect mean-shift on the second dimension\ncost_function = CostL2OnSingleDim(dim=1)\nalgo_on_dim_1 = rpt.Window(width=WINDOW_SIZE, custom_cost=cost_function, jump=1).fit(\n    signal_with_noise\n)\nbkps_pred = algo_on_dim_1.predict(\n    n_bkps=len(bkps_on_dim_1) - 1\n)  # the number of changes is known\n# display signal and changes\nfig, axes = rpt.display(signal_with_noise, bkps_on_dim_1, bkps_pred)\n_ = axes[0].set_title(\n    (\n        f\"\"\"Detection of mean-shifts using only Dimension 1:\\n\"\"\"\n        f\"\"\"true changes: {bkps_on_dim_1[:-1]}, detected changes: {bkps_pred[:-1]}.\"\"\"\n    )\n)\n</code></pre> <p>Observe that, depending on the considered cost function (<code>CostL2OnSingleDim(dim=0)</code> or <code>CostL2OnSingleDim(dim=1)</code>), the detected changes are not the same even though the input signal is the same.</p> <p>Displaying the scores of the cost functions further illustrates the difference between the two costs. Recall that change-points correspond to high scores.</p> <pre><code>for dim, algo in enumerate([algo_on_dim_0, algo_on_dim_1]):\n    fig, ax = fig_ax()\n    ax.plot(np.r_[np.zeros(WINDOW_SIZE // 2), algo.score, np.zeros(WINDOW_SIZE // 2)])\n    ax.set_xmargin(0)\n    ax.set_title(f\"Score for CostL2OnSingleDim(dim={dim})\")\n</code></pre> <p>The question now for a user is how to combine the two cost functions to either detect all changes or only the changes that occur accros all dimensions simultaneously.</p> <pre><code># concatenate scores\nscore_arr = np.c_[algo_on_dim_0.score, algo_on_dim_1.score]\n# intersection aggregation\nalgo_expert_intersection = rpt.Window(width=WINDOW_SIZE, jump=1).fit(signal_with_noise)\nalgo_expert_intersection.score = (minmax_scale(score_arr)).min(\n    axis=1\n)  # scaling + pointwise min\n# only one change point is shared by both dimensions\nbkps_intersection_predicted = algo_expert_intersection.predict(n_bkps=1)\n# display the aggregated score\nfig, ax = fig_ax()\nax.plot(\n    np.r_[\n        np.zeros(WINDOW_SIZE // 2),\n        algo_expert_intersection.score,\n        np.zeros(WINDOW_SIZE // 2),\n    ]\n)\nax.set_xmargin(0)\n_ = ax.set_title(\n    (\n        \"\"\"Aggregated score (intersection of experts)\\n\"\"\"\n        f\"\"\"true change: {bkps_on_dim_1[0]}, detected change: {bkps_intersection_predicted[0]}\"\"\"\n    )\n)\n</code></pre> <pre><code># union aggregation\nalgo_expert_union = rpt.Window(width=WINDOW_SIZE, jump=1).fit(signal_with_noise)\nalgo_expert_union.score = minmax_scale(score_arr).max(axis=1)  # scaling + pointwise max\n# 5 change points are present\nbkps_union_predicted = algo_expert_union.predict(n_bkps=5)\n# display the aggregated score\nfig, ax = fig_ax()\nax.plot(\n    np.r_[\n        np.zeros(WINDOW_SIZE // 2), algo_expert_union.score, np.zeros(WINDOW_SIZE // 2)\n    ]\n)\nax.set_xmargin(0)\n_ = ax.set_title(\n    (\n        \"\"\"Aggregated score (intersection of experts)\\n\"\"\"\n        f\"\"\"true change: {bkps_all_dims[:-1]}, detected change: {bkps_union_predicted[:-1]}\"\"\"\n    )\n)\n</code></pre>"},{"location":"examples/merging-cost-functions/#combining-cost-functions","title":"Combining cost functions","text":"<p>Info</p> <ul> <li>Try this notebook in an executable environment with Binder.</li> <li>Download this notebook here.</li> </ul>"},{"location":"examples/merging-cost-functions/#introduction","title":"Introduction","text":"<p>In <code>ruptures</code>, change point detection procedures can only use a single cost function. The choice of this cost function is crucial as it is related to the type of change to find.  For instance, CostL2 can detect shifts in the mean, CostNormal can detect shifts in the mean and the covariance structure, CostAR can detect shifts in the auto-regressive structure, etc.</p> <p>However, in many settings, several types of changes exist within a signal and a single cost function is not able to detect all changes simultaneously. To cope with this issue, a procedure to combine cost functions is needed. In this example, a technique inspired by [Katser2021] is presented. In a nutshell, different costs are combined to yield an aggregated cost function which is sensitive to several types of changes. The aggregated cost can then be used with the window search method to create a change point detection algorithm.</p> <p>For simplicity, we focus on mean-shifts that occur in different dimensions of a multivariate signal. Each considered cost function can only detect mean-shift in a single dimension. The user can choose between two aggregations procedures that can:</p> <ul> <li>either detect shifts that occur simultaneously on all dimensions (intersection of experts),</li> <li>or detect shifts that occur on any dimension (union of experts).</li> </ul> <p>Here, only CostL2 is considered for all dimensions, but all other costs could be used. The focus is then on the way the costs are combined (see intersection and union). For simplicity, the number of changes is assumed to be known by the user.</p>"},{"location":"examples/merging-cost-functions/#setup","title":"Setup","text":"<p>First, we make the necessary imports and define a few utility functions.</p>"},{"location":"examples/merging-cost-functions/#detection-with-a-single-cost-function","title":"Detection with a single cost function","text":"<p>Consider the following cost function (derived from CostL2) that can only detect mean-shift on one dimension at a time.</p>"},{"location":"examples/merging-cost-functions/#detection-with-an-aggregated-cost-function","title":"Detection with an aggregated cost function","text":""},{"location":"examples/merging-cost-functions/#intersection","title":"Intersection","text":"<p>A first way of aggregating the scores is to consider each score along a dimension as being an expert. The idea is to take the intersection of the experts so that the predicted change points are the change points where all the experts are confident. This method is useful when the user is interested in collecting the change points that occur on all dimensions simultaneously. To that end, the scores are scaled to the [0, 1] range then the pointwise minimum is taken.</p> <p>In the following cell, the intersection procedure correctly predicts the only common change point between the two dimensions. The aggregated score shows clearly the only change point to predict.</p>"},{"location":"examples/merging-cost-functions/#union","title":"Union","text":"<p>Another way of aggregating the scores is to take the union of the experts so that the predicted change points are the change points where at least one expert is confident. This method is useful when the user is interested in collecting all the change points that are present along all dimensions. To that end, the scores are scaled to the [0, 1] range then the pointwise maximum is taken.</p> <p>In the following cell, the union procedure correctly predicts all the change points. The aggregated score shows 5 clear peaks from which to take the change points.</p>"},{"location":"examples/merging-cost-functions/#conclusion","title":"Conclusion","text":"<p>This example shows two ways to aggregate cost functions for a 2D toy signal which contains mean-shifts that do not always co-occur:</p> <ul> <li>the intersection of experts detects changes that are detected by all cost functions simultaneously,</li> <li>the union of experts detects changes that are detected by at least one cost function.</li> </ul> <p>Those aggregation procedures can be applied to any set of cost functions and in a wide range of settings.</p>"},{"location":"examples/merging-cost-functions/#authors","title":"Authors","text":"<p>This example notebook has been authored by Th\u00e9o VINCENT and edited by Olivier Boulant and Charles Truong.</p>"},{"location":"examples/merging-cost-functions/#references","title":"References","text":"<p>[Katser2021] Katser, I., Kozitsin, V., Lobachev, V., &amp; Maksimov, I. (2021). Unsupervised Offline change point Detection Ensembles. Applied Sciences, 11(9), 4280.</p>"},{"location":"examples/music-segmentation/","title":"Music segmentation","text":"<pre><code>import matplotlib.pyplot as plt\nimport librosa\nimport numpy as np\nfrom IPython.display import Audio, display\n\nimport ruptures as rpt  # our package\n</code></pre> <p>We can also define a utility function.</p> <pre><code>def fig_ax(figsize=(15, 5), dpi=150):\n    \"\"\"Return a (matplotlib) figure and ax objects with given size.\"\"\"\n    return plt.subplots(figsize=figsize, dpi=dpi)\n</code></pre> <pre><code>duration = 30  # in seconds\nsignal, sampling_rate = librosa.load(librosa.ex(\"nutcracker\"), duration=duration)\n\n# listen to the music\ndisplay(Audio(data=signal, rate=sampling_rate))\n\n# look at the envelope\nfig, ax = fig_ax()\nax.plot(np.arange(signal.size) / sampling_rate, signal)\nax.set_xlim(0, signal.size / sampling_rate)\nax.set_xlabel(\"Time (s)\")\n_ = ax.set(title=\"Sound envelope\")\n</code></pre> <pre>\n<code>Downloading file 'Kevin_MacLeod_-_P_I_Tchaikovsky_Dance_of_the_Sugar_Plum_Fairy.ogg' from 'https://librosa.org/data/audio/Kevin_MacLeod_-_P_I_Tchaikovsky_Dance_of_the_Sugar_Plum_Fairy.ogg' to '/home/runner/.cache/librosa'.\n</code>\n</pre>                      Your browser does not support the audio element.                  <pre><code># Compute the onset strength\nhop_length_tempo = 256\noenv = librosa.onset.onset_strength(\n    y=signal, sr=sampling_rate, hop_length=hop_length_tempo\n)\n# Compute the tempogram\ntempogram = librosa.feature.tempogram(\n    onset_envelope=oenv,\n    sr=sampling_rate,\n    hop_length=hop_length_tempo,\n)\n# Display the tempogram\nfig, ax = fig_ax()\n_ = librosa.display.specshow(\n    tempogram,\n    ax=ax,\n    hop_length=hop_length_tempo,\n    sr=sampling_rate,\n    x_axis=\"s\",\n    y_axis=\"tempo\",\n)\n</code></pre> <pre><code># Choose detection method\nalgo = rpt.KernelCPD(kernel=\"linear\").fit(tempogram.T)\n\n# Choose the number of changes (elbow heuristic)\nn_bkps_max = 20  # K_max\n# Start by computing the segmentation with most changes.\n# After start, all segmentations with 1, 2,..., K_max-1 changes are also available for free.\n_ = algo.predict(n_bkps_max)\n\narray_of_n_bkps = np.arange(1, n_bkps_max + 1)\n\n\ndef get_sum_of_cost(algo, n_bkps) -&amp;gt; float:\n    \"\"\"Return the sum of costs for the change points `bkps`\"\"\"\n    bkps = algo.predict(n_bkps=n_bkps)\n    return algo.cost.sum_of_costs(bkps)\n\n\nfig, ax = fig_ax((7, 4))\nax.plot(\n    array_of_n_bkps,\n    [get_sum_of_cost(algo=algo, n_bkps=n_bkps) for n_bkps in array_of_n_bkps],\n    \"-*\",\n    alpha=0.5,\n)\nax.set_xticks(array_of_n_bkps)\nax.set_xlabel(\"Number of change points\")\nax.set_title(\"Sum of costs\")\nax.grid(axis=\"x\")\nax.set_xlim(0, n_bkps_max + 1)\n\n# Visually we choose n_bkps=5 (highlighted in red on the elbow plot)\nn_bkps = 5\n_ = ax.scatter([5], [get_sum_of_cost(algo=algo, n_bkps=5)], color=\"r\", s=100)\n</code></pre> <p>Visually, we choose 5 change points (highlighted in red on the elbow plot).</p> <pre><code># Segmentation\nbkps = algo.predict(n_bkps=n_bkps)\n# Convert the estimated change points (frame counts) to actual timestamps\nbkps_times = librosa.frames_to_time(bkps, sr=sampling_rate, hop_length=hop_length_tempo)\n\n# Displaying results\nfig, ax = fig_ax()\n_ = librosa.display.specshow(\n    tempogram,\n    ax=ax,\n    x_axis=\"s\",\n    y_axis=\"tempo\",\n    hop_length=hop_length_tempo,\n    sr=sampling_rate,\n)\n\nfor b in bkps_times[:-1]:\n    ax.axvline(b, ls=\"--\", color=\"white\", lw=4)\n</code></pre> <p>Visually, the estimated change points indeed separate portions of signal with a relatively constant tempo profile. Going back to the original music signal, this intuition can be verified by listening to the individual segments defined by the changes points.</p> <pre><code># Compute change points corresponding indexes in original signal\nbkps_time_indexes = (sampling_rate * bkps_times).astype(int).tolist()\n\nfor segment_number, (start, end) in enumerate(\n    rpt.utils.pairwise([0] + bkps_time_indexes), start=1\n):\n    segment = signal[start:end]\n    print(f\"Segment n\u00b0{segment_number} (duration: {segment.size/sampling_rate:.2f} s)\")\n    display(Audio(data=segment, rate=sampling_rate))\n</code></pre> <pre>\n<code>Segment n\u00b01 (duration: 1.76 s)\n</code>\n</pre>                      Your browser does not support the audio element.                  <pre>\n<code>Segment n\u00b02 (duration: 8.03 s)\n</code>\n</pre>                      Your browser does not support the audio element.                  <pre>\n<code>Segment n\u00b03 (duration: 8.46 s)\n</code>\n</pre>                      Your browser does not support the audio element.                  <pre>\n<code>Segment n\u00b04 (duration: 2.73 s)\n</code>\n</pre>                      Your browser does not support the audio element.                  <pre>\n<code>Segment n\u00b05 (duration: 5.97 s)\n</code>\n</pre>                      Your browser does not support the audio element.                  <pre>\n<code>Segment n\u00b06 (duration: 3.04 s)\n</code>\n</pre>                      Your browser does not support the audio element.                  <p>The first segment corresponds to the soundless part of the signal (visible on the plot of the signal enveloppe). The following segments correspond to different rythmic portions and the associated change points occur when various instruments enter or exit the play.</p>"},{"location":"examples/music-segmentation/#music-segmentation","title":"Music segmentation","text":"<p>Info</p> <ul> <li>Try this notebook in an executable environment with Binder.</li> <li>Download this notebook here.</li> </ul>"},{"location":"examples/music-segmentation/#introduction","title":"Introduction","text":"<p>Music segmentation can be seen as a change point detection task and therefore can be carried out with <code>ruptures</code>. Roughly, it consists in finding the temporal boundaries of meaningful sections, e.g. the intro, verse, chorus and outro in a song. This is an important task in the field of music information retrieval.</p> <p>The adopted approach is summarized as follows:</p> <ul> <li>the original sound is transformed into an informative (multivariate) representation;</li> <li>mean shifts are detected in this new representation using a dynamic programming approach.</li> </ul> <p>In this example, we use the well-known tempogram representation, which is based on the onset strength envelope of the input signal, and captures tempo information [Grosche2010].</p> <p>To load and manipulate sound data, we use the librosa package [McFee2015].</p>"},{"location":"examples/music-segmentation/#setup","title":"Setup","text":"<p>First, we make the necessary imports.</p>"},{"location":"examples/music-segmentation/#load-the-data","title":"Load the data","text":"<p>A number of music files are available in Librosa. See here for a complete list. In this example, we choose the Dance of the Sugar Plum Fairy from The Nutcracker by Tchaikovsky.</p> <p>We can listen to the music as well as display the sound envelope.</p>"},{"location":"examples/music-segmentation/#signal-segmentation","title":"Signal segmentation","text":""},{"location":"examples/music-segmentation/#transform-the-signal-into-a-tempogram","title":"Transform the signal into a tempogram","text":"<p>The tempogram measures the tempo (measured in Beats Per Minute, BPM) profile along the time axis.</p>"},{"location":"examples/music-segmentation/#detection-algorithm","title":"Detection algorithm","text":"<p>We choose to detect changes in the mean of the tempogram, which is a multivariate signal. This amounts to selecting the \\(L_2\\) cost function (see CostL2). To that end, two methods are available in <code>ruptures</code>:</p> <ul> <li><code>rpt.Dynp(model=\"l2\")</code></li> <li><code>rpt.KernelCPD(kernel=\"linear\")</code></li> </ul> <p>Both will return the same results but the latter is implemented in C and therefore significatively faster.</p>"},{"location":"examples/music-segmentation/#number-of-changes","title":"Number of changes","text":"<p>In order to choose the number of change points, we use the elbow method. In the change point detection setting, this heuritic consists in:</p> <ul> <li>plotting the sum of costs for 1, 2,...,\\(K_{\\text{max}}\\) change points,</li> <li>picking the number of changes at the \"elbow\" of the curve.</li> </ul> <p>Intuitively, adding change points beyond the \"elbow\" only provides a marginal decrease of the sum of costs.</p> <p>Here, we set \\(K_{\\text{max}}\\):=20.</p> <p>Note</p> <p>In <code>rpt.Dynp</code> and <code>rpt.KernelCPD</code>, whenever a segmentation with \\(K\\) changes is computed, all segmentations with 1,2,..., \\(K-1\\) are also computed and stored. Indeed, thanks to the dynamic programming approach, segmentations with less changes are avalaible for free as intermediate calculations. Therefore, users who need to compute segmentations with several numbers of changes should start with the one with the most changes.</p> <p>In addition, note that, in <code>ruptures</code>, the sum of costs of a segmentation defined by a set of change points <code>bkps</code> can easily be computed using:</p> <p><pre><code>algo = rpt.KernelCPD(kernel=\"linear\").fit(signal)\nalgo.cost.sum_of_costs(bkps)\n</code></pre> (Replace <code>rpt.KernelCPD</code> by the algorithm you are actually using, if different.)</p>"},{"location":"examples/music-segmentation/#results","title":"Results","text":"<p>The tempogram can now be segmented into homogeous (from a tempo standpoint) portions. The results are show in the following figure.</p>"},{"location":"examples/music-segmentation/#conclusion","title":"Conclusion","text":"<p>This example shows how to apply <code>ruptures</code> on a music segmentation task. More precisely, we detected mean shifts on a well-suited representation (the tempogram) of a music signal. The number of changes was heuristically determined (with the \"elbow\" method) and the results agreed with visually and auditory intuition.</p> <p>Such results can then be used to characterize the structure of music and songs, for music classification, recommandation, instrument recognition, etc. This procedure could also be enriched with other musically relevant representations (e.g. the chromagram) to detect other types of changes.</p>"},{"location":"examples/music-segmentation/#authors","title":"Authors","text":"<p>This example notebook has been authored by Olivier Boulant and edited by Charles Truong. </p>"},{"location":"examples/music-segmentation/#references","title":"References","text":"<p>[Grosche2010] Grosche, P., M\u00fcller, M., &amp; Kurth, F. (2010). Cyclic tempogram - a mid-level tempo representation for music signals. Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 5522\u20135525.</p> <p>[McFee2015] McFee, B., Raffel, C., Liang, D., Ellis, D. P. W., McVicar, M., Battenberg, E., &amp; Nieto, O. (2015). Librosa: audio and music signal analysis in Python. Proceedings of the Python in Science Conference, 8, 18\u201325.</p>"},{"location":"examples/text-segmentation/","title":"Text segmentation","text":"<pre><code>from pathlib import Path\n\nimport nltk\nimport numpy as np\nimport ruptures as rpt  # our package\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import regexp_tokenize\nfrom ruptures.base import BaseCost\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom matplotlib.colors import LogNorm\n</code></pre> <pre><code>nltk.download(\"stopwords\")\nSTOPWORD_SET = set(\n    stopwords.words(\"english\")\n)  # set of stopwords of the English language\nPUNCTUATION_SET = set(\"!\\\"#$%&amp;amp;'()*+,-./:;&amp;lt;=&amp;gt;?@[\\\\]^_`{|}~\")\n</code></pre> <pre>\n<code>[nltk_data] Downloading package stopwords to /home/runner/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n</code>\n</pre> <p>Utility functions.</p> <pre><code>def preprocess(list_of_sentences: list) -&amp;gt; list:\n    \"\"\"Preprocess each sentence (remove punctuation, stopwords, then stemming.)\"\"\"\n    transformed = list()\n    for sentence in list_of_sentences:\n        ps = PorterStemmer()\n        list_of_words = regexp_tokenize(text=sentence.lower(), pattern=\"\\w+\")\n        list_of_words = [\n            ps.stem(word) for word in list_of_words if word not in STOPWORD_SET\n        ]\n        transformed.append(\" \".join(list_of_words))\n    return transformed\n</code></pre> <pre><code>def draw_square_on_ax(start, end, ax, linewidth=0.8):\n    \"\"\"Draw a square on the given ax object.\"\"\"\n    ax.vlines(\n        x=[start - 0.5, end - 0.5],\n        ymin=start - 0.5,\n        ymax=end - 0.5,\n        linewidth=linewidth,\n    )\n    ax.hlines(\n        y=[start - 0.5, end - 0.5],\n        xmin=start - 0.5,\n        xmax=end - 0.5,\n        linewidth=linewidth,\n    )\n    return ax\n</code></pre> <pre><code># Loading the text\nfilepath = Path(\"../data/text-segmentation-data.txt\")\noriginal_text = filepath.read_text().split(\"\\n\")\nTRUE_BKPS = [11, 20, 30, 40, 49, 59, 69, 80, 90, 99]  # read from the data description\n\nprint(f\"There are {len(original_text)} sentences, from {len(TRUE_BKPS)} documents.\")\n</code></pre> <pre>\n<code>There are 99 sentences, from 10 documents.\n</code>\n</pre> <p>The objective is to automatically recover the boundaries of the 10 excerpts, using the fact that they come from quite different documents and therefore have distinct topics.</p> <p>For instance, in the small extract of text printed in the following cell, an accurate text segmentation procedure would be able to detect that the first two sentences (10 and 11) and the last three sentences (12 to 14) belong to two different documents and have very different semantic fields.</p> <pre><code># print 5 sentences from the original text\nstart, end = 9, 14\nfor line_number, sentence in enumerate(original_text[start:end], start=start + 1):\n    sentence = sentence.strip(\"\\n\")\n    print(f\"{line_number:&amp;gt;2}: {sentence}\")\n</code></pre> <pre>\n<code>10: That could be easily done , but there is little reason in it .\n11: It would come down to saying that Fromm paints with a broad brush , and that , after all , is not a conclusion one must work toward but an impression he has from the outset .\n12: the effect of the digitalis glycosides is inhibited by a high concentration of potassium in the incubation medium and is enhanced by the absence of potassium ( Wolff , 1960 ) .\n13: B. Organification of iodine The precise mechanism for organification of iodine in the thyroid is not as yet completely understood .\n14: However , the formation of organically bound iodine , mainly mono-iodotyrosine , can be accomplished in cell-free systems .\n</code>\n</pre> <pre><code># transform text\ntransformed_text = preprocess(original_text)\n# print original and transformed\nind = 97\nprint(\"Original sentence:\")\nprint(f\"\\t{original_text[ind]}\")\nprint()\nprint(\"Transformed:\")\nprint(f\"\\t{transformed_text[ind]}\")\n</code></pre> <pre>\n<code>Original sentence:\n    Then Heywood Sullivan , Kansas City catcher , singled up the middle and Throneberry was across with what proved to be the winning run .\n\nTransformed:\n    heywood sullivan kansa citi catcher singl middl throneberri across prove win run\n</code>\n</pre> <pre><code># Once the text is preprocessed, each sentence is transformed into a vector of word counts.\nvectorizer = CountVectorizer(analyzer=\"word\")\nvectorized_text = vectorizer.fit_transform(transformed_text)\n\nmsg = f\"There are {len(vectorizer.get_feature_names_out())} different words in the corpus, e.g. {vectorizer.get_feature_names_out()[20:30]}.\"\nprint(msg)\n</code></pre> <pre>\n<code>There are 842 different words in the corpus, e.g. ['acid' 'across' 'act' 'activ' 'actual' 'ad' 'adair' 'addit' 'administ'\n 'administr'].\n</code>\n</pre> <p>Note that the vectorized text representation is a (very) sparse matrix.</p> <p>To compare (the vectorized representation of) two sentences, [Choi2000] uses the cosine similarity \\(k_{\\text{cosine}}: \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}\\):</p> \\[ k_{\\text{cosine}}(x, y) := \\frac{\\langle x \\mid y \\rangle}{\\|x\\|\\|y\\|} \\] <p>where \\(x\\) and \\(y\\) are two \\(d\\)-dimensionnal vectors of word counts.</p> <p>Text segmentation now amounts to a kernel change point detection (see [Truong2020] for more details). However, this particular kernel is not implemented in <code>ruptures</code> therefore we need to create a custom cost function. (Actually, it is implemented in <code>ruptures</code> but the current implementation does not exploit the sparse structure of the vectorized text representation and can therefore be slow.)</p> <p>Let \\(y=\\{y_0, y_1,\\dots,y_{T-1}\\}\\) be a \\(d\\)-dimensionnal signal with \\(T\\) samples. Recall that a cost function \\(c(\\cdot)\\) that derives from a kernel \\(k(\\cdot, \\cdot)\\) is such that</p> \\[ c(y_{a..b}) = \\sum_{t=a}^{b-1} G_{t, t} - \\frac{1}{b-a} \\sum_{a \\leq s &lt; b } \\sum_{a \\leq t &lt; b} G_{s,t} \\] <p>where \\(y_{a..b}\\) is the subsignal \\(\\{y_a, y_{a+1},\\dots,y_{b-1}\\}\\) and \\(G_{st}:=k(y_s, y_t)\\) (see [Truong2020] for more details). In other words, \\((G_{st})_{st}\\) is the \\(T\\times T\\) Gram matrix of \\(y\\). Thanks to this formula, we can now implement our custom cost function (named <code>CosineCost</code> in the following cell).</p> <pre><code>class CosineCost(BaseCost):\n    \"\"\"Cost derived from the cosine similarity.\"\"\"\n\n    # The 2 following attributes must be specified for compatibility.\n    model = \"custom_cosine\"\n    min_size = 2\n\n    def fit(self, signal):\n        \"\"\"Set the internal parameter.\"\"\"\n        self.signal = signal\n        self.gram = cosine_similarity(signal, dense_output=False)\n        return self\n\n    def error(self, start, end) -&amp;gt; float:\n        \"\"\"Return the approximation cost on the segment [start:end].\n\n        Args:\n            start (int): start of the segment\n            end (int): end of the segment\n        Returns:\n            segment cost\n        Raises:\n            NotEnoughPoints: when the segment is too short (less than `min_size` samples).\n        \"\"\"\n        if end - start &amp;lt; self.min_size:\n            raise NotEnoughPoints\n        sub_gram = self.gram[start:end, start:end]\n        val = sub_gram.diagonal().sum()\n        val -= sub_gram.sum() / (end - start)\n        return val\n</code></pre> <pre><code>n_bkps = 9  # there are 9 change points (10 text segments)\n\nalgo = rpt.Dynp(custom_cost=CosineCost(), min_size=2, jump=1).fit(vectorized_text)\npredicted_bkps = algo.predict(n_bkps=n_bkps)\n\nprint(f\"True change points are\\t\\t{TRUE_BKPS}.\")\nprint(f\"Detected change points are\\t{predicted_bkps}.\")\n</code></pre> <pre>\n<code>True change points are        [11, 20, 30, 40, 49, 59, 69, 80, 90, 99].\nDetected change points are  [12, 19, 30, 40, 49, 59, 70, 78, 94, 99].\n</code>\n</pre> <p>(Note that the last change point index is simply the length of the signal. This is by design.)</p> <p>Predicted breakpoints are quite close to the true change points. Indeed, most estimated changes are less than one sentence away from a true change. The last change is less accurately predicted with an error of 4 sentences. To overcome this issue, one solution would be to consider a richer representation (compared to the sparse word frequency vectors).</p> <pre><code>true_segment_list = rpt.utils.pairwise([0] + TRUE_BKPS)\npredicted_segment_list = rpt.utils.pairwise([0] + predicted_bkps)\n\nfor n_paragraph, (true_segment, predicted_segment) in enumerate(\n    zip(true_segment_list, predicted_segment_list), start=1\n):\n    print(f\"Paragraph n\u00b0{n_paragraph:02d}\")\n    start_true, end_true = true_segment\n    start_pred, end_pred = predicted_segment\n\n    start = min(start_true, start_pred)\n    end = max(end_true, end_pred)\n    msg = \" \".join(\n        f\"{ind+1:02d}\" if (start_true &amp;lt;= ind &amp;lt; end_true) else \"  \"\n        for ind in range(start, end)\n    )\n    print(f\"(true)\\t{msg}\")\n    msg = \" \".join(\n        f\"{ind+1:02d}\" if (start_pred &amp;lt;= ind &amp;lt; end_pred) else \"  \"\n        for ind in range(start, end)\n    )\n    print(f\"(pred)\\t{msg}\")\n    print()\n</code></pre> <pre>\n<code>Paragraph n\u00b001\n(true)  01 02 03 04 05 06 07 08 09 10 11   \n(pred)  01 02 03 04 05 06 07 08 09 10 11 12\n\nParagraph n\u00b002\n(true)  12 13 14 15 16 17 18 19 20\n(pred)     13 14 15 16 17 18 19   \n\nParagraph n\u00b003\n(true)     21 22 23 24 25 26 27 28 29 30\n(pred)  20 21 22 23 24 25 26 27 28 29 30\n\nParagraph n\u00b004\n(true)  31 32 33 34 35 36 37 38 39 40\n(pred)  31 32 33 34 35 36 37 38 39 40\n\nParagraph n\u00b005\n(true)  41 42 43 44 45 46 47 48 49\n(pred)  41 42 43 44 45 46 47 48 49\n\nParagraph n\u00b006\n(true)  50 51 52 53 54 55 56 57 58 59\n(pred)  50 51 52 53 54 55 56 57 58 59\n\nParagraph n\u00b007\n(true)  60 61 62 63 64 65 66 67 68 69   \n(pred)  60 61 62 63 64 65 66 67 68 69 70\n\nParagraph n\u00b008\n(true)  70 71 72 73 74 75 76 77 78 79 80\n(pred)     71 72 73 74 75 76 77 78      \n\nParagraph n\u00b009\n(true)        81 82 83 84 85 86 87 88 89 90            \n(pred)  79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94\n\nParagraph n\u00b010\n(true)  91 92 93 94 95 96 97 98 99\n(pred)              95 96 97 98 99\n\n</code>\n</pre> <p>Show the Gram matrix.</p> <p>In addition, the text segmentation can be shown on the Gram matrix that was used to detect changes. This is done in the following cell.</p> <p>Most segments (represented by the blue squares) are similar between the true segmentation and the predicted segmentation, except for last two. This is mainly due to the fact that, in the penultimate excerpt, all sentences are dissimilar (with respect to the cosine measure).</p> <pre><code>fig, ax_arr = plt.subplots(nrows=1, ncols=2, figsize=(7, 5), dpi=200)\n\n# plot config\ntitle_fontsize = 10\nlabel_fontsize = 7\ntitle_list = [\"True text segmentation\", \"Predicted text segmentation\"]\n\nfor ax, title, bkps in zip(ax_arr, title_list, [TRUE_BKPS, predicted_bkps]):\n    # plot gram matrix\n    ax.imshow(algo.cost.gram.toarray(), cmap=cm.plasma, norm=LogNorm())\n    # add text segmentation\n    for start, end in rpt.utils.pairwise([0] + bkps):\n        draw_square_on_ax(start=start, end=end, ax=ax)\n    # add labels and title\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Sentence index\", fontsize=label_fontsize)\n    ax.set_ylabel(\"Sentence index\", fontsize=label_fontsize)\n    ax.tick_params(axis=\"both\", which=\"major\", labelsize=label_fontsize)\n</code></pre> <pre><code>for start, end in rpt.utils.pairwise([0] + TRUE_BKPS):\n    excerpt = original_text[start:end]\n    for n_line, sentence in enumerate(excerpt, start=start + 1):\n        sentence = sentence.strip(\"\\n\")\n        print(f\"{n_line:&amp;gt;2}: {sentence}\")\n    print()\n</code></pre> <pre>\n<code> 1: The Sane Society is an ambitious work .\n 2: Its scope is as broad as the question : What does it mean to live in modern society ? ?\n 3: A work so broad , even when it is directed by a leading idea and informed by a moral vision , must necessarily `` fail '' .\n 4: Even a hasty reader will easily find in it numerous blind spots , errors of fact and argument , important exclusions , areas of ignorance and prejudice , undue emphases on trivia , examples of broad positions supported by flimsy evidence , and the like .\n 5: Such books are easy prey for critics .\n 6: Nor need the critic be captious .\n 7: A careful and orderly man , who values precision and a kind of tough intellectual responsibility , might easily be put off by such a book .\n 8: It is a simple matter , for one so disposed , to take a work like The Sane Society and shred it into odds and ends .\n 9: The thing can be made to look like the cluttered attic of a large and vigorous family -- a motley jumble of discarded objects , some outworn and some that were never useful , some once whole and bright but now chipped and tarnished , some odd pieces whose history no one remembers , here and there a gem , everything fascinating because it suggests some part of the human condition -- the whole adding up to nothing more than a glimpse into the disorderly history of the makers and users .\n10: That could be easily done , but there is little reason in it .\n11: It would come down to saying that Fromm paints with a broad brush , and that , after all , is not a conclusion one must work toward but an impression he has from the outset .\n\n12: the effect of the digitalis glycosides is inhibited by a high concentration of potassium in the incubation medium and is enhanced by the absence of potassium ( Wolff , 1960 ) .\n13: B. Organification of iodine The precise mechanism for organification of iodine in the thyroid is not as yet completely understood .\n14: However , the formation of organically bound iodine , mainly mono-iodotyrosine , can be accomplished in cell-free systems .\n15: In the absence of additions to the homogenate , the product formed is an iodinated particulate protein ( Fawcett and Kirkwood , 1953 ; ; Taurog , Potter and Chaikoff , 1955 ; ; Taurog , Potter , Tong , and Chaikoff , 1956 ; ; Serif and Kirkwood , 1958 ; ; De Groot and Carvalho , 1960 ) .\n16: This iodoprotein does not appear to be the same as what is normally present in the thyroid , and there is no evidence so far that thyroglobulin can be iodinated in vitro by cell-free systems .\n17: In addition , the iodoamino acid formed in largest quantity in the intact thyroid is di-iodotyrosine .\n18: If tyrosine and a system generating hydrogen peroxide are added to a cell-free homogenate of the thyroid , large quantities of free mono-iodotyrosine can be formed ( Alexander , 1959 ) .\n19: It is not clear whether this system bears any resemblance to the in vivo iodinating mechanism , and a system generating peroxide has not been identified in thyroid tissue .\n20: On chemical grounds it seems most likely that iodide is first converted to Afj and then to Afj as the active iodinating species .\n\n21: the statement empirical , for goodness was not a quality like red or squeaky that could be seen or heard .\n22: What were they to do , then , with these awkward judgments of value ? ?\n23: To find a place for them in their theory of knowledge would require them to revise the theory radically , and yet that theory was what they regarded as their most important discovery .\n24: It appeared that the theory could be saved in one way only .\n25: If it could be shown that judgments of good and bad were not judgments at all , that they asserted nothing true or false , but merely expressed emotions like `` Hurrah '' or `` Fiddlesticks '' , then these wayward judgments would cease from troubling and weary heads could be at rest .\n26: This is the course the positivists took .\n27: They explained value judgments by explaining them away .\n28: Now I do not think their view will do .\n29: But before discussing it , I should like to record one vote of thanks to them for the clarity with which they have stated their case .\n30: It has been said of John Stuart Mill that he wrote so clearly that he could be found out .\n\n31: Greer Garson , world-famous star of stage , screen and television , will be honored for the high standard in tasteful sophisticated fashion with which she has created a high standard in her profession .\n32: As a Neiman-Marcus award winner the titian-haired Miss Garson is a personification of the individual look so important to fashion this season .\n33: She will receive the 1961 `` Oscar '' at the 24th annual Neiman-Marcus Exposition , Tuesday and Wednesday in the Grand Ballroom of the Sheraton-Dallas Hotel .\n34: The only woman recipient , Miss Garson will receive the award with Ferdinando Sarmi , creator of chic , beautiful women 's fashions ; ; Harry Rolnick , president of the Byer-Rolnick Hat Corporation and designer of men 's hats ; ; Sydney Wragge , creator of sophisticated casuals for women and Roger Vivier , designer of Christian Dior shoes Paris , France , whose squared toes and lowered heels have revolutionized the shoe industry .\n35: The silver and ebony plaques will be presented at noon luncheons by Stanley Marcus , president of Neiman-Marcus , Beneficiary of the proceeds from the two showings will be the Dallas Society for Crippled Children Cerebral Palsy Treatment Center .\n36: The attractive Greer Garson , who loves beautiful clothes and selects them as carefully as she does her professional roles , prefers timeless classical designs .\n37: Occasionally she deserts the simple and elegant for a fun piece simply because `` It 's unlike me '' .\n38: In private life , Miss Garson is Mrs. E.E. Fogelson and on the go most of the time commuting from Dallas , where they maintain an apartment , to their California home in Los Angeles ' suburban Bel-Air to their ranch in Pecos , New Mexico .\n39: Therefore , her wardrobe is largely mobile , to be packed at a moment 's notice and to shake out without a wrinkle .\n40: Her creations in fashion are from many designers because she does n't want a complete wardrobe from any one designer any more than she wants `` all of her pictures by one painter '' .\n\n41: Wage-price policies of industry are the result of a complex of forces -- no single explanation has been found which applies to all cases .\n42: The purpose of this paper is to analyze one possible force which has not been treated in the literature , but which we believe makes a significant contribution to explaining the wage-price behavior of a few very important industries .\n43: While there may be several such industries to which the model of this paper is applicable , the authors make particular claim of relevance to the explanation of the course of wages and prices in the steel industry of the United States since World War 2 .\n44: Indeed , the apparent stiffening of the industry 's attitude in the recent steel strike has a direct explanation in terms of the model here presented .\n45: The model of this paper considers an industry which is not characterized by vigorous price competition , but which is so basic that its wage-price policies are held in check by continuous critical public scrutiny .\n46: Where the industry 's product price has been kept below the `` profit-maximizing '' and `` entry-limiting '' prices due to fears of public reaction , the profit seeking producers have an interest in offering little real resistance to wage demands .\n47: The contribution of this paper is a demonstration of this proposition , and an exploration of some of its implications .\n48: In order to focus clearly upon the operation of this one force , which we may call the effect of `` public-limit pricing '' on `` key '' wage bargains , we deliberately simplify the model by abstracting from other forces , such as union power , which may be relevant in an actual situation .\n49: For expository purposes , this is best treated as a model which spells out the conditions under which an important industry affected with the public interest would find it profitable to raise wages even in the absence of union pressures for higher wages .\n\n50: The vast Central Valley of California is one of the most productive agricultural areas in the world .\n51: During the summer of 1960 , it became the setting for a bitter and basic labor-management struggle .\n52: The contestants in this economic struggle are the Agricultural Workers Organizing Committee ( AWOC ) of the AFL-CIO and the agricultural employers of the State .\n53: By virtue of the legal responsibilities of the Department of Employment in the farm placement program , we necessarily found ourselves in the middle between these two forces .\n54: It is not a pleasant or easy position , but one we have endeavored to maintain .\n55: We have sought to be strictly neutral as between the parties , but at the same time we have been required frequently to rule on specific issues or situations as they arose .\n56: Inevitably , one side was pleased and the other displeased , regardless of how we ruled .\n57: Often the displeased parties interpreted our decision as implying favoritism toward the other .\n58: We have consoled ourselves with the thought that this is a normal human reaction and is one of the consequences of any decision in an adversary proceeding .\n59: It is disconcerting , nevertheless , to read in a labor weekly , `` Perluss knuckles down to growers '' , and then to be confronted with a growers ' publication which states , `` Perluss recognizes obviously phony and trumped-up strikes as bona fide '' .\n\n60: Rookie Ron Nischwitz continued his pinpoint pitching Monday night as the Bears made it two straight over Indianapolis , 5-3 .\n61: The husky 6-3 , 205-pound lefthander , was in command all the way before an on-the-scene audience of only 949 and countless of television viewers in the Denver area .\n62: It was Nischwitz ' third straight victory of the new season and ran the Grizzlies ' winning streak to four straight .\n63: They now lead Louisville by a full game on top of the American Association pack .\n64: Nischwitz fanned six and walked only Charley Hinton in the third inning .\n65: He has given only the one pass in his 27 innings , an unusual characteristic for a southpaw .\n66: The Bears took the lead in the first inning , as they did in Sunday 's opener , and never lagged .\n67: Dick McAuliffe cracked the first of his two doubles against Lefty Don Rudolph to open the Bear 's attack .\n68: After Al Paschal gruonded out , Jay Cooke walked and Jim McDaniel singled home McAuliffe .\n69: Alusik then moved Cooke across with a line drive to left .\n\n70: Unemployed older workers who have no expectation of securing employment in the occupation in which they are skilled should be able to secure counseling and retraining in an occupation with a future .\n71: Some vocational training schools provide such training , but the current need exceeds the facilities .\n72: Current programs The present Federal program of vocational education began in 1917 with the passage of the Smith-Hughes Act , which provided a continuing annual appropriation of $ 7 million to support , on a matching basis , state-administered programs of vocational education in agriculture , trades , industrial skills and home economics .\n73: Since 1917 some thirteen supplementary and related acts have extended this Federal program .\n74: The George-Barden Act of 1946 raised the previous increases in annual authorizations to $ 29 million in addition to the $ 7 million under the Smith Act .\n75: The Health Amendment Act of 1956 added $ 5 million for practical nurse training .\n76: The latest major change in this program was introduced by the National Defense Education Act of 1958 , Title 8 , of which amended the George-Barden Act .\n77: Annual authorizations of $ 15 million were added for area vocational education programs that meet national defense needs for highly skilled technicians .\n78: The Federal program of vocational education merely provides financial aid to encourage the establishment of vocational education programs in public schools .\n79: The initiative , administration and control remain primarily with the local school districts .\n80: Even the states remain primarily in an assisting role , providing leadership and teacher training .\n\n81: briefly , the topping configuration must be examined for its inferences .\n82: Then the fact that the lower channel line was pierced had further forecasting significance .\n83: And then the application of the count rules to the width ( horizontally ) of the configuration gives us an intial estimate of the probable depth of the decline .\n84: The very idea of there being `` count rules '' implies that there is some sort of proportion to be expected between the amount of congestive activity and the extent of the breakaway ( run up or run down ) movement .\n85: This expectation is what really `` sold '' point and figure .\n86: But there is no positive and consistently demonstrable relationship in the strictest sense .\n87: Experience will show that only the vaguest generalities apply , and in fine , these merely dwell upon a relationship between the durations and intensities of events .\n88: After all , too much does not happen too suddenly , nor does very little take long .\n89: The advantages and disadvantages of these two types of charting , bar charting and point and figure charting , remain the subject of fairly good-natured litigation among their respective professional advocates , with both methods enjoying in common , one irrevocable merit .\n90: They are both trend-following methods .\n\n91: Miami , Fla. , March 17 -- The Orioles tonight retained the distinction of being the only winless team among the eighteen Major-League clubs as they dropped their sixth straight spring exhibition decision , this one to the Kansas City Athletics by a score of 5 to 3 .\n92: Indications as late as the top of the sixth were that the Birds were to end their victory draught as they coasted along with a 3-to-o advantage .\n93: Siebern hits homer Over the first five frames , Jack Fisher , the big righthander who figures to be in the middle of Oriole plans for a drive on the 1961 American League pennant , held the A 's scoreless while yielding three scattered hits .\n94: Then Dick Hyde , submarine-ball hurler , entered the contest and only five batters needed to face him before there existed a 3-to-3 deadlock .\n95: A two-run homer by Norm Siebern and a solo blast by Bill Tuttle tied the game , and single runs in the eighth and ninth gave the Athletics their fifth victory in eight starts .\n96: House throws wild With one down in the eighth , Marv Throneberry drew a walk and stole second as Hyde fanned Tuttle .\n97: Catcher Frank House 's throw in an effort to nab Throneberry was wide and in the dirt .\n98: Then Heywood Sullivan , Kansas City catcher , singled up the middle and Throneberry was across with what proved to be the winning run .\n99: Rookie southpaw George Stepanovich relieved Hyde at the start of the ninth and gave up the A 's fifth tally on a walk to second baseman Dick Howser , a wild pitch , and Frank Cipriani 's single under Shortstop Jerry Adair 's glove into center .\n\n</code>\n</pre>"},{"location":"examples/text-segmentation/#linear-text-segmentation","title":"Linear text segmentation","text":"<p>Info</p> <ul> <li>Try this notebook in an executable environment with Binder.</li> <li>Download this notebook here.</li> </ul>"},{"location":"examples/text-segmentation/#introduction","title":"Introduction","text":"<p>Linear text segmentation consists in dividing a text into several meaningful segments. Linear text segmentation can be seen as a change point detection task and therefore can be carried out with <code>ruptures</code>.  This example performs exactly that on a well-known data set intoduced in [Choi2000].</p>"},{"location":"examples/text-segmentation/#setup","title":"Setup","text":"<p>First we import packages and define a few utility functions. This section can be skipped at first reading.</p> <p>Library imports.</p>"},{"location":"examples/text-segmentation/#data","title":"Data","text":""},{"location":"examples/text-segmentation/#description","title":"Description","text":"<p>The text to segment is a concatenation of excerpts from ten different documents randomly selected from the so-called Brown corpus (described here). Each excerpt has nine to eleven sentences, amounting to 99 sentences in total. The complete text is shown in Appendix A.</p> <p>These data stem from a larger data set which is thoroughly described in [Choi2000] and can be downloaded here. This is a common benchmark to evaluate text segmentation methods.</p>"},{"location":"examples/text-segmentation/#preprocessing","title":"Preprocessing","text":"<p>Before performing text segmentation, the original text is preprocessed. In a nutshell (see [Choi2000] for more details),</p> <ul> <li>the punctuation and stopwords are removed;</li> <li>words are reduced to their stems (e.g., \"waited\" and \"waiting\" become \"wait\");</li> <li>a vector of word counts is computed.</li> </ul>"},{"location":"examples/text-segmentation/#text-segmentation","title":"Text segmentation","text":""},{"location":"examples/text-segmentation/#cost-function","title":"Cost function","text":""},{"location":"examples/text-segmentation/#compute-change-points","title":"Compute change points","text":"<p>If the number \\(K\\) of change points is assumed to be known, we can use dynamic programming to search for the exact segmentation \\(\\hat{t}_1,\\dots,\\hat{t}_K\\) that minimizes the sum of segment costs:</p> \\[ \\hat{t}_1,\\dots,\\hat{t}_K := \\text{arg}\\min_{t_1,\\dots,t_K} \\left[ c(y_{0..t_1}) + c(y_{t_1..t_2}) + \\dots + c(y_{t_K..T}) \\right]. \\]"},{"location":"examples/text-segmentation/#visualize-segmentations","title":"Visualize segmentations","text":"<p>Show sentence numbers.</p> <p>In the following cell, the two segmentations (true and predicted) can be visually compared. For each paragraph, the sentence numbers are shown.</p>"},{"location":"examples/text-segmentation/#conclusion","title":"Conclusion","text":"<p>This example shows how to apply <code>ruptures</code> on a text segmentation task. In detail, we detected shifts in the vocabulary of a collection of sentences using common NLP preprocessing and transformation. This task amounts to a kernel change point detection procedure where the kernel is the cosine kernel.</p> <p>Such results can then be used to characterize the structure of the text for subsequent NLP tasks. This procedure should certainly be enriched with more relevant and compact representations to better detect changes.</p>"},{"location":"examples/text-segmentation/#appendix-a","title":"Appendix A","text":"<p>The complete text used in this notebook is as follows. Note that the line numbers and the blank lines (added to visually mark the boundaries between excerpts) are not part of the text fed to the segmentation method.</p>"},{"location":"examples/text-segmentation/#authors","title":"Authors","text":"<p>This example notebook has been authored by Olivier Boulant and edited by Charles Truong.</p>"},{"location":"examples/text-segmentation/#references","title":"References","text":"<p>[Choi2000] Choi, F. Y. Y. (2000). Advances in domain independent linear text segmentation. Proceedings of the North American Chapter of the Association for Computational Linguistics Conference (NAACL), 26\u201333.</p> <p>[Truong2020] Truong, C., Oudre, L., &amp; Vayatis, N. (2020). Selective review of offline change point detection methods. Signal Processing, 167.</p>"},{"location":"getting-started/basic-usage/","title":"Basic usage","text":"<pre><code>import matplotlib.pyplot as plt  # for display purposes\n\nimport ruptures as rpt  # our package\n</code></pre> <pre><code>n_samples, n_dims, sigma = 1000, 3, 2\nn_bkps = 4  # number of breakpoints\nsignal, bkps = rpt.pw_constant(n_samples, n_dims, n_bkps, noise_std=sigma)\n</code></pre> <p>The true change points of this synthetic signal are available in the <code>bkps</code> variable.</p> <pre><code>print(bkps)\n</code></pre> <pre>\n<code>[202, 393, 585, 784, 1000]\n</code>\n</pre> <p>Note that the first four element are change point indexes while the last is simply the number of samples. (This is a technical convention so that functions in <code>ruptures</code> always know the length of the signal at hand.)</p> <p>It is also possible to plot our \\(\\mathbb{R}^3\\)-valued signal along with the true change points with the <code>rpt.display</code> function. In the following image, the color changes whenever the mean of the signal shifts.</p> <pre><code>fig, ax_array = rpt.display(signal, bkps)\n</code></pre> <pre><code># detection\nalgo = rpt.Dynp(model=\"l2\").fit(signal)\nresult = algo.predict(n_bkps=4)\n\nprint(result)\n</code></pre> <pre>\n<code>[200, 395, 585, 785, 1000]\n</code>\n</pre> <p>Again the first elements are change point indexes and the last is the number of samples.</p> <p>To visualy compare the true segmentation (<code>bkps</code>) and the estimated one (<code>result</code>), we can resort to <code>rpt.display</code> a second time. In the following image, the alternating colors indicate the true breakpoints and the dashed vertical lines, the estimated breakpoints.</p> <pre><code># display\nrpt.display(signal, bkps, result)\nplt.show()\n</code></pre> <p>In this simple example, both are quite similar and almost undistinguishable.</p>"},{"location":"getting-started/basic-usage/#basic-usage","title":"Basic usage","text":"<p>Info</p> <ul> <li>Try this notebook in an executable environment with Binder.</li> <li>Download this notebook here.</li> </ul> <p>Let us start with a simple example to illustrate the use of <code>ruptures</code>: generate a 3-dimensional piecewise constant signal with noise and estimate the change points.</p>"},{"location":"getting-started/basic-usage/#setup","title":"Setup","text":"<p>First, we make the necessary imports.</p>"},{"location":"getting-started/basic-usage/#generate-and-display-the-signal","title":"Generate and display the signal","text":"<p>Let us generate a 3-dimensional piecewise constant signal with Gaussian noise.</p>"},{"location":"getting-started/basic-usage/#change-point-detection","title":"Change point detection","text":"<p>We can now perform change point detection, meaning that we find the indexes where the signal mean changes. To that end, we minimize the sum of squared errors when approximating the signal by a piecewise constant signal. Formally, for a signal \\(y_0,y_1,\\dots,y_{T-1}\\) (\\(T\\) samples), we solve the following optimization problem, over all possible change positions \\(t_1&lt;t_2&lt;\\dots&lt;t_k\\) $\\(=\"\" \\(\\bar{y}_{t_k..t_{k+1}}\\)=\"\" \\(k\\)=\"\" \\(t_0=\"0\\)\" \\(t_{k+1}=\"T\\).)\" (y_{t_k},=\"\" (by=\"\" (more=\"\" (where=\"\" :=\"\\sum_{k=0}^K\\sum_{t=t_k}^{t_{k+1}-1}\" &lt;=\"\" [<code>dynp</code>](..=\"\" [user=\"\" [what=\"\" \\hat{t}_1,=\"\" \\hat{t}_2,\\dots,\\hat{t}_k=\"\\arg\\min_{t_1,\\dots,t_K}\" \\|y_t-\\bar{y}_{t_k..t_{k+1}}\\|^2=\"\" and=\"\" by=\"\" change=\"\" changes=\"\" class.=\"\" convention=\"\" defined=\"\" detection=\"\" detection?](=\"\" div=\"\" dynamic=\"\" dynp.md)=\"\" empirical=\"\" guide](=\"\" in=\"\" information=\"\" is=\"\" mean=\"\" number=\"\" of=\"\" optimization=\"\" point=\"\" programming,=\"\" section=\"\" solved=\"\" sub-signal=\"\" the=\"\" this=\"\" user):=\"\" user-guide=\"\" user-guide).)=\"\" using=\"\" v(t_1,t_2,\\dots,t_k)=\"\" what-is-cpd)=\"\" where=\"\" with=\"\" y_{t_k+1},\\dots,y_{t_{k+1}-1}\\).=\"\"&gt; &lt;/t_2&lt;\\dots&lt;t_k\\)&gt;</p>"},{"location":"getting-started/basic-usage/#display-the-results","title":"Display the results","text":""},{"location":"user-guide/","title":"User guide","text":"<p>This section describes the algorithms and utility functions of <code>ruptures</code>. Each entry of the user guide is linked to a companion entry in the Code reference section, where the API is detailed.</p>"},{"location":"user-guide/evaluation/","title":"Evaluation and visualization","text":""},{"location":"user-guide/costs/costautoregressive/","title":"Autoregressive model change (<code>CostAR</code>)","text":""},{"location":"user-guide/costs/costautoregressive/#description","title":"Description","text":"<p>Let \\(0&lt;t_1&lt;t_2&lt;\\dots&lt;n\\) be unknown change points indexes. Consider the following piecewise autoregressive model</p> \\[     y_t = z_t' \\delta_j + \\varepsilon_t, \\quad \\forall t=t_j,\\dots,t_{j+1}-1 \\] <p>where \\(j&gt;1\\) is the segment number, \\(z_t=[y_{t-1}, y_{t-2},\\dots,y_{t-p}]\\) is the lag vector,and \\(p&gt;0\\) is the order of the process.</p> <p>The least-squares estimates of the break dates is obtained by minimizing the sum of squared residuals [Bai2000]. Formally, the associated cost function on an interval \\(I\\) is</p> \\[ c(y_{I}) = \\min_{\\delta\\in\\mathbb{R}^p} \\sum_{t\\in I} \\|y_t - \\delta' z_t \\|_2^2. \\] <p>Currently, this function is limited to 1D signals.</p>"},{"location":"user-guide/costs/costautoregressive/#usage","title":"Usage","text":"<p>Start with the usual imports and create a signal with piecewise linear trends.</p> <pre><code>from itertools import cycle\nimport numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\nn = 2000\nn_bkps, sigma = 4, 0.5  # number of change points, noise standart deviation\nbkps = [400, 1000, 1300, 1800, n]\nf1 = np.array([0.075, 0.1])\nf2 = np.array([0.1, 0.125])\nfreqs = np.zeros((n, 2))\nfor sub, val in zip(np.split(freqs, bkps[:-1]), cycle([f1, f2])):\n    sub += val\ntt = np.arange(n)\nsignal = np.sum((np.sin(2 * np.pi * tt * f) for f in freqs.T))\nsignal += np.random.normal(scale=sigma, size=signal.shape)\n# display signal\nrpt.show.display(signal, bkps, figsize=(10, 6))\nplt.show()\n</code></pre> <p>Then create a CostAR instance and print the cost of the sub-signal <code>signal[50:150]</code>. The autoregressive order can be specified through the keyword <code>'order'</code>.</p> <pre><code>c = rpt.costs.CostAR(order=10).fit(signal)\nprint(c.error(50, 150))\n</code></pre> <p>You can also compute the sum of costs for a given list of change points.</p> <pre><code>print(c.sum_of_costs(bkps))\nprint(c.sum_of_costs([10, 100, 200, 250, n]))\n</code></pre> <p>In order to use this cost class in a change point detection algorithm (inheriting from BaseEstimator), either pass a CostAR instance (through the argument <code>'custom_cost'</code>) or set <code>model=\"ar\"</code>. Additional parameters can be passed to the cost instance through the keyword <code>'params'</code>.</p> <pre><code>c = rpt.costs.CostAR(order=10)\nalgo = rpt.Dynp(custom_cost=c)\n# is equivalent to\nalgo = rpt.Dynp(model=\"ar\", params={\"order\": 10})\n</code></pre>"},{"location":"user-guide/costs/costautoregressive/#reference","title":"Reference","text":"<p>[Bai2000] Bai, J. (2000). Vector autoregressive models with structural changes in regression coefficients and in variance\u2013covariance matrices. Annals of Economics and Finance, 1(2), 301\u2013336.</p>"},{"location":"user-guide/costs/costclinear/","title":"Continuous linear change (<code>CostCLinear</code>)","text":""},{"location":"user-guide/costs/costclinear/#description","title":"Description","text":"<p>For a given set of indexes (also called knots) \\(t_k\\) (\\(k=1,\\dots,K\\)), a linear spline \\(f\\) is such that:</p> <ol> <li>\\(f\\) is affine on each interval \\(t_k..t_{k+1}\\), i.e. \\(f(t)=\\alpha_k (t-t_k) + \\beta_k\\) (\\(\\alpha_k, \\beta_k \\in \\mathbb{R}^d\\)) for all \\(t=t_k,t_k+1,\\dots,t_{k+1}-1\\);</li> <li>\\(f\\) is continuous.</li> </ol> <p>The cost function <code>CostCLinear</code> measures the error when approximating the signal with a linear spline. Formally, it is defined for \\(0&lt;a&lt;b\\leq T\\) by</p> \\[ c(y_{a..b}) := \\sum_{t=a}^{b-1} \\left\\lVert y_t - y_{a-1} - \\frac{t-a+1}{b-a}(y_{b-1}-y_{a-1}) \\right\\rVert^2 \\] <p>and \\(c(y_{0..b}):=c(y_{1..b})\\) (by convention).</p>"},{"location":"user-guide/costs/costclinear/#usage","title":"Usage","text":"<p>Start with the usual imports and create a signal with piecewise linear trends.</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\nn_samples, n_dims = 500, 3  # number of samples, dimension\nn_bkps, sigma = 3, 5  # number of change points, noise standard deviation\nsignal, bkps = rpt.pw_constant(n_samples, n_dims, n_bkps, noise_std=sigma)\nsignal = np.cumsum(signal, axis=1)\n</code></pre> <p>Then create a <code>CostCLinear</code> instance and print the cost of the sub-signal <code>signal[50:150]</code>.</p> <pre><code>c = rpt.costs.CostCLinear().fit(signal)\nprint(c.error(50, 150))\n</code></pre> <p>You can also compute the sum of costs for a given list of change points.</p> <pre><code>print(c.sum_of_costs(bkps))\nprint(c.sum_of_costs([10, 100, 200, 250, n]))\n</code></pre> <p>In order to use this cost class in a change point detection algorithm (inheriting from <code>BaseEstimator</code>), either pass a <code>CostCLinear</code> instance (through the argument <code>custom_cost</code>) or set <code>model=\"clinear\"</code>.</p> <pre><code>c = rpt.costs.CostCLinear()\nalgo = rpt.Dynp(custom_cost=c)\n# is equivalent to\nalgo = rpt.Dynp(model=\"clinear\")\n</code></pre>"},{"location":"user-guide/costs/costcosine/","title":"Kernelized mean change (<code>CostCosine</code>)","text":""},{"location":"user-guide/costs/costcosine/#description","title":"Description","text":"<p>Given a positive semi-definite kernel \\(k(\\cdot, \\cdot) : \\mathbb{R}^d\\times \\mathbb{R}^d \\mapsto \\mathbb{R}\\) and its associated feature map \\(\\Phi:\\mathbb{R}^d \\mapsto \\mathcal{H}\\) (where \\(\\mathcal{H}\\) is an appropriate Hilbert space), this cost function detects changes in the mean of the embedded signal \\(\\{\\Phi(y_t)\\}_t\\) [Arlot2019]. Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\(I\\),</p> \\[ c(y_{a..b}) = \\sum_{t=a}^{b-1} \\| \\Phi(y_t) - \\bar{\\mu} \\|_{\\mathcal{H}}^2 \\] <p>where \\(\\bar{\\mu}_{a..b}\\) is the empirical mean of the embedded sub-signal \\(\\{\\Phi(y_t)\\}_{a\\leq t &lt; b-1}\\). Here the kernel is the cosine similarity:</p> \\[ k(x, y) = \\frac{\\langle x\\mid y\\rangle}{\\|x\\|\\|y\\|} \\] <p>where \\(\\langle \\cdot\\mid\\cdot \\rangle\\) and \\(\\| \\cdot \\|\\) are the Euclidean scalar product and norm respectively. In other words, it is equal to the L2-normalized dot product of vectors. This cost function has been used for music segmentation tasks [Cooper2002] and topic segmentation of text [Hearst1994].</p>"},{"location":"user-guide/costs/costcosine/#usage","title":"Usage","text":"<p>Start with the usual imports and create a signal.</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\nn, dim = 500, 3  # number of samples, dimension\nn_bkps, sigma = 3, 5  # number of change points, noise standart deviation\nsignal, bkps = rpt.pw_constant(n, dim, n_bkps, noise_std=sigma)\n</code></pre> <p>Then create a <code>CostCosine</code> instance and print the cost of the sub-signal <code>signal[50:150]</code>.</p> <pre><code>c = rpt.costs.CostCosine().fit(signal)\nprint(c.error(50, 150))\n</code></pre> <p>You can also compute the sum of costs for a given list of change points.</p> <pre><code>print(c.sum_of_costs(bkps))\nprint(c.sum_of_costs([10, 100, 200, 250, n]))\n</code></pre> <p>In order to use this cost class in a change point detection algorithm (inheriting from <code>BaseEstimator</code>), either pass a <code>CostCosine</code> instance (through the argument <code>custom_cost</code>) or set <code>model=\"cosine\"</code>.</p> <pre><code>c = rpt.costs.CostCosine()\nalgo = rpt.Dynp(custom_cost=c)\n# is equivalent to\nalgo = rpt.Dynp(model=\"cosine\")\n</code></pre>"},{"location":"user-guide/costs/costcosine/#references","title":"References","text":"<p>[Hearst1994] Hearst, M. A. (1994). Multi-paragraph segmentation of expository text. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (pp. 9\u201316). Las Cruces, New Mexico, USA.</p> <p>[Cooper2002] Cooper, M., &amp; Foote, J. (2002). Automatic music summarization via similarity analysis. In Proceedings of the International Conference on Music Information Retrieval (ISMIR) (pp. 81\u201385). Paris, France.</p> <p>[Arlot2019] Arlot, S., Celisse, A., &amp; Harchaoui, Z. (2019). A kernel multiple change-point algorithm via model selection. Journal of Machine Learning Research, 20(162), 1\u201356.</p>"},{"location":"user-guide/costs/costcustom/","title":"Custom cost class","text":"<p>Users who are interested in detecting a specific type of change can easily do so by creating a custom cost function. Provided, they subclass the base cost function <code>BaseCost</code>, they will be able to seamlessly run the algorithms implemented in <code>ruptures</code>.</p> <p>Important</p> <p>The custom cost class must at least implement the two following methods: <code>.fit(signal)</code> and <code>.error(start, end)</code> (see user guide).</p>"},{"location":"user-guide/costs/costcustom/#example","title":"Example","text":"<p>Let \\(\\{y_t\\}_t\\) denote a 1D piecewise stationary random process. Assume that the \\(y_t\\) are independent and exponentially distributed with a scale parameter that shifts at some unknown instants \\(t_1,t_2,\\dots\\) The change points estimates are the minimizers of the negative log-likelihood, and the associated cost function is given by</p> \\[ c(y_I) = |I| \\log \\bar{\\mu}_I \\] <p>where \\(I,\\, y_I\\) and \\(\\bar{\\mu}_I\\) are respectively an interval, the sub-signal on this interval and the empirical mean of this sub-signal. The following code implements this cost function:</p> <pre><code>from math import log\nfrom ruptures.base import BaseCost\n\n\nclass MyCost(BaseCost):\n\n    \"\"\"Custom cost for exponential signals.\"\"\"\n\n    # The 2 following attributes must be specified for compatibility.\n    model = \"\"\n    min_size = 2\n\n    def fit(self, signal):\n        \"\"\"Set the internal parameter.\"\"\"\n        self.signal = signal\n        return self\n\n    def error(self, start, end):\n        \"\"\"Return the approximation cost on the segment [start:end].\n\n        Args:\n            start (int): start of the segment\n            end (int): end of the segment\n\n        Returns:\n            float: segment cost\n        \"\"\"\n        sub = self.signal[start:end]\n        return (end - start) * log(sub.mean())\n</code></pre> <p>Warning</p> <p>For compatibility reasons, the static attributes <code>model</code> and <code>min_size</code> must be explicitly specified:</p> <ul> <li><code>model</code> is simply a string containing the name of the cost function (can be empty);</li> <li><code>min_size</code> is a positive integer that indicates the minimum segment size (in number of samples) on which the cost function can be applied.</li> </ul> <p>This cost function can now be used with all algorithms from <code>ruptures</code>. For instance,</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\na = np.random.exponential(scale=1, size=100)\nb = np.random.exponential(scale=2, size=200)\nsignal, bkps = np.r_[a, b, a], [100, 300, 400]\n# cost\nalgo = rpt.Pelt(custom_cost=MyCost()).fit(signal)\nmy_bkps = algo.predict(pen=10)\n# display\nrpt.display(signal, bkps, my_bkps)\nplt.show()\n</code></pre>"},{"location":"user-guide/costs/costl1/","title":"Least absolute deviation (<code>CostL1</code>)","text":""},{"location":"user-guide/costs/costl1/#description","title":"Description","text":"<p>This cost function detects changes in the median of a signal. Overall, it is a robust estimator of a shift in the central point (mean, median, mode) of a distribution [Bai1995]. Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\(I\\),</p> \\[ c(y_{I}) = \\sum_{t\\in I} \\|y_t - \\bar{y}\\|_1 \\] <p>where \\(\\bar{y}\\) is the componentwise median of \\(\\{y_t\\}_{t\\in I}\\).</p>"},{"location":"user-guide/costs/costl1/#usage","title":"Usage","text":"<p>Start with the usual imports and create a signal.</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\nn, dim = 500, 3  # number of samples, dimension\nn_bkps, sigma = 3, 5  # number of change points, noise standart deviation\nsignal, bkps = rpt.pw_constant(n, dim, n_bkps, noise_std=sigma)\n</code></pre> <p>Then create a <code>CostL1</code> instance and print the cost of the sub-signal <code>signal[50:150]</code>.</p> <pre><code>c = rpt.costs.CostL1().fit(signal)\nprint(c.error(50, 150))\n</code></pre> <p>You can also compute the sum of costs for a given list of change points.</p> <pre><code>print(c.sum_of_costs(bkps))\nprint(c.sum_of_costs([10, 100, 200, 250, n]))\n</code></pre> <p>In order to use this cost class in a change point detection algorithm (inheriting from <code>BaseEstimator</code>, either pass a <code>CostL1</code> instance (through the argument <code>custom_cost</code>) or set <code>model=\"l1\"</code>.</p> <pre><code>c = rpt.costs.CostL1()\nalgo = rpt.Dynp(custom_cost=c)\n# is equivalent to\nalgo = rpt.Dynp(model=\"l1\")\n</code></pre>"},{"location":"user-guide/costs/costl1/#references","title":"References","text":"<p>[Bai1995] Bai, J. (1995). Least absolute deviation of a shift. Econometric Theory, 11(3), 403\u2013436.</p>"},{"location":"user-guide/costs/costl2/","title":"Least squared deviation (<code>CostL2</code>)","text":""},{"location":"user-guide/costs/costl2/#description","title":"Description","text":"<p>This cost function detects mean-shifts in a signal. Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\(I\\),</p> \\[ c(y_{I}) = \\sum_{t\\in I} \\|y_t - \\bar{y}\\|_2^2 \\] <p>where \\(\\bar{y}\\) is the mean of \\(\\{y_t\\}_{t\\in I}\\).</p>"},{"location":"user-guide/costs/costl2/#usage","title":"Usage","text":"<p>Start with the usual imports and create a signal.</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\nn, dim = 500, 3  # number of samples, dimension\nn_bkps, sigma = 3, 5  # number of change points, noise standart deviation\nsignal, bkps = rpt.pw_constant(n, dim, n_bkps, noise_std=sigma)\n</code></pre> <p>Then create a <code>CostL2</code> instance and print the cost of the sub-signal <code>signal[50:150]</code>.</p> <pre><code>c = rpt.costs.CostL2().fit(signal)\nprint(c.error(50, 150))\n</code></pre> <p>You can also compute the sum of costs for a given list of change points.</p> <pre><code>print(c.sum_of_costs(bkps))\nprint(c.sum_of_costs([10, 100, 200, 250, n]))\n</code></pre> <p>In order to use this cost class in a change point detection algorithm (inheriting from <code>BaseEstimator</code>), either pass a <code>CostL2</code> instance (through the argument <code>custom_cost</code>) or set <code>model=\"l2\"</code>.</p> <pre><code>c = rpt.costs.CostL2()\nalgo = rpt.Dynp(custom_cost=c)\n# is equivalent to\nalgo = rpt.Dynp(model=\"l2\")\n</code></pre>"},{"location":"user-guide/costs/costlinear/","title":"Linear model change (<code>CostLinear</code>)","text":""},{"location":"user-guide/costs/costlinear/#description","title":"Description","text":"<p>Let \\(0 &lt; t_1 &lt; t_2 &lt; \\dots &lt; n\\) be unknown change points indexes. Consider the following multiple linear regression model</p> \\[ y_t = x_t' \\delta_j + \\varepsilon_t, \\quad \\forall t=t_j,\\dots,t_{j+1}-1 \\] <p>for \\(j&gt;1\\). Here, the observed dependant variable is \\(y_t\\in\\mathbb{R}\\), the covariate vector is \\(x_t \\in\\mathbb{R}^p\\), the disturbance is \\(\\varepsilon_t\\in\\mathbb{R}\\). The vectors \\(\\delta_j\\in\\mathbb{R}^p\\) are the parameter vectors (or regression coefficients).</p> <p>The least-squares estimates of the break dates is obtained by minimizing the sum of squared residuals [Bai2003]. Formally, the associated cost function on an interval \\(I\\) is</p> \\[ c(y_{I}) = \\min_{\\delta\\in\\mathbb{R}^p} \\sum_{t\\in I} \\|y_t - \\delta' x_t \\|_2^2. \\]"},{"location":"user-guide/costs/costlinear/#usage","title":"Usage","text":"<p>Start with the usual imports and create a signal with piecewise linear trends.</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\nn, n_reg = 2000, 3  # number of samples, number of regressors (including intercept)\nn_bkps = 3  # number of change points\n# regressors\ntt = np.linspace(0, 10 * np.pi, n)\nX = np.vstack((np.sin(tt), np.sin(5 * tt), np.ones(n))).T\n# parameter vectors\ndeltas, bkps = rpt.pw_constant(n, n_reg, n_bkps, noise_std=None, delta=(1, 3))\n# observed signal\ny = np.sum(X * deltas, axis=1)\ny += np.random.normal(size=y.shape)\n# display signal\nrpt.show.display(y, bkps, figsize=(10, 6))\nplt.show()\n</code></pre> <p>Then create a <code>CostLinear</code> instance and print the cost of the sub-signal <code>signal[50:150]</code>.</p> <pre><code># stack observed signal and regressors.\n# first dimension is the observed signal.\nsignal = np.column_stack((y.reshape(-1, 1), X))\nc = rpt.costs.CostLinear().fit(signal)\nprint(c.error(50, 150))\n</code></pre> <p>You can also compute the sum of costs for a given list of change points.</p> <pre><code>print(c.sum_of_costs(bkps))\nprint(c.sum_of_costs([10, 100, 200, 250, n]))\n</code></pre> <p>In order to use this cost class in a change point detection algorithm (inheriting from <code>BaseEstimator</code>), either pass a <code>CostLinear</code> instance (through the argument <code>custom_cost</code>) or set <code>model=\"linear\"</code>.</p> <pre><code>c = rpt.costs.CostLinear()\nalgo = rpt.Dynp(custom_cost=c)\n# is equivalent to\nalgo = rpt.Dynp(model=\"linear\")\n</code></pre>"},{"location":"user-guide/costs/costlinear/#references","title":"References","text":"<p>[Bai2003] J. Bai and P. Perron. Critical values for multiple structural change tests. Econometrics Journal, 6(1):72\u201378, 2003.</p>"},{"location":"user-guide/costs/costml/","title":"Change detection with a Mahalanobis-type metric (<code>CostMl</code>)","text":""},{"location":"user-guide/costs/costml/#description","title":"Description","text":"<p>Given a positive semi-definite matrix \\(M\\in\\mathbb{R}^{d\\times d}\\), this cost function detects changes in the mean of the embedded signal defined by the pseudo-metric</p> \\[ \\| x - y \\|_M^2 = (x-y)^t M (x-y). \\] <p>Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\(I\\), the cost function is equal to</p> \\[ c(y_{I}) = \\sum_{t\\in I} \\| y_t - \\bar{\\mu} \\|_{M}^2 \\] <p>where \\(\\bar{\\mu}\\) is the empirical mean of the sub-signal \\(\\{y_t\\}_{t\\in I}\\). The matrix \\(M\\) can for instance be the result of a similarity learning algorithm [Xing2003, Truong2019] or the inverse of the empirical covariance matrix (yielding the Mahalanobis distance).</p>"},{"location":"user-guide/costs/costml/#usage","title":"Usage","text":"<p>Start with the usual imports and create a signal.</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\nn, dim = 500, 3  # number of samples, dimension\nn_bkps, sigma = 3, 5  # number of change points, noise standart deviation\nsignal, bkps = rpt.pw_constant(n, dim, n_bkps, noise_std=sigma)\n</code></pre> <p>Then create a <code>CostMl</code> instance and print the cost of the sub-signal <code>signal[50:150]</code>.</p> <pre><code>M = np.eye(dim)\nc = rpt.costs.CostMl(metric=M).fit(signal)\nprint(c.error(50, 150))\n</code></pre> <p>You can also compute the sum of costs for a given list of change points.</p> <pre><code>print(c.sum_of_costs(bkps))\nprint(c.sum_of_costs([10, 100, 200, 250, n]))\n</code></pre> <p>In order to use this cost class in a change point detection algorithm (inheriting from <code>BaseEstimator</code>), either pass a <code>CostMl</code> instance (through the argument <code>custom_cost</code>) or set <code>model=\"mahalanobis\"</code>.</p> <pre><code>c = rpt.costs.CostMl(metric=M)\nalgo = rpt.Dynp(custom_cost=c)\n# is equivalent to\nalgo = rpt.Dynp(model=\"mahalanobis\", params={\"metric\": M})\n</code></pre>"},{"location":"user-guide/costs/costml/#references","title":"References","text":"<p>[Xing2003] Xing, E. P., Jordan, M. I., &amp; Russell, S. J. (2003). Distance metric learning, with application to clustering with side-Information. Advances in Neural Information Processing Systems (NIPS), 521\u2013528.</p> <p>[Truong2019] Truong, C., Oudre, L., &amp; Vayatis, N. (2019). Supervised kernel change point detection with partial annotations. Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1\u20135.</p>"},{"location":"user-guide/costs/costnormal/","title":"Gaussian process change (<code>CostNormal</code>)","text":""},{"location":"user-guide/costs/costnormal/#description","title":"Description","text":"<p>This cost function detects changes in the mean and covariance matrix of a sequence of multivariate Gaussian random variables. Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\(I\\), $$ c(y_{I}) = |I| \\log\\det(\\widehat{\\Sigma}_I + \\epsilon\\text{Id}) $$ where \\(\\widehat{\\Sigma}_I\\) is the empirical covariance matrix of the sub-signal \\(\\{y_t\\}_{t\\in I}\\) and \\(\\epsilon&gt;0\\) is a small constant added to cope with badly conditioned covariance matrices (new in version 1.1.5, see Issue 196). It is robust to strongly dependant processes; for more information, see [Lavielle1999] (univariate case) and [Lavielle2006] (multivariate case).</p>"},{"location":"user-guide/costs/costnormal/#usage","title":"Usage","text":"<p>Start with the usual imports and create a signal.</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\nn, dim = 500, 3  # number of samples, dimension\nn_bkps, sigma = 3, 5  # number of change points, noise standart deviation\nsignal, bkps = rpt.pw_constant(n, dim, n_bkps, noise_std=sigma)\n</code></pre> <p>Then create a <code>CostNormal</code> instance and print the cost of the sub-signal <code>signal[50:150]</code>.</p> <pre><code>c = rpt.costs.CostNormal().fit(signal)\nprint(c.error(50, 150))\n</code></pre> <p>You can also compute the sum of costs for a given list of change points.</p> <pre><code>print(c.sum_of_costs(bkps))\nprint(c.sum_of_costs([10, 100, 200, 250, n]))\n</code></pre> <p>In order to use this cost class in a change point detection algorithm (inheriting from <code>BaseEstimator</code>), either pass a <code>CostNormal</code> instance (through the argument <code>custom_cost</code>) or set <code>model=\"normal\"</code>.</p> <pre><code>c = rpt.costs.CostNormal()\nalgo = rpt.Dynp(custom_cost=c)\n# is equivalent to\nalgo = rpt.Dynp(model=\"normal\")\n</code></pre> <p>To set the small diagonal bias to 0 (default behaviour in versions 1.1.4 and before), simply do the following (change <code>Dynp</code> by the search method you need). <pre><code>c = rpt.costs.CostNormal(add_small_diag=False)\nalgo = rpt.Dynp(custom_cost=c)\n# or, equivalently,\nalgo = rpt.Dynp(model=\"normal\", params={\"add_small_diag\": False})\n</code></pre></p>"},{"location":"user-guide/costs/costnormal/#references","title":"References","text":"<p>[Lavielle1999] Lavielle, M. (1999). Detection of multiples changes in a sequence of dependant variables. Stochastic Processes and Their Applications, 83(1), 79\u2013102.</p> <p>[Lavielle2006] Lavielle, M., &amp; Teyssi\u00e8re, G. (2006). Detection of multiple change-points in multivariate time series. Lithuanian Mathematical Journal, 46(3).</p>"},{"location":"user-guide/costs/costrank/","title":"Rank-based cost function (<code>CostRank</code>)","text":""},{"location":"user-guide/costs/costrank/#description","title":"Description","text":"<p>This cost function detects general distribution changes in multivariate signals, using a rank transformation [Lung-Yut-Fong2015]. Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\([a, b)\\),</p> \\[ c_{rank}(a, b) = -(b - a) \\bar{r}_{a..b}' \\hat{\\Sigma}_r^{-1} \\bar{r}_{a..b} \\] <p>where \\(\\bar{r}_{a..b}\\) is the empirical mean of the sub-signal \\(\\{r_t\\}_{t=a+1}^b\\), and \\(\\hat{\\Sigma}_r\\) is the covariance matrix of the complete rank signal \\(r\\).</p>"},{"location":"user-guide/costs/costrank/#usage","title":"Usage","text":"<p>Start with the usual imports and create a signal.</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\nn, dim = 500, 3  # number of samples, dimension\nn_bkps, sigma = 3, 5  # number of change points, noise standard deviation\nsignal, bkps = rpt.pw_constant(n, dim, n_bkps, noise_std=sigma)\n</code></pre> <p>Then create a <code>CostRank</code> instance and print the cost of the sub-signal <code>signal[50:150]</code>.</p> <pre><code>c = rpt.costs.CostRank().fit(signal)\nprint(c.error(50, 150))\n</code></pre> <p>You can also compute the sum of costs for a given list of change points.</p> <pre><code>print(c.sum_of_costs(bkps))\nprint(c.sum_of_costs([10, 100, 200, 250, n]))\n</code></pre> <p>In order to use this cost class in a change point detection algorithm (inheriting from <code>BaseEstimator</code>), either pass a <code>CostRank</code> instance (through the argument <code>custom_cost</code>) or set <code>model=\"rank\"</code>.</p> <pre><code>c = rpt.costs.CostRank()\nalgo = rpt.Dynp(custom_cost=c)\n# is equivalent to\nalgo = rpt.Dynp(model=\"rank\")\n</code></pre>"},{"location":"user-guide/costs/costrank/#references","title":"References","text":"<p>[Lung-Yut-Fong2015] Lung-Yut-Fong, A., L\u00e9vy-Leduc, C., &amp; Capp\u00e9, O. (2015). Homogeneity and change-point detection tests for multivariate data using rank statistics. Journal de La Soci\u00e9t\u00e9 Fran\u00e7aise de Statistique, 156(4), 133\u2013162.</p>"},{"location":"user-guide/costs/costrbf/","title":"Kernelized mean change (<code>CostRbf</code>)","text":""},{"location":"user-guide/costs/costrbf/#description","title":"Description","text":"<p>Given a positive semi-definite kernel \\(k(\\cdot, \\cdot) : \\mathbb{R}^d\\times \\mathbb{R}^d \\mapsto \\mathbb{R}\\) and its associated feature map \\(\\Phi:\\mathbb{R}^d \\mapsto \\mathcal{H}\\) (where \\(\\mathcal{H}\\) is an appropriate Hilbert space), this cost function detects changes in the mean of the embedded signal \\(\\{\\Phi(y_t)\\}_t\\) [Garreau2018, Arlot2019]. Formally, for a signal \\(\\{y_t\\}_t\\) on an interval \\(I\\),</p> \\[ c(y_{I}) = \\sum_{t\\in I} \\| \\Phi(y_t) - \\bar{\\mu} \\|_{\\mathcal{H}}^2 \\] <p>where \\(\\bar{\\mu}\\) is the empirical mean of the embedded sub-signal \\(\\{\\Phi(y_t)\\}_{t\\in I}\\). Here the kernel is the radial basis function (rbf):</p> \\[ k(x, y) = \\exp(-\\gamma \\| x - y \\|^2 ) \\] <p>where \\(\\| \\cdot \\|\\) is the Euclidean norm and \\(\\gamma&gt;0\\) is the so-called bandwidth parameter and is determined according to median heuristics (i.e. equal to the inverse of median of all pairwise distances).</p> <p>In a nutshell, this cost function is able to detect changes in the distribution of an iid sequence of random variables. Because it is non-parametric, it is performs reasonably well on a wide range of tasks.</p>"},{"location":"user-guide/costs/costrbf/#usage","title":"Usage","text":"<p>Start with the usual imports and create a signal.</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\nn, dim = 500, 3  # number of samples, dimension\nn_bkps, sigma = 3, 5  # number of change points, noise standart deviation\nsignal, bkps = rpt.pw_constant(n, dim, n_bkps, noise_std=sigma)\n</code></pre> <p>Then create a <code>CostRbf</code> instance and print the cost of the sub-signal <code>signal[50:150]</code>.</p> <pre><code>c = rpt.costs.CostRbf().fit(signal)\nprint(c.error(50, 150))\n</code></pre> <p>You can also compute the sum of costs for a given list of change points.</p> <pre><code>print(c.sum_of_costs(bkps))\nprint(c.sum_of_costs([10, 100, 200, 250, n]))\n</code></pre> <p>In order to use this cost class in a change point detection algorithm (inheriting from <code>BaseEstimator</code>), either pass a <code>CostRbf</code> instance (through the argument <code>custom_cost</code>) or set <code>model=\"rbf\"</code>.</p> <pre><code>c = rpt.costs.CostRbf()\nalgo = rpt.Dynp(custom_cost=c)\n# is equivalent to\nalgo = rpt.Dynp(model=\"rbf\")\n</code></pre>"},{"location":"user-guide/costs/costrbf/#references","title":"References","text":"<p>[Garreau2018] Garreau, D., &amp; Arlot, S. (2018). Consistent change-point detection with kernels. Electronic Journal of Statistics, 12(2), 4440\u20134486.</p> <p>[Arlot2019] Arlot, S., Celisse, A., &amp; Harchaoui, Z. (2019). A kernel multiple change-point algorithm via model selection. Journal of Machine Learning Research, 20(162), 1\u201356.</p>"},{"location":"user-guide/datasets/pw_constant/","title":"Piecewise constant (<code>pw_constant</code>)","text":""},{"location":"user-guide/datasets/pw_constant/#description","title":"Description","text":"<p>For a given number of samples \\(T\\), number \\(K\\) of change points and noise variance \\(\\sigma^2\\), the function <code>pw_constant</code> generates change point dexes \\(0 &lt; t_1 &lt; \\dots &lt; t_K &lt; T\\) and a piecewise constant signal \\(\\{y_t\\}_t\\) with additive Gaussian noise.</p>"},{"location":"user-guide/datasets/pw_constant/#usage","title":"Usage","text":"<p>Start with the usual imports and create a signal.</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\nn, dim = 500, 3  # number of samples, dimension\nn_bkps, sigma = 3, 5  # number of change points, noise standard deviation\nsignal, bkps = rpt.pw_constant(n, dim, n_bkps, noise_std=sigma)\nrpt.display(signal, bkps)\n</code></pre> <p>The mean shift amplitude is uniformly drawn from an interval that can be changed through the keyword <code>delta</code>.</p> <pre><code>signal, bkps = rpt.pw_constant(n, dim, n_bkps, noise_std=sigma, delta=(1, 10))\n</code></pre>"},{"location":"user-guide/datasets/pw_linear/","title":"Piecewise linear (<code>pw_linear</code>)","text":""},{"location":"user-guide/datasets/pw_linear/#description","title":"Description","text":"<p>This function <code>pw_linear</code> simulates a piecewise linear model (see Cost linear). The covariates are standard Gaussian random variables. The response variable is a (piecewise) linear combination of the covariates.</p>"},{"location":"user-guide/datasets/pw_linear/#usage","title":"Usage","text":"<p>Start with the usual imports and create a signal.</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\nn, dim = 500, 3  # number of samples, dimension of the covariates\nn_bkps, sigma = 3, 5  # number of change points, noise standart deviation\nsignal, bkps = rpt.pw_linear(n, dim, n_bkps, noise_std=sigma)\nrpt.display(signal, bkps)\n</code></pre>"},{"location":"user-guide/datasets/pw_normal/","title":"Piecewise 2D Gaussian process (<code>pw_normal</code>)","text":""},{"location":"user-guide/datasets/pw_normal/#description","title":"Description","text":"<p>The function <code>pw_normal</code> simulates a 2D signal of Gaussian i.i.d. random variables with zero mean and covariance matrix alternating between \\([[1, 0.9], [0.9, 1]]\\) and \\([[1, -0.9], [-0.9, 1]]\\) at every change point.</p> <p> Top and middle: 2D signal example. Bottom: Scatter plot for each regime type</p>"},{"location":"user-guide/datasets/pw_normal/#usage","title":"Usage","text":"<p>Start with the usual imports and create a signal.</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\nn = 500  # number of samples\nn_bkps = 3  # number of change points\nsignal, bkps = rpt.pw_normal(n, n_bkps)\nrpt.display(signal, bkps)\n</code></pre>"},{"location":"user-guide/datasets/pw_wavy/","title":"Piecewise sinusoidal signal (<code>pw_wavy</code>)","text":""},{"location":"user-guide/datasets/pw_wavy/#description","title":"Description","text":"<p>The function <code>pw_wavy</code> simulates a sum-of-sine signal \\(y_t=\\sin(2\\pi f_1 t)+\\sin(2\\pi f_2 t)\\) where \\(t=0,\\dots,T-1\\). The frequency vector \\([f_1, f_2]\\) alternates between \\([0.075, 0.1]\\) and \\([0.1, 0.125]\\) at each change point index. Gaussian white noise can be added to the signal.</p> <p> Top: signal example. Bottom: associated spectrogram.</p>"},{"location":"user-guide/datasets/pw_wavy/#usage","title":"Usage","text":"<p>Start with the usual imports and create a signal.</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\nn, dim = 500, 3  # number of samples, dimension\nn_bkps, sigma = 3, 5  # number of change points, noise standart deviation\nsignal, bkps = rpt.pw_wavy(n, n_bkps, noise_std=sigma)\nrpt.display(signal, bkps)\n</code></pre>"},{"location":"user-guide/detection/binseg/","title":"Binary segmentation (<code>Binseg</code>)","text":""},{"location":"user-guide/detection/binseg/#description","title":"Description","text":"<p>Binary change point detection is used to perform fast signal segmentation and is implemented in <code>Binseg</code>. It is a sequential approach: first, one change point is detected in the complete input signal, then series is split around this change point, then the operation is repeated on the two resulting sub-signals. For a theoretical and algorithmic analysis of <code>Binseg</code>, see for instance [Bai1997] and [Fryzlewicz2014]. The benefits of binary segmentation includes low complexity (of the order of \\(\\mathcal{O}(Cn\\log n)\\), where \\(n\\) is the number of samples and \\(C\\) the complexity of calling the considered cost function on one sub-signal), the fact that it can extend any single change point detection method to detect multiple changes points and that it can work whether the number of regimes is known beforehand or not.</p> <p> Schematic view of the binary segmentation algorithm</p>"},{"location":"user-guide/detection/binseg/#usage","title":"Usage","text":"<p>Start with the usual imports and create a signal.</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\nn = 500  # number of samples\nn_bkps, sigma = 3, 5  # number of change points, noise standard deviation\nsignal, bkps = rpt.pw_constant(n, dim, n_bkps, noise_std=sigma)\n</code></pre> <p>To perform a binary segmentation of a signal, initialize a <code>BinSeg</code> instance.</p> <p><pre><code># change point detection\nmodel = \"l2\"  # \"l1\", \"rbf\", \"linear\", \"normal\", \"ar\",...\nalgo = rpt.Binseg(model=model).fit(signal)\nmy_bkps = algo.predict(n_bkps=3)\n\n# show results\nrpt.show.display(signal, bkps, my_bkps, figsize=(10, 6))\nplt.show()\n</code></pre> In the situation in which the number of change points is unknown, one can specify a penalty using the <code>pen</code> parameter or a threshold on the residual norm using <code>epsilon</code>.</p> <pre><code>my_bkps = algo.predict(pen=np.log(n) * dim * sigma**2)\n# or\nmy_bkps = algo.predict(epsilon=3 * n * sigma**2)\n</code></pre> <p>For faster predictions, one can modify the <code>jump</code> parameter during initialization. The higher it is, the faster the prediction is achieved (at the expense of precision).</p> <pre><code>algo = rpt.Binseg(model=model, jump=10).fit(signal)\n</code></pre>"},{"location":"user-guide/detection/binseg/#references","title":"References","text":"<p>[Bai1997] Bai, J. (1997). Estimating multiple breaks one at a time. Econometric Theory, 13(3), 315\u2013352.</p> <p>[Fryzlewicz2014] Fryzlewicz, P. (2014). Wild binary segmentation for multiple change-point detection. The Annals of Statistics, 42(6), 2243\u20132281.</p>"},{"location":"user-guide/detection/bottomup/","title":"Bottom-up segmentation (<code>BottomUp</code>)","text":""},{"location":"user-guide/detection/bottomup/#description","title":"Description","text":"<p>Bottom-up change point detection is used to perform fast signal segmentation and is implemented in <code>BottomUp</code> in a sequential manner. Contrary to binary segmentation, which is a greedy procedure, bottom-up segmentation is generous: it starts with many change points and successively deletes the less significant ones. First, the signal is divided in many sub-signals along a regular grid. Then contiguous segments are successively merged according to a measure of how similar they are. See for instance [Keogh2001] or [Fryzlewicz2007] for an algorithmic analysis of <code>BottomUp</code>. The benefits of bottom-up segmentation includes low complexity (of the order of \\(\\mathcal{O}(n\\log n)\\), where \\(n\\) is the number of samples), the fact that it can extend any single change point detection method to detect multiple changes points and that it can work whether the number of regimes is known beforehand or not.</p> <p> Schematic view of the bottom-up segmentation algorithm</p>"},{"location":"user-guide/detection/bottomup/#usage","title":"Usage","text":"<p>Start with the usual imports and create a signal.</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\nn, dim = 500, 3  # number of samples, dimension\nn_bkps, sigma = 3, 5  # number of change points, noise standart deviation\nsignal, bkps = rpt.pw_constant(n, dim, n_bkps, noise_std=sigma)\n</code></pre> <p>To perform a bottom-up segmentation of a signal, initialize a <code>BottomUp</code> instance.</p> <pre><code># change point detection\nmodel = \"l2\"  # \"l1\", \"rbf\", \"linear\", \"normal\", \"ar\"\nalgo = rpt.BottomUp(model=model).fit(signal)\nmy_bkps = algo.predict(n_bkps=3)\n\n# show results\nrpt.show.display(signal, bkps, my_bkps, figsize=(10, 6))\nplt.show()\n</code></pre> <p>In the situation in which the number of change points is unknown, one can specify a penalty using the <code>pen</code> parameter or a threshold on the residual norm using <code>epsilon</code>.</p> <pre><code>my_bkps = algo.predict(pen=np.log(n) * dim * sigma**2)\n# or\nmy_bkps = algo.predict(epsilon=3 * n * sigma**2)\n</code></pre> <p>For faster predictions, one can modify the <code>jump</code> parameter during initialization. The higher it is, the faster the prediction is achieved (at the expense of precision).</p> <pre><code>algo = rpt.BottomUp(model=model, jump=10).fit(signal)\n</code></pre>"},{"location":"user-guide/detection/bottomup/#references","title":"References","text":"<p>[Keogh2001] Keogh, E., Chu, S., Hart, D., &amp; Pazzani, M. (2001). An online algorithm for segmenting time series. Proceedings of the IEEE International Conference on Data Mining (ICDM), 289\u2013296.</p> <p>[Fryzlewicz2007] Fryzlewicz, P. (2007). Unbalanced Haar technique for nonparametric function estimation. Journal of the American Statistical Association, 102(480), 1318\u20131327.</p>"},{"location":"user-guide/detection/dynp/","title":"Dynamic programming (<code>Dynp</code>)","text":""},{"location":"user-guide/detection/dynp/#description","title":"Description","text":"<p>The method is implemented in both <code>Dynp</code>, which is a full native python implementation for which the user can choose any cost functions defined in <code>ruptures.costs</code></p> <p>It finds the (exact) minimum of the sum of costs by computing the cost of all subsequences of a given signal. It is called \"dynamic programming\" because the search over all possible segmentations is ordered using a dynamic programming approach.</p> <p>In order to work, the user must specify in advance the number of changes to detect. (Consider using penalized methods when this number is unknown.)</p> <p>The complexity of the dynamic programming approach is of the order \\(\\mathcal{O}(CKn^2)\\), where \\(K\\) is the number of change points to detect, \\(n\\) the number of samples and \\(C\\) the complexity of calling the considered cost function on one sub-signal. Consequently, piecewise constant models (<code>model=l2</code>) are significantly faster than linear or autoregressive models.</p> <p>To reduce the computational cost, you can consider only a subsample of possible change point indexes, by changing the <code>min_size</code> and <code>jump</code> arguments when instantiating Dynp:</p> <ul> <li><code>min_size</code> controls the minimum distance between change points; for instance, if <code>min_size=10</code>, all change points will be at least 10 samples apart.</li> <li><code>jump</code> controls the grid of possible change points; for instance, if <code>jump=k</code>, only changes at <code>k, 2*k, 3*k,...</code> are considered.</li> </ul>"},{"location":"user-guide/detection/dynp/#usage","title":"Usage","text":"<pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\nn, dim = 500, 3\nn_bkps, sigma = 3, 5\nsignal, bkps = rpt.pw_constant(n, dim, n_bkps, noise_std=sigma)\n\n# change point detection\nmodel = \"l1\"  # \"l2\", \"rbf\"\nalgo = rpt.Dynp(model=model, min_size=3, jump=5).fit(signal)\nmy_bkps = algo.predict(n_bkps=3)\n\n# show results\nrpt.show.display(signal, bkps, my_bkps, figsize=(10, 6))\nplt.show()\n</code></pre>"},{"location":"user-guide/detection/kernelcpd/","title":"Kernel change point detection","text":""},{"location":"user-guide/detection/kernelcpd/#problem-formulation","title":"Problem formulation","text":"<p>In this section, the kernel change point detection setting is briefly described. The interested reader can refer to [Celisse2018, Arlot2019] for a more complete introduction. Let \\(y = \\{y_0,y_1,\\dots,y_{T-1}\\}\\) denote a \\(\\mathbb{R}^d\\)-valued signal with \\(T\\) samples. This signal is mapped onto a reproducing Hilbert space (rkhs) \\(\\mathcal{H}\\) associated with a user-defined kernel function \\(k(\\cdot, \\cdot):\\mathbb{R}^d\\times\\mathbb{R}^d\\rightarrow\\mathbb{R}\\). The mapping function \\(\\phi:\\mathbb{R}^d\\rightarrow\\mathcal{H}\\) onto this rkhs is implicitly defined by \\(\\phi(y_t) = k(y_t, \\cdot)\\in\\mathcal{H}\\) resulting in the following inner-product and norm:</p> \\[ \\langle\\phi(y_s)\\mid\\phi(y_t)\\rangle_{\\mathcal{H}} = k(y_s,y_t) \\] <p>and</p> \\[ \\|\\phi(y_t)\\|_{\\mathcal{H}}^2 = k(y_t,y_t) \\] <p>for any samples \\(y_s,y_t\\in\\mathbb{R}^d\\). Kernel change point detection consists in finding mean-shifts in the mapped signal \\(\\phi(y)\\) by minimizing \\(V(\\cdot)\\) where</p> \\[ V(t_1,\\dots,t_K) := \\sum_{k=0}^K\\sum_{t=t_k}^{t_{k+1}-1} \\|\\phi(y_t)-\\bar{\\mu}_{t_k..t_{k+1}}\\|^2_{\\mathcal{H}} \\] <p>where \\(\\bar{\\mu}_{t_k..t_{k+1}}\\) is the empirical mean of the sub-signal \\(\\phi(y_{t_k}), \\phi(y_{t_k+1}),\\dots,\\phi(y_{t_{k+1}-1})\\), and \\(t_1,t_2,\\dots,t_K\\) are change point indexes, in increasing order. (By convention \\(t_0=0\\) and \\(t_{K+1}=T\\).)</p> <p>If the number of changes is known beforehand, we solve the following optimization problem, over all possible change positions \\(t_1&lt;t_2&lt;\\dots&lt;t_K\\) (where the number \\(K\\) of changes is provided by the user):</p> \\[ \\hat{t}_1,\\dots,\\hat{t}_K := \\arg\\min_{t_1,\\dots,t_K} V(t_1,\\dots,t_K). \\] <p>The exact optimization procedure is described in [Celisse2018].</p> <p>If the number of changes is not known, we solve the following penalized optimization problem</p> \\[ \\hat{K}, \\{\\hat{t}_1,\\dots,\\hat{t}_{\\hat{K}}\\} := \\arg\\min_{K, \\{t_1,\\dots, t_K\\}} V(t_1,\\dots, t_K) + \\beta K \\] <p>where \\(\\beta&gt;0\\) is the smoothing parameter (provided by the user) and \\(\\hat{K}\\) is the estimated number of change points. Higher values of \\(\\beta\\) produce lower \\(\\hat{K}\\). The exact optimization procedure is described in [Killick2012].</p>"},{"location":"user-guide/detection/kernelcpd/#available-kernels","title":"Available kernels","text":"<p>We list below a number of kernels that are already implemented in <code>ruptures</code>. In the following, \\(u\\) and \\(v\\) are two d-dimensional vectors and \\(\\|\\cdot\\|\\) is the Euclidean norm.</p> Kernel Description Cost function Linear<code>model=\"linear\"</code> \\(k_{\\text{linear}}(u, v) = u^T v\\). <code>CostL2</code> Gaussian<code>model=\"rbf\"</code> \\(k_{\\text{Gaussian}}(u,v)=\\exp(-\\gamma \\|u-v\\|^2)\\)where \\(\\gamma&gt;0\\) is a user-defined parameter. <code>CostRbf</code> Cosine<code>model=\"cosine\"</code> \\(k_{\\text{cosine}}(u, v) = (u^T v)/(\\|u\\|\\|v\\|)\\) <code>CostCosine</code>"},{"location":"user-guide/detection/kernelcpd/#implementation-and-usage","title":"Implementation and usage","text":"<p>Kernel change point detection is implemented in the class <code>KernelCPD</code>, which is a C implementation of dynamic programming and PELT. To see it in action, please look at the gallery of examples, in particular:</p> <ul> <li>Kernel change point detection: a performance comparison</li> </ul> <p>The exact class API is available here.</p>"},{"location":"user-guide/detection/kernelcpd/#references","title":"References","text":"<p>[Gretton2012] Gretton, A., Borgwardt, K. M., Rasch, M. J., Sch\u00f6lkopf, B., &amp; Smola, A. (2012). A kernel two-sample test. The Journal of Machine Learning Research, 13, 723\u2013773.</p> <p>[Killick2012] Killick, R., Fearnhead, P., &amp; Eckley, I. (2012). Optimal detection of changepoints with a linear computational cost. Journal of the American Statistical Association, 107(500), 1590\u20131598.</p> <p>[Celisse2018] Celisse, A., Marot, G., Pierre-Jean, M., &amp; Rigaill, G. (2018). New efficient algorithms for multiple change-point detection with reproducing kernels. Computational Statistics and Data Analysis, 128, 200\u2013220.</p> <p>[Arlot2019] Arlot, S., Celisse, A., &amp; Harchaoui, Z. (2019). A kernel multiple change-point algorithm via model selection. Journal of Machine Learning Research, 20(162), 1\u201356.</p>"},{"location":"user-guide/detection/pelt/","title":"Linearly penalized segmentation (<code>Pelt</code>)","text":""},{"location":"user-guide/detection/pelt/#description","title":"Description","text":"<p>The method is implemented in <code>Pelt</code>.</p> <p>Because the enumeration of all possible partitions impossible, the algorithm relies on a pruning rule. Many indexes are discarded, greatly reducing the computational cost while retaining the ability to find the optimal segmentation. The implementation follows [Killick2012]. In addition, under certain conditions on the change point repartition, the avarage computational complexity is of the order of \\(\\mathcal{O}(CKn)\\), where \\(K\\) is the number of change points to detect, \\(n\\) the number of samples and \\(C\\) the complexity of calling the considered cost function on one sub-signal. Consequently, piecewise constant models (<code>model=l2</code>) are significantly faster than linear or autoregressive models.</p> <p>To reduce the computational cost, you can consider only a subsample of possible change point indexes, by changing the <code>min_size</code> and <code>jump</code> arguments when instantiating Pelt:</p> <ul> <li><code>min_size</code> controls the minimum distance between change points; for instance, if <code>min_size=10</code>, all change points will be at least 10 samples apart.</li> <li><code>jump</code> controls the grid of possible change points; for instance, if <code>jump=k</code>, only changes at <code>k, 2*k, 3*k,...</code> are considered.</li> </ul>"},{"location":"user-guide/detection/pelt/#usage","title":"Usage","text":"<pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\nn, dim = 500, 3\nn_bkps, sigma = 3, 1\nsignal, bkps = rpt.pw_constant(n, dim, n_bkps, noise_std=sigma)\n\n# change point detection\nmodel = \"l1\"  # \"l2\", \"rbf\"\nalgo = rpt.Pelt(model=model, min_size=3, jump=5).fit(signal)\nmy_bkps = algo.predict(pen=3)\n\n# show results\nfig, ax_arr = rpt.display(signal, bkps, my_bkps, figsize=(10, 6))\nplt.show()\n</code></pre>"},{"location":"user-guide/detection/pelt/#references","title":"References","text":"<p>[Killick2012] Killick, R., Fearnhead, P., &amp; Eckley, I. (2012). Optimal detection of changepoints with a linear computational cost. Journal of the American Statistical Association, 107(500), 1590\u20131598.</p>"},{"location":"user-guide/detection/window/","title":"Window-based change point detection (<code>Window</code>)","text":""},{"location":"user-guide/detection/window/#description","title":"Description","text":"<p>Window-based change point detection is used to perform fast signal segmentation and is implemented in <code>Window</code>. The algorithm uses two windows which slide along the data stream. The statistical properties of the signals within each window are compared with a discrepancy measure. For a given cost function \\(c(\\cdot)\\), a discrepancy measure is derived \\(d(\\cdot,\\cdot)\\) as follows:</p> \\[ d(y_{u..v}, y_{v..w}) = c(y_{u..w}) - c(y_{u..v}) - c(y_{v..w}) \\] <p>where \\(\\{y_t\\}_t\\) is the input signal and \\(u &lt; v &lt; w\\) are indexes. The discrepancy is the cost gain of splitting the sub-signal \\(y_{u..w}\\) at the index \\(v\\). If the sliding windows \\(u..v\\) and \\(v..w\\) both fall into a segment, their statistical properties are similar and the discrepancy between the first window and the second window is low. If the sliding windows fall into two dissimilar segments, the discrepancy is significantly higher, suggesting that the boundary between windows is a change point. The discrepancy curve is the curve, defined for all indexes \\(t\\) between \\(w/2\\) and \\(n-w/2\\) (\\(n\\) is the number of samples),</p> \\[ \\big(t, d(y_{t-w/2..t}, y_{t..t+w/2})\\big) \\] <p>where \\(w\\) is the window length. A sequential peak search is performed on the discrepancy curve in order to detect change points.</p> <p>The benefits of window-based segmentation includes low complexity (of the order of \\(\\mathcal{O}(n w)\\), where \\(n\\) is the number of samples), the fact that it can extend any single change point detection method to detect multiple changes points and that it can work whether the number of regimes is known beforehand or not.</p> <p> Schematic view of the window sliding segmentation algorithm</p>"},{"location":"user-guide/detection/window/#usage","title":"Usage","text":"<p>Start with the usual imports and create a signal.</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\nn, dim = 500, 3  # number of samples, dimension\nn_bkps, sigma = 3, 5  # number of change points, noise standart deviation\nsignal, bkps = rpt.pw_constant(n, dim, n_bkps, noise_std=sigma)\n</code></pre> <p>To perform a binary segmentation of a signal, initialize a <code>Window</code> instance.</p> <pre><code># change point detection\nmodel = \"l2\"  # \"l1\", \"rbf\", \"linear\", \"normal\", \"ar\"\nalgo = rpt.Window(width=40, model=model).fit(signal)\nmy_bkps = algo.predict(n_bkps=3)\n\n# show results\nrpt.show.display(signal, bkps, my_bkps, figsize=(10, 6))\nplt.show()\n</code></pre> <p>The window length (in number of samples) is modified through the argument <code>width</code>. Usual methods assume that the window length is smaller than the smallest regime length.</p> <p>In the situation in which the number of change points is unknown, one can specify a penalty using the <code>pen</code> parameter or a threshold on the residual norm using <code>epsilon</code>.</p> <pre><code>my_bkps = algo.predict(pen=np.log(n) * dim * sigma**2)\n# or\nmy_bkps = algo.predict(epsilon=3 * n * sigma**2)\n</code></pre> <p>For faster predictions, one can modify the <code>jump</code> parameter during initialization. The higher it is, the faster the prediction is achieved (at the expense of precision).</p> <pre><code>algo = rpt.Window(model=model, jump=10).fit(signal)\n</code></pre>"},{"location":"user-guide/metrics/hausdorff/","title":"Hausdorff metric (<code>hausdorff</code>)","text":""},{"location":"user-guide/metrics/hausdorff/#description","title":"Description","text":"<p>The <code>hausdorff</code> function computes the Hausdorff metric which measures the worst prediction error. Assume a set of change point indexes \\(t_1,t_2,\\dots\\) and their estimates \\(\\hat{t}_1, \\hat{t}_2,\\dots\\). The Hausdorff metric is then equal to</p> \\[ \\text{Hausdorff}(\\{t_k\\}_k, \\{\\hat{t}_k\\}_k) :=  \\max \\{ \\max_k \\min_l |t_k - \\hat{t}_l| \\, , \\max_k \\min_l |\\hat{t}_k - t_l|\\}. \\] <p> Schematic example: true segmentation in gray, estimated segmentation in dashed lines. Here, Hausdorff is equal to \\(\\max(\\Delta t_1, \\Delta t_2, \\Delta t_3)\\).</p>"},{"location":"user-guide/metrics/hausdorff/#usage","title":"Usage","text":"<p>Start with the usual imports and create two segmentations to compare.</p> <pre><code>from ruptures.metrics import hausdorff\n\nbkps1, bkps2 = [100, 200, 500], [105, 115, 350, 400, 500]\nprint(hausdorff(bkps1, bkps2))\n</code></pre>"},{"location":"user-guide/metrics/precisionrecall/","title":"Precision and recall (<code>precision_recall</code>)","text":""},{"location":"user-guide/metrics/precisionrecall/#description","title":"Description","text":"<p>The precision and recall of an estimated segmentation is computed by the function <code>precision_recall</code> as follows. A true change point is declared \"detected\" (or positive) if there is at least one computed change point at less than \"margin\" points from it. Formally, assume a set of change point indexes \\(t_1,t_2,\\dots\\) and their estimates \\(\\hat{t}_1, \\hat{t}_2,\\dots\\) In the context of change point detection, precision and recall are defined as follows:</p> \\[ \\text{precision}:=|\\text{TP}|/|\\{\\hat{t}_l\\}_l| \\quad \\text{and}\\quad\\text{recall}:=|\\text{TP}|/|\\{t_k\\}_k| \\] <p>where, for a given margin \\(M\\), true positives \\(\\text{TP}\\) are true change points for which there is an estimated one at less than \\(M\\) samples, i.e.</p> \\[ \\text{TP}:= \\{t_k\\,|\\, \\exists\\, \\hat{t}_l\\,\\, \\text{s.t.}\\, |\\hat{t}_l - t_k|&lt;M \\}. \\] <p> Schematic example: true segmentation in gray, estimated segmentation in dashed lines and margin in dashed areas. Here, precision is 2/3 and recall is 2/2.</p>"},{"location":"user-guide/metrics/precisionrecall/#usage","title":"Usage","text":"<p>Start with the usual imports and create two change point sets to compare.</p> <pre><code>from ruptures.metrics import precision_recall\n\nbkps1, bkps2 = [100, 200, 500], [105, 115, 350, 400, 500]\np, r = precision_recall(bkps1, bkps2)\nprint((p, r))\n</code></pre> <p>The margin parameter \\(M\\) can be changed through the keyword <code>margin</code> (default is 10 samples).</p> <pre><code>p, r = precision_recall(bkps1, bkps2, margin=10)\nprint((p, r))\np, r = precision_recall(bkps1, bkps2, margin=20)\nprint((p, r))\n</code></pre>"},{"location":"user-guide/metrics/randindex/","title":"Rand index (<code>randindex</code>)","text":""},{"location":"user-guide/metrics/randindex/#description","title":"Description","text":"<p>The Rand index (\\(RI\\)) measures the similarity between two segmentations and is equal to the proportion of aggreement between two partitions. Formally, for \\(\\mathcal{T}_1\\) and \\(\\mathcal{T}_2\\) two partitions of \\(\\{1, 2,\\dots,T\\}\\),</p> \\[ RI := \\frac{N_0 + N_1}{T(T+1)/2} \\] <p>where</p> <ul> <li>\\(N_0\\) is the number of pairs of samples that belong to the same segment according to \\(\\mathcal{T}_1\\) and \\(\\mathcal{T}_2\\),</li> <li>\\(N_1\\) is the number of pairs of samples that belong to different segments according to \\(\\mathcal{T}_1\\) and \\(\\mathcal{T}_2\\).</li> </ul> <p>\\(RI\\) is between 0 (total disagreement) and 1 (total agreement). It is available in the <code>randindex</code> function which uses the efficient implementation of [Prates2021].</p>"},{"location":"user-guide/metrics/randindex/#usage","title":"Usage","text":"<p>Start with the usual imports and create two segmentations to compare.</p> <pre><code>from ruptures.metrics import randindex\n\nbkps1, bkps2 = [100, 200, 500], [105, 115, 350, 400, 500]\nprint(randindex(bkps1, bkps2))\n</code></pre>"},{"location":"user-guide/metrics/randindex/#references","title":"References","text":"<p>[Prates2021] Prates, L. (2021). A more efficient algorithm to compute the Rand Index for change-point problems. ArXiv:2112.03738.</p>"},{"location":"user-guide/show/display/","title":"Display (<code>display</code>)","text":""},{"location":"user-guide/show/display/#description","title":"Description","text":"<p>The function <code>display</code> displays a signal and the change points provided in alternating colors. If another set of change point indexes is provided, they are displayed with dashed vertical dashed lines.</p>"},{"location":"user-guide/show/display/#usage","title":"Usage","text":"<p>Start with the usual imports and create a signal.</p> <pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport ruptures as rpt\n\n# creation of data\nn, dim = 500, 2  # number of samples, dimension\nn_bkps, sigma = 3, 5  # number of change points, noise standart deviation\nsignal, bkps = rpt.pw_constant(n, dim, n_bkps, noise_std=sigma)\nrpt.display(signal, bkps)\n</code></pre> <p>If we computed another set of change points, for instance <code>[110, 150, 320, 500]</code>, we can easily compare the two segmentations.</p> <pre><code>rpt.display(signal, bkps, [110, 150, 320, 500])\n</code></pre> <p> Example output of the function <code>display</code>.</p>"}]}